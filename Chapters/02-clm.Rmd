# Complex Linear Regression {#CLR}

```{r echo=FALSE}
options(scipen = 100)
```

## Simple Complex Linear Regression {#simpleCLR}
### Model formulation
The simple Complex Linear Regression (CLR) can be written as:
\begin{equation}
    y_j = \beta_0 + \beta_1 x_j + \epsilon_j,
    (\#eq:SimpleCLRComplex)
\end{equation}
where $j$ is the index for an observation and every parameter and variable is a complex number, i.e. $y_j = y_{r,j}+i y_{i,j}$ is the complex response variable, $x_j = x_{r,j}+i x_{i,j}$ is the complex explanatory variable, $\beta_{l} = \beta_{l,r} + i \beta_{l,i}$ is the $l$-th parameter and $\epsilon_j = \epsilon_{r,j} + i \epsilon_{i,j}$ is the error term. For now, we do not make any specific assumptions about the distribution of the complex error term, we will come back to this later in this chapter. Inserting these values in \@ref(eq:SimpleCLRComplex) leads to:
\begin{equation}
    y_{r,j}+i y_{i,j} = (\beta_{0,r} + i \beta_{0,i}) + (\beta_{1,r} + i \beta_{1,i}) (x_{r,j}+i x_{i,j}) + (\epsilon_{r,j} + i \epsilon_{i,j}),
    (\#eq:SimpleCLR)
\end{equation}
which is in fact a multivariate model, capturing how the change of values in pair of variables $x_r$, $x_i$ leads to the change in the pair of variables $y_r$ and $y_i$. Given that any complex equation can be represented as a system of two equations, the model \@ref(eq:SimpleCLR) can be represented as a system of two linear regressions:
\begin{equation}
    \begin{aligned}
        y_{r,j} = & \beta_{0,r} + \beta_{1,r} x_{r,j} - \beta_{1,i} x_i + \epsilon_{r,j} \\
        y_{i,j} = & \beta_{0,i} + \beta_{1,r} x_{i,j} + \beta_{1,i} x_r + \epsilon_{i,j}
    \end{aligned}
    (\#eq:SimpleCLRSystem)
\end{equation}
This model captures a very specific dynamics between the real and imaginary parts, given that they share the same set of parameters for the slope. But most importantly it shows how each complex variable $y$ relates to variable $x$ in a four dimensional space. Note that the real and imaginary parts of $y$ can be exchanged without a serious impact on the model: in that case the values of parameters would change, but the relation between $x$ and $y$ would stay the same. Similarly, changing $x_r$ with $x_i$ would only lead to different estimates of parameters, the general relation will hold.

Going even further, using the vector and matrix representations of complex variables, the same system of equations \@ref(eq:SimpleCLRSystem) can be rewritten as:
\begin{equation}
    \begin{pmatrix} y_{r,j} \\ y_{i,j} \end{pmatrix} = \begin{pmatrix} \beta_{0,r} \\ \beta_{0,i} \end{pmatrix} + \begin{pmatrix} x_{r,j} & -x_{i,j} \\ x_{i,j} & x_{r,j} \end{pmatrix} \begin{pmatrix} \beta_{1,r} \\ \beta_{1,i} \end{pmatrix} + \begin{pmatrix} \epsilon_{r,j} \\ \epsilon_{i,j} \end{pmatrix} ,
    (\#eq:SimpleCLRSystemVector01)
\end{equation}
or uniting all the parameters in one vector:
\begin{equation}
    \begin{pmatrix} y_{r,j} \\ y_{i,j} \end{pmatrix} = \begin{pmatrix} 1 & 0 & x_{r,j} & -x_{i,j} \\ 0 & 1 & x_{i,j} & x_{r,j} \end{pmatrix} \begin{pmatrix} \beta_{0,r} \\ \beta_{0,i} \\ \beta_{1,r} \\ \beta_{1,i} \end{pmatrix} + \begin{pmatrix} \epsilon_{r,j} \\ \epsilon_{i,j} \end{pmatrix} .
    (\#eq:SimpleCLRSystemVector02)
\end{equation}
This form can then be represented in a classical matrix notations:
\begin{equation}
    \mathbf{y}_j = \mathbf{X}_j \boldsymbol{\beta} + \boldsymbol{\epsilon}_j .
    (\#eq:SimpleCLRSystemVector03)
\end{equation}
And if we stack each of the vectors and matrices for each $j=1 \dots n$ we get even more compact form:
\begin{equation}
    \mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{E} ,
    (\#eq:SimpleCLRSystemVectorFinal)
\end{equation}
where $\mathbf{Y}=\begin{pmatrix}\mathbf{y}_1 \\ \mathbf{y}_2\\ \vdots \\ \mathbf{y}_n \end{pmatrix}$, $\mathbf{X}=\begin{pmatrix}\mathbf{X}_1 \\ \mathbf{X}_2\\ \vdots \\ \mathbf{X}_n \end{pmatrix}$ and $\mathbf{E}=\begin{pmatrix}\boldsymbol{\epsilon}_1 \\ \boldsymbol{\epsilon}_2\\ \vdots \\ \boldsymbol{\epsilon}_n \end{pmatrix}$. The form \@ref(eq:SimpleCLRSystemVectorFinal) shows the connection between the complex regression and the conventional real valued one. As it can be seen, they both can be represented in matrix forms, which means that some of the approaches used for the latter can be transferred to the former under some conditions.


### Estimation
Whenever we estimate a model, we substitute the "true" parameters by their sample estimates. So, for the CLR, the model applied to the data should be written as:
\begin{equation}
    y_j = b_0 + b_1 x_j + e_j,
    (\#eq:SimpleCLRComplexEstimated)
\end{equation}
or
\begin{equation}
    y_{r,j}+i y_{i,j} = (b_{0,r} + i b_{0,i}) + (b_{1,r} + i b_{1,i}) (x_{r,j}+i x_{i,j}) + (e_{r,j} + i e_{i,j}),
    (\#eq:SimpleCLREstimated)
\end{equation}
or
\begin{equation}
    \begin{aligned}
        y_{r,j} = & b_{0,r} + b_{1,r} x_{r,j} - b_{1,i} x_i + e_{r,j} \\
        y_{i,j} = & b_{0,i} + b_{1,r} x_{i,j} + b_{1,i} x_r + e_{i,j}
    \end{aligned}
    (\#eq:SimpleCLRSystemEstimated)
\end{equation}
or equivalently in matrix notations:
\begin{equation}
    \mathbf{y}_j = \mathbf{X}_j \boldsymbol{b} + \boldsymbol{e}_j ,
    (\#eq:SimpleCLRSystemVector03Estimated)
\end{equation}
where $b_{l}$, $b_{l,r}+ib_{l,i}$ and $\boldsymbol{b}$ are the estimates of respective $\beta_l$, $\beta_{l,r} + i \beta_{l,i}$ and $\boldsymbol{\beta}$ and $e_j$, $e_{r,j} + i e_{i,j}$ and $\boldsymbol{e}_j$ are the residuals of the model. Now in order to get estimates of parameters, we can use one of several approaches.


#### Ordinary Least Squares
We start with the conventional Ordinary Least Squares (OLS), which relies on the multiplication by conjugate number. For the simple CLR, it comes to minimising the following loss based on the residuals:
\begin{equation}
    \sum_{j=1}^n (e_j \tilde{e}_j) = \sum_{j=1}^n (e_{r,j}^2 + e_{i,j}^2).
    (\#eq:SimpleCLROLSLoss)
\end{equation}
The residuals can be substituted as: $e_j = y_j - b_0 - b_1 x_j$ and $\tilde{e}_j = \tilde{y}_j - \tilde{b}_0 - \tilde{b}_1 \tilde{x}_j$ to get:
\begin{equation}
    \sum_{j=1}^n (y_j - b_0 - b_1 x_j) (\tilde{y}_j - \tilde{b}_0 - \tilde{b}_1 \tilde{x}_j),
    (\#eq:SimpleCLROLSLoss01)
\end{equation}
which after opening the brackets becomes:
\begin{equation}
    \sum_{j=1}^n \left(y_j \tilde{y}_j - y_j \tilde{b}_0 - y_j \tilde{b}_1 \tilde{x}_j - b_0\tilde{y}_j + b_0 \tilde{b}_0 + b_0 \tilde{b}_1 \tilde{x}_j - b_1 x_j \tilde{y}_j + b_1 x_j \tilde{b}_0 + b_1 x_j \tilde{b}_1 \tilde{x}_j \right) .
    (\#eq:SimpleCLROLSLoss02)
\end{equation}
In order to minimise the sum of squared errors \@ref(eq:SimpleCLROLSLoss), we need to take derivative of \@ref(eq:SimpleCLROLSLoss02) with respect to each of the parameters $b_{0,r}$, $b_{0,i}$, $b_{1,r}$ and $b_{1,i}$ and equate each of the resulting equations to zero to find the extrema:
\begin{equation}
    \begin{aligned}
        & \frac{d \sum_{j=1}^n (e_j \tilde{e}_j)}{d b_{0,r}} = 0 \\
        & \frac{d \sum_{j=1}^n (e_j \tilde{e}_j)}{d b_{0,i}} = 0 \\
        & \frac{d \sum_{j=1}^n (e_j \tilde{e}_j)}{d b_{1,r}} = 0 \\
        & \frac{d \sum_{j=1}^n (e_j \tilde{e}_j)}{d b_{1,i}} = 0
    \end{aligned}
    (\#eq:SimpleCLROLSLossSystem01)
\end{equation}
to get:
\begin{equation}
    \begin{aligned}
        & - \sum_{j=1}^n y_j - \sum_{j=1}^n \tilde{y}_j + 2 n b_{0,r} + \tilde{b}_1 \sum_{j=1}^n \tilde{x}_j + {b}_1 \sum_{j=1}^n {x}_j = 0 \\
        & i \sum_{j=1}^n y_j - i \sum_{j=1}^n \tilde{y}_j + 2 n b_{0,i} + i \tilde{b}_1 \sum_{j=1}^n \tilde{x}_j - i {b}_1 \sum_{j=1}^n {x}_j = 0 \\
        & - \sum_{j=1}^n y_j \tilde{x}_j + b_0 \sum_{j=1}^n \tilde{x}_j - \sum_{j=1}^n x_j \tilde{y}_j + \tilde{b}_0 \sum_{j=1}^n {x}_j + 2 b_{1,r} \sum_{j=1}^n x_j \tilde{x}_j = 0 \\
        & i \sum_{j=1}^n y_j \tilde{x}_j - i b_0 \sum_{j=1}^n \tilde{x}_j - i \sum_{j=1}^n x_j \tilde{y}_j + i \tilde{b}_0 \sum_{j=1}^n {x}_j + 2 b_{1,i} \sum_{j=1}^n x_j \tilde{x}_j = 0 .
    \end{aligned}
    (\#eq:SimpleCLROLSLossSystem02)
\end{equation}
As shown by @Svetunkov2012, solving the system of equations \@ref(eq:SimpleCLROLSLossSystem02) gives the following formulae for the parameters of the model:
\begin{equation}
    \begin{aligned}
        & b_{1} = \frac{\sum_{j=1}^n (y_{j}-\hat{\mu}_{y}) (\tilde{x}_j-\hat{\tilde{\mu}}_{x})}{\sum_{j=1}^n (x_{j}-\hat{\mu}_{x}) (\tilde{x}_j-\hat{\tilde{\mu}}_{x})} \\
        & b_0 = \frac{1}{n} \sum_{j=1}^n y_j - b_1 \sum_{j=1}^n x_j ,
    \end{aligned}
    (\#eq:SimpleCLROLSLossParameters)
\end{equation}
which can also be written in terms of conjugate moments as:
\begin{equation}
    \begin{aligned}
        & b_{1} = \frac{\hat{\sigma}_{x,y}}{\hat{\sigma}_x^2} \\
        & b_0 = \hat{\mu}_{y} - b_1 \hat{\mu}_{x} .
    \end{aligned}
    (\#eq:SimpleCLROLSLossParametersMoments)
\end{equation}
As we can see, the formula \@ref(eq:SimpleCLROLSLossParametersMoments) is similar to the one that is typically used for the conventional simple linear regression with the only difference that the parameters in \@ref(eq:SimpleCLROLSLossParametersMoments) are complex and that each of the moments in \@ref(eq:SimpleCLROLSLossParametersMoments) is a moment for respective complex variable.

It is apparent what the estimates of parameters \@ref(eq:SimpleCLROLSLossParametersMoments) correspond to: they minimise the loss \@ref(eq:SimpleCLROLSLoss), thus in the case, when $\mathrm{E}(e_j)=0$ they minimise variances of real and imaginary parts of the complex residuals. A thing to note is that they ignore the potential covariance between them, which in some cases might be a desirable property, but in the others might cause issues.


#### Complex Least Squares
An alternative estimation technique involves the minimisation of a rather exotic loss function:
\begin{equation}
    \sum_{j=1}^n (e_j e_j) = \sum_{j=1}^n (e_{r,j}^2 - e_{i,j}^2 + i 2 e_{r,j} e_{i,j}),
    (\#eq:SimpleCLRCLSLoss)
\end{equation}
for which in case of $\mathrm{E}(e_j)=0$, the real part corresponds to the difference between the variances of the real and imaginary parts of the residuals, while the imaginary parts corresponds to the covariance between them. This loss be considered exotic, because its value is a complex number. It is difficult to explain how one can minimise a complex number, but from what follows, we show that the estimation technique has some meaning, works and gives adequate estimates of parameters.

The logic for the derivation of CLS is similar to OLS. We expand \@ref(eq:SimpleCLRCLSLoss) to:
\begin{equation}
    \sum_{j=1}^n (e_j e_j) = \sum_{j=1}^n y_j^2 + n b_0^2 + b_1^2 \sum_{j=1}^n x_j^2 - 2 b_0 \sum_{j=1}^n y_j - 2 b_1 \sum_{j=1}^n x_j y_j + 2 b_0 b_1 \sum_{j=1}^n x_j 
    (\#eq:SimpleCLRCLSLoss02)
\end{equation}
and then take derivatives of \@ref(eq:SimpleCLRCLSLoss02) with respect to parameter $b_0$ and $b_1$ and then equate the resulting equations to zero:
\begin{equation}
    \begin{aligned}
        & \frac{d \sum_{j=1}^n (e_j^2)}{d b_{0}} = 0 \\
        & \frac{d \sum_{j=1}^n (e_j^2)}{d b_{1}} = 0 .
    \end{aligned}
    (\#eq:SimpleCLRCLSLossSystem01)
\end{equation}
The derivatives \@ref(eq:SimpleCLRCLSLossSystem01) give the following system of complex equations:
\begin{equation}
    \begin{aligned}
        & 2 n b_0 - 2 \sum_{j=1}^n y_j + 2 b_1 \sum_{j=1}^n x_j = 0 \\
        & 2 b_1 \sum_{j=1}^n x_j^2 - 2 \sum_{j=1}^n x_j y_j + 2 b_0 \sum_{j=1}^n x_j = 0 .
    \end{aligned}
    (\#eq:SimpleCLRCLSLossSystem02)
\end{equation}
The solution for this system of equations, as it was shown by @Svetunkov2012, is:
\begin{equation}
    \begin{aligned}
        & b_{1} = \frac{\sum_{j=1}^n (y_{j}-\hat{\mu}_{y}) ({x}_j-\hat{\mu}_{x})}{\sum_{j=1}^n (x_{j}-\hat{\mu}_{x})^2} \\
        & b_0 = \frac{1}{n} \sum_{j=1}^n y_j - b_1 \sum_{j=1}^n x_j ,
    \end{aligned}
    (\#eq:SimpleCLRCLSLossParameters)
\end{equation}
or in terms of direct moments for complex random variables:
\begin{equation}
    \begin{aligned}
        & b_{1} = \frac{\hat{\varsigma}_{x,y}}{\hat{\varsigma}_x^2} \\
        & b_0 = \hat{\mu}_{y} - b_1 \hat{\mu}_{x} .
    \end{aligned}
    (\#eq:SimpleCLRCLSLossParametersMoments)
\end{equation}

In order to better understand what is specifically minimised, when the formulae \@ref(eq:SimpleCLRCLSLossParametersMoments) are used for the estimation of parameters, we conduct a small experiment in R with the following code:

```{r echo=FALSE}
load("./data/CLSMDS.Rdata")
```

```{r eval=FALSE}
# Create real part of a c.r.v. x
xr <- rnorm(1000,0,10)
# Create a c.r.v. x
x <- complex(real=xr, imaginary=1.5*xr+rnorm(1000,0,10))
# Create a c.r.v. y
y <- (1.5 + 1.2i) * x +
    complex(real=rnorm(1000,0,10), imaginary=rnorm(1000,0,10))

# Define number of iterations and the matrix with the values
nsim <- 10000
clsValues <- matrix(NA, nsim, 4,
                    dimnames=list(NULL,
                                  c("b1r","b1i","CLSr","CLSi")))

# CLS loss function
clsLoss <- function(y, yHat){
    return(sum((y - yHat)^2))
}

# Loop for values of b1r from 0.52 to 2.5 and
# for b1i from 0.22 to 2.2
l <- 1
for(i in 1:(nsim/100)){
    for(j in 1:(nsim/100)){
        b <- complex(real=i/100*2+0.5, imaginary=j/100*2+0.2)
        yHat <- (10 + 15i) + b * x
        clsResult <- clsLoss(y, yHat)
        clsValues[l,1] <- Re(b);
        clsValues[l,2] <- Im(b);
        clsValues[l,3] <- Re(clsResult);
        clsValues[l,4] <- Im(clsResult);
        l <- l+1;
    }
}

# Estimate b1 using CLS
bOptimal <- ccov(x, y, method="direct") /
            cvar(x, method="direct")
# Produce fitted values
yHat <- bOptimal * x
# Calculate the loss
clsResult <- clsLoss(y, yHat)
```

In this experiment we generate complex variables $x$ and $y$ and then calculate values of the complex loss function based on a combination of values of the complex slope $b_1$ (which we call just `b` in the code). After running the code above we end up with 10,000 values of the loss function and one additional, which corresponds to the optimal point according to \@ref(eq:SimpleCLRCLSLossParametersMoments). We can then produce several plots to better understand what is happening.

```{r clsScatter, fig.cap="Variety of CLS loss function values for different values of $b_1$."}
plot(clsValues[,3:4], type="p",
     xlab="Re(CLS loss)", ylab="Im(CLS loss)")
abline(h=0, col="grey", lty=2, lwd=2)
abline(v=0, col="grey", lty=2, lwd=2)
points(Re(clsResult), Im(clsResult), col="red", pch=19)
```

Figure \@ref(fig:clsScatter) shows a scatterplot of values of the complex loss \@ref(eq:SimpleCLRCLSLoss). The red point close to the origin corresponds to the estimate obtained using \@ref(eq:SimpleCLRCLSLossParametersMoments). As we can see, it is close to the origin, which implies that the minimisation of the loss \@ref(eq:SimpleCLRCLSLoss) is equivalent to making both real and imaginary parts of it close to zero. This means that CLS makes variances of real and imaginary residuals similar and shrinks the covariance between them to zero.

Another way of looking at how CLS works is to reduce its dimensionality via the Multidimensional Scaling [@Gower] and then visualise it. The R code below is relatively slow but produces a solution for this task.

```{r eval=FALSE}
# Calculate distance matrix for all losses
# (including the CLS point)
clsMDSDistance <- dist(rbind(clsValues[,3:4],
                             c(Re(clsResult),Im(clsResult))))

# Do Multidimensional scaling
clsMDS <- cmdscale(clsMDSDistance, k=1)

# Create a data frame with the coordinates
clsMDSValues <- data.frame(z=clsMDS,
                           x=c(clsValues[,1],Re(bOptimal)),
                           y=c(clsValues[,2],Im(bOptimal)))

# Create a matrix with loss values for 3d plotting
clsMDSValuesZ <- matrix(clsMDSValues$z[-10001], 100, 100)
```

Then, to produce the 3d surface on the plane of "loss - $b_{1,r}$ - $b_{1,i}$, we can use functions from the `plotly` package in R as shown in the code below:

```{r eval=FALSE}
plot_ly(z=clsMDSValuesZ,
        x=unique(clsMDSValues$x[-10001]),
        y=unique(clsMDSValues$y[-10001])) |>
    plotly::layout(scene=list(xaxis = list(title = "Re(b1)"),
                      yaxis = list(title = "Im(b1)"),
                      zaxis = list(title = "MDS of CLS loss"))) |>
    add_surface() |>
    add_trace(data = tail(clsMDSValues,1),
              x = ~x,
              y = ~y,
              z = ~z,
              mode = "markers",
              type = "scatter3d",
              marker = list(size = 10))
```

```{r plotlyCLS, echo=FALSE, fig.cap="Plot of the 3d surface for the MDS of the CLS loss."}
knitr::include_graphics("images/CLSPlot.png")
```

Figure \@ref(fig:plotlyCLS) demonstrates how the projection of the two-dimensional loss behaves for a variety of parameters of the regression. The orange point in the middle corresponds to the one obtained via CLS. We can see that it corresponds to the loss being close to zero, and represents a 2-dimensional inflection point, where the surface bends in different directions.

So, we can see that despite being counter-intuitive, the CLS produces the estimates of parameters that lead to the residuals being close to spherical (normal if we assume normality), because the variances of real and imaginary parts of the residuals become closer to each other, while the covariance between them becomes close to zero.


#### Likelihood
Finally, another way of estimating the simple CLR is by assuming a distribution of the residuals and maximising the respective likelihood. Given the connection between the linear and the vector forms of the complex regression, we can assume that the error term follows a multivariate normal distribution with a covariance matrix:
\begin{equation}
    \boldsymbol{\Sigma}_\epsilon = \begin{pmatrix} \sigma_{\epsilon_r}^2 & \sigma_{\epsilon_r, \epsilon_i} \\ \sigma_{\epsilon_r, \epsilon_i} & \sigma_{\epsilon_i}^2 \end{pmatrix} .
    (\#eq:SimpleCLRLikelihoodSigma)
\end{equation}
The log-likelihood in this case can be written as:
\begin{equation}
	\ell(\boldsymbol{\theta}, \boldsymbol{\Sigma}_\epsilon | \mathbf{Y}) = -\frac{n}{2} \left( 2 \log(2 \pi) + \log | \boldsymbol{\Sigma}_\epsilon| \right) -\frac{1}{2} \sum_{j=1}^n \left( \boldsymbol{\epsilon}_j^\prime \boldsymbol{\Sigma}_\epsilon^{-1} \boldsymbol{\epsilon}_j \right) ,
    (\#eq:additiveLogLik)
\end{equation}
where $\boldsymbol{\theta}$ is the vector of estimated parameters and $\boldsymbol{\epsilon}_j = \begin{pmatrix} \epsilon_{r,j} \\  \epsilon_{i,j} \end{pmatrix}$ is the two dimensional error term. This likelihood is maximised when the covariance matrix \@ref(eq:SimpleCLRLikelihoodSigma) is estimated via:
\begin{equation}
	\hat{\boldsymbol{\Sigma}}_\epsilon = \frac{1}{n} \sum_{j=1}^{n} \boldsymbol{e}_j \boldsymbol{e}_j^\prime ,
    (\#eq:Sigmaest)
\end{equation}
where $\prime$ denotes transposition of a matrix. It can be shown that if the \@ref(eq:Sigmaest) is inserted in \@ref(eq:additiveLogLik) then the following concentrated log-likelihood can be obtained [see, for example, @Snyder2017]:
\begin{equation}
	\ell(\boldsymbol{\theta}, \hat{\boldsymbol{\Sigma}}_\epsilon | \mathbf{Y}) = -\frac{n}{2} \left( 2 \log(2 \pi e) + \log | \hat{\boldsymbol{\Sigma}}_\epsilon | \right) .
	(\#eq:additiveLogLikConcentrated)
\end{equation}
It is obvious that the maximisation of the log-likelihood \@ref(eq:additiveLogLikConcentrated) is equivalent to minimising the generalised variance of the complex error term (as discussed in Subsection \@ref(crvSecondMoment)):
\begin{equation}
    \mathrm{GV} = |\hat{\boldsymbol{\Sigma}}_\epsilon| = \sigma_{\epsilon_r}^2 \sigma_{\epsilon_i}^2 - \sigma_{\epsilon_r, \epsilon_i}^2 .
    (\#eq:additiveLogLikConcentratedGV)
\end{equation}
The minimisation of GV in its turn implies the joint minimisation of variances of the real and imaginary part of the residuals and a maximisation of the square of the covariance between them, thus making the residuals more linearly related. This loss function might be especially useful if we indeed can assume that the real and imaginary part of the residuals are linearly related and we want to use this information in the estimation.


#### Comparing different losses



Number of estimated parameters is k, number of degrees of freedom per series is k/2.


### Inference




## Correlations analysis of complex random variables
When it comes to measuring associations between variables, most frequently analysts use coefficient of correlation. While it is straightforward for real-valued variables, for complex variables this become challenging, because each c.r.v. has two parts, so the correlation needs to take them both into account.

For modelling purposes, it might be useful to have the information about all possible correlations involved relation of two c.r.v. This comes to analysing the following covariances:

1. $\sigma_{x_r,x_i}$,
2. $\sigma_{y_r,y_i}$,
3. $\sigma_{x_r,y_r}$,
4. $\sigma_{x_i,y_i}$,
5. $\sigma_{x_r,y_i}$,
6. $\sigma_{y_r,x_i}$.


### Visualisation of relations {#correlationVisual}
In order to better understand what correlations between c.r.v. imply, we need to understand how to produce scatterplots for them. While in case of two real variables it is straightforward (a variable per axes), in our situation, this becomes challenging. We propose considering a set of scatterplots shown in Figure \@ref(fig:crvScatterplots) for two generated complex random variables $x$ and $y$, created using `cplot()` function from `complex` package in R.

```{r crvScatterplots, fig.width=4, fig.height=4, fig.cap="Visualisation of relations between two complex variables"}
# Create real part of a c.r.v. x
xr <- rnorm(1000,0,10)
# Create a c.r.v. x
x <- complex(real=xr, imaginary=1.5*xr+rnorm(1000,0,10))
# Create a c.r.v. y
y <- (10 + 15i) + (1.5 + 1.2i) * x +
    complex(real=rnorm(1000,0,10), imaginary=rnorm(1000,0,10))
# Produce the plot
cplot(x, y, which=1)
```

This scatterplot has several important elements in it:

- It shows relations between real and imaginary parts of each variable (e.g. the two scatterplots in the bold dark red frame),
- It shows cross-relations between parts of one variable and parts of the other one (e.g. the rest four plots),
- The colour shows ordering of the original variable $x$ with dark values corresponding to point with higher magnitude and light ones being closer to zero. This way, we can see what the original points in $x$ correspond to in $y$.

The plots are positioned to satisfy two rules:

1. When a scatterplot for a c.r.v. is produced, the real part should be in x-axis, while the imaginary should be in the y-axis.
2. When parts of variables $x$ and $y$ are compared, the part for $x$ should be in x-axis, while the part for $y$ should be in y-axis, which should the reflect the idea that $x$ could be an explanatory variable for $y$.

While a simple scatterplot matrix could have been constructed instead of Figure \@ref(fig:crvScatterplots), we argue that the latter has a logical grouping and should be preferred for analysis of complex variables. For example, based on the plots in Figure \@ref(fig:crvScatterplots) we can conclude that:

- There is a positive relation between the real and imaginary parts of $x$,
- There is a negative relation between the real and imaginary parts of $y$,
- Real parts of $x$ and $y$ do not exhibit a strong linear relation,
- But the respective imaginary parts of $x$ and $y$ have the positive relation between them,
- Finally, we see that with the increase of real and imaginary parts of $x$, the real part of $y$ decreases, while the imaginary one increases. This sort of behaviour implies positive complex slope in the potential linear regression (discussed in Section \@ref(simpleCLR)).

We think that this visualisation is useful when analysing relations between two complex random variables. But it also shows how complicated it is to capture the relation between them and how many aspects need to be considered.

As an alternative to the plot above, it is also possible to use some dimensionality reduction techniques to plot complex variables on a two dimensional plot. For example, we can use Multidimensional Scaling for this [MDS, @refMDS] to create a project of one complex variable on x-axis and another one on the y-axis. In R, we can use the `cmdscale()` function from `stats` package for this (in the example below, we use euclidean distance for the dissimilarities matrix via `dist()` function from `stats`, and we use the `complex2vec()` function from `complex` package to transform complex variable to a collection of vectors):

```{r, eval=FALSE}
xScaled <- cmdscale(dist(complex2vec(x)), k=1)
yScaled <- cmdscale(dist(complex2vec(y)), k=1)
plot(xScaled,yScaled)
```

The same code is implemented in `cplot()` function from `complex` package in R:

```{r crvScatterplotMDS, fig.width=4, fig.height=4, fig.cap="Scatterplot of MDS of two complex variables."}
cplot(x, y, which=2, main="")
```

The plot in Figure \@ref(fig:crvScatterplotMDS) is much easier to read than the collection of scatterplots in Figure \@ref(fig:crvScatterplots), and in our example, we can conclude that the two complex variables exhibit strong linear relation.

<!-- The only thing to note is that due to the nature of MDS, the direction of the relation and the strength of the relationship might be lost during scaling. -->


### Conjugate and Direct correlations
The literature knows two correlation coefficients for complex variables [@ref]: the conjugate and the direct correlation (the latter is known in the literature as "pseudo-correlation"). Their formula are based on the respective covariances and variances (conjugate and direct discussed in Subsection \@ref(crvSecondMoment)):

1. Conjugate correlation
\begin{equation}
    \rho_{x,y} = \frac{\sqrt{\sigma_{x,y} \sigma_{y,x}}}{\sigma_x \sigma_y},
    (\#eq:correlationConventional)
\end{equation}

2. Direct correlation
\begin{equation}
    \varrho_{x,y} = \frac{\varsigma_{x,y}}{\varsigma_x \varsigma_y}.
    (\#eq:correlationPseudo)
\end{equation}

Note that the conjugate correlation has the geometric mean of standard deviations in the numerator. This is needed because of the issue with the conjugate covariance (its value changes with the change of conjugate number). If we use only one of covariances [as done, for example, by @Panchev1971] then the value of correlation coefficient will be ambiguous, implying that the correlation between $x$ and $y$ differs from the correlation between $y$ and $x$. Furthermore, such correlation coefficient would not work as intended. For example, if we have a positive functional linear relation between $x$ and $y$, the coefficient should be equal to one. But as an R example below demonstrates this value is obtained only if we have the geometric mean of covariances.

```{r}
x <- complex(real=rnorm(100,0,10), imaginary=rnorm(100,0,10))
y <- (10 + 15i) + (1 + 1i) * x
# Variant 1
ccov(y,x,method="conj") /
    sqrt(cvar(x,method="conj")*cvar(y,method="conj"))
# Variant 2
ccov(x,y,method="conj") /
    sqrt(cvar(x,method="conj")*cvar(y,method="conj"))
# Variant 3 (correct conjugate correlation)
sqrt(ccov(y,x,method="conj")*ccov(x,y,method="conj")) /
    sqrt((cvar(x,method="conj")*cvar(y,method="conj")))
```

The formulae \@ref(eq:correlationConventional) and \@ref(eq:correlationPseudo) are derived based on the original definition of Pearson's correlation coefficient [@refPearson]. It follows from the idea that the correlation coefficient equals to the geometric mean of slopes of two regression lines:
\begin{equation}
    \begin{aligned}
        &y = \beta_0 + \beta_1 x + \epsilon \\
        &x = \alpha_0 + \alpha_1 y + \upsilon ,
    \end{aligned}
    (\#eq:twoRegressions)
\end{equation}
where $\alpha_0$ and $\beta_0$ are the intercepts, $\alpha_1$ and $\beta_1$ are the slopes of the regression lines and $e$ and $u$ are the residuals of the models. Note that all of these variables and parameters in our case are complex. As discussed in Section \@ref(simpleCLR), the parameters of slope can be estimated using either Ordinary Least Squares (OLS), or the Complex Least Squares (CLS). For the OLS, the formulae for the slopes are:
\begin{equation}
    \begin{aligned}
        &b_1 = \frac{\hat{\sigma}_{x,y}}{\hat{\sigma}_x} \\
        &a_1 = \frac{\hat{\sigma}_{y,x}}{\hat{\sigma}_y} .
    \end{aligned}
    (\#eq:twoRegressionsOLS)
\end{equation}
Taking their geometric means leads to:
\begin{equation}
    \hat{\rho}_{x,y} = \sqrt{a_1 b_1},
    (\#eq:correlationConventionalEstimate)
\end{equation}
which then leads to the formula \@ref(eq:correlationConventional). Similarly, for CLS estimated regressions, we have:
\begin{equation}
    \begin{aligned}
        &b_1 = \frac{\hat{\varsigma}_{x,y}}{\hat{\varsigma}_x} \\
        &a_1 = \frac{\hat{\varsigma}_{x,y}}{\hat{\varsigma}_y} ,
    \end{aligned}
    (\#eq:twoRegressionsCLS)
\end{equation}
which after taking the same geometric means leads to \@ref(eq:correlationPseudo). Note that the estimates of the slope parameters will differ between the OLS and the CLS and thus the direct and conjugate correlations will differ as well.


### Conjugate correlation
When it comes to the interpretation of the coefficients, the conjugate one is a real number. It can be written as:
\begin{equation}
    {\rho}_{x,y} = \frac{\sqrt{(\sigma_{x_r, y_r} + \sigma_{x_i, y_i})^2 + (\sigma_{x_i, y_r} - \sigma_{x_r, y_i})^2}}{\sqrt{(\sigma_{x_r}^2 + \sigma_{x_i}^2)(\sigma_{y_r}^2 + \sigma_{y_i}^2)}} .
    (\#eq:correlationConventionalExpanded)
\end{equation}
<!-- As can be seen from the formula \@ref(eq:correlationConventionalExpanded), the coefficient becomes a real number only when the cross-covariances between the real and imaginary parts of $x$ and $y$ are equal. In practice, this exotic condition can be met, when the parts for both variables are not correlated. In all the other cases, the resulting correlation coefficient will be a complex number. -->
In general, the real part of the conjugate correlation $\rho_{x,y}$ shows the average strength of linear relation between two complex variables $x$ and $y$.
<!-- , while the imaginary part of $\rho_{x,y}$ shows the difference between linear relations between the cross values of real and imaginary parts of $x$ and $y$. -->
The coefficient will be equal to zero, when there are no linear relations between the parts of complex variables $x$ and $y$. On the other hand, its real part will be close to one by absolute value if the complex relation between variables $y$ and $x$ is close to linear, i.e. $a_1 = \frac{1}{b_1}$.

In order to better understand what the real and imaginary parts of the conjugate correlation mean, we expand the formula \@ref(eq:correlationConventionalEstimate) by substituting $b_1 = b_{1,r} + i b_{1,i}$ and $a_1 = a_{1,r} + i a_{1,i}$:
\begin{equation}
    \begin{aligned}
        \hat{\rho}_{x,y} = & \sqrt{(a_{1,r} + i a_{1,i}) (b_{1,r}+ib_{1,i})} = \\
        & \sqrt{a_{1,r} b_{1,r} - a_{1,i} b_{1,i} + i(a_{1,r} b_{1,i} + a_{1,i} b_{1,r})},
    \end{aligned}
    (\#eq:correlationConjugateExpanded01)
\end{equation}
or in the exponential form:
\begin{equation}
    \hat{\rho}_{x,y} = R^{\frac{1}{2}} e^{i \frac{1}{2} \phi} ,
    (\#eq:correlationConjugateExpanded02)
\end{equation}
where
\begin{equation}
    \begin{aligned}
        & R = \sqrt{(a_{1,r} b_{1,r} - a_{1,i} b_{1,i})^2 + (a_{1,r} b_{1,i} + a_{1,i} b_{1,r})^2} \\
        & \phi=\arctan\left(\frac{a_{1,r} b_{1,i} + a_{1,i} b_{1,r}}{a_{1,r} b_{1,r} - a_{1,i} b_{1,i}}\right),
    \end{aligned}
    (\#eq:correlationConjugateExpanded03)
\end{equation}
As can be seen from \@ref(eq:correlationConjugateExpanded03), there is a multitude of combinations of parameters of the model that can give the unity magnitude $R$. For example, if $a_{1,r} = 0.5$, $b_{1,r} = 1$, $a_{1,i} = -0.5$ and $b_{1,i} = 1$, $R$ would be equal to one. Similarly, there is a multitude of values of parameters that would give angles of $0$ and $\pi$, leading to positive or negative real numbers. In fact, for the example above, $\phi=0$, implying that the correlation coefficient will be equal to one, implying that the variables $x$ and $y$ exhibit a strong linear relation.

An R example of a conjugate correlation (via `ccor()` function from `complex` package) with the aforementioned values of parameters is shown below and in Figure \@ref(fig:crvCorConjugate).

```{r crvCorConjugate, fig.width=4, fig.height=4, fig.cap="Two complex variables with conjugate correlation being close to one."}
# Create a c.r.v. x
x <- complex(real=rnorm(100,0,10), imaginary=rnorm(100,0,10))
# Create a c.r.v. y
y <- (10 + 15i) + (1 + 1i) * x +
    complex(real=rnorm(100,0,1), imaginary=rnorm(100,0,1))
# Produce the plot
cplot(x, y, main="")
# Conjugate correlation
ccor(x, y, method="conjugate")
```

As can be seen from the Figure \@ref(fig:crvCorConjugate) and the value of the conjugate correlation, there is a relation between the two complex variables $x$ and $y$. The conjugate correlation says that this is a strong linear relation, and one of ways how we can check this is by analysing the MDS of the two variables.

```{r crvCorConjugateMDS, fig.width=4, fig.height=4, fig.cap="Visualisation of relations between two complex variables"}
cplot(x, y, which=2, main="")
```

As can be seen from the plot in Figure \@ref(fig:crvCorConjugateMDS), it seems that the variables indeed exhibit a strong linear relation. So, the conjugate correlation has provided an adequate information about it.



### Direct correlation
The direct correlation can be expanded to:
\begin{equation}
    {\varrho}_{x,y} = \frac{\sigma_{x_r, y_r} - \sigma_{x_i, y_i} + i (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})}{\sqrt{(\sigma_{x_r}^2 - \sigma_{x_i}^2 + i2 \sigma_{x_r,x_i})(\sigma_{y_r}^2 - \sigma_{y_i}^2 + i2 \sigma_{y_r,y_i})}}.
    (\#eq:correlationPseudoExpanded)
\end{equation}
Given that it contains a complex number in the denominator, it is more complicated than the conjugate one. But it has several apparent properties:

1. The magnitude of the coefficient will be equal to zero (and thus the coefficient will be equal to zero) only when $\sigma_{x_i,y_r}=\sigma_{x_r,y_i}=0$ and $\sigma_{x_r,y_r}=\sigma_{x_i,y_i}$. One of the special cases of this is when all cross-covariances between the real and imaginary parts of $x$ and $y$ are equal to zero.

2. When the complex variables $x$ and $y$ have positive functional relation between them, so that $a_1 = \frac{1}{b_1}$, the coefficient will be equal to one.

Note however that due to the division by a complex number, the value of 1 can also be obtained due to the values of direct variance of variables $x$ and $y$.

Father's stuff here.

<!-- To get other insights about the direct correlation coefficient, we express its denominator in the exponential form: -->
<!-- \begin{equation} -->
<!--     {\sqrt{(\sigma_{x_r}^2 - \sigma_{x_i}^2 + i2 \sigma_{x_r,x_i})(\sigma_{y_r}^2 - \sigma_{y_i}^2 + i2 \sigma_{y_r,y_i})}} = R_1^{\frac{1}{2}} e^{i \frac{\phi_1}{2}}, -->
<!--     (\#eq:correlationPseudoExpandedExp01) -->
<!-- \end{equation} -->
<!-- where -->
<!-- \begin{equation} -->
<!--     \resizebox{0.8\textwidth}{!}{$ -->
<!--     \begin{aligned} -->
<!--         R_1 = & \sqrt{\left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) - 4 \sigma_{x_r,x_i} \sigma_{y_r,y_i} \right)^2+ 4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2} \\ -->
<!--         \phi_1 = &\arctan \left(\frac{2 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)}{(\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) - 4 \sigma_{x_r,x_i} \sigma_{y_r,y_i}}\right) , -->
<!--     \end{aligned} $} -->
<!--     (\#eq:correlationPseudoExpandedExpRPhi) -->
<!-- \end{equation} -->
<!-- which are obtained by opening the brackets inside the square root of \@ref(eq:correlationPseudoExpandedExp01). If we now insert \@ref(eq:correlationPseudoExpandedExp01) in \@ref(eq:correlationPseudoExpanded) and multiply both numerator and denominator by conjugate number to the \@ref(eq:correlationPseudoExpandedExp01) we will get: -->
<!-- \begin{equation} -->
<!--     {\varrho}_{x,y} = \frac{\left(\sigma_{x_r, y_r} - \sigma_{x_i, y_i} + i (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})\right)e^{-i \frac{\phi_1}{2}}}{R_1^{\frac{1}{2}}}. -->
<!--     (\#eq:correlationPseudoExpandedExp02) -->
<!-- \end{equation} -->
<!-- Representing the $\sigma_{x_r, y_r} - \sigma_{x_i, y_i} + i (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})$ as $R_2 e^{i \phi_2}$, where $R_2= \sqrt{(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 +  (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2}$ and $\phi_2= \arctan \left( \frac{\sigma_{x_i, y_r} + \sigma_{x_r, y_i}}{\sigma_{x_r, y_r} - \sigma_{x_i, y_i}} \right)$, and inserting these values in \@ref(eq:correlationPseudoExpandedExp02) we get: -->
<!-- \begin{equation} -->
<!--     {\varrho}_{x,y} = \frac{R_2}{\sqrt{R_1}} e^{i \left(\phi_2 - \frac{\phi_1}{2} \right)}. -->
<!--     (\#eq:correlationPseudoExpandedExp03) -->
<!-- \end{equation} -->
<!-- or in an even shorter exponential form ${\varrho}_{x,y} = |{\varrho}_{x,y}| e ^{i \arg({\varrho}_{x,y})}$, where: -->
<!-- \begin{equation} -->
<!--     \resizebox{0.85\textwidth}{!}{$ -->
<!--     |{\varrho}_{x,y}|= \frac{R_2}{\sqrt{R_1}} = \sqrt{\frac{(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 +  (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2}{\sqrt{\left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) - 4 \sigma_{x_r,x_i} \sigma_{y_r,y_i} \right)^2+ 4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2}}}$} -->
<!--     (\#eq:correlationPseudoExpandedExpR) -->
<!-- \end{equation} -->
<!-- and  -->
<!-- \begin{equation} -->
<!--     \resizebox{0.85\textwidth}{!}{$ -->
<!--     \arg({\varrho}_{x,y}) = \left(\phi_2 - \frac{\phi_1}{2} \right) = \arctan \left( \frac{\sigma_{x_i, y_r} + \sigma_{x_r, y_i}}{\sigma_{x_r, y_r} - \sigma_{x_i, y_i}} \right) - \frac{1}{2}\arctan \left(\frac{2 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)}{(\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) - 4 \sigma_{x_r,x_i} \sigma_{y_r,y_i}}\right) $}. -->
<!--     (\#eq:correlationPseudoExpandedExpPhi) -->
<!-- \end{equation} -->
<!-- Analysing \@ref(eq:correlationPseudoExpandedExpR) \@ref(eq:correlationPseudoExpandedExpPhi), we can identify several conditions that lead to specific values of the direct correlation coefficient: -->


<!-- Trying to derive |rho|=1 -->
<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 +  (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 = \sqrt{\left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) - 4 \sigma_{x_r,x_i} \sigma_{y_r,y_i} \right)^2+ 4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2}$ -->

<!-- $\left((\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 +  (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 \right)^2 = \left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) - 4 \sigma_{x_r,x_i} \sigma_{y_r,y_i} \right)^2+ 4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2$ -->

<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^4 + 2 (\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 + (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^4 = \left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) - 4 \sigma_{x_r,x_i} \sigma_{y_r,y_i} \right)^2+ 4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2$ -->

<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^4 + 2 (\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 + (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^4 = \left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) \right)^2 - 8 (\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \sigma_{y_r,y_i} + \left(4 \sigma_{x_r,x_i} \sigma_{y_r,y_i}\right)^2 +    4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2$ -->

<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^4 + 2 (\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 + (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^4 = \left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) \right)^2 - 8 (\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \sigma_{y_r,y_i} + \left(4 \sigma_{x_r,x_i} \sigma_{y_r,y_i}\right)^2 + 4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} \right)^2  + 8(\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} + 4\left((\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2$ -->

<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^4 + 2 (\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 + (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^4 = \left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) \right)^2 + \left(4 \sigma_{x_r,x_i} \sigma_{y_r,y_i}\right)^2 + 4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} \right)^2 + 4\left((\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2$ -->


<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^4 + 2 (\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 + (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^4 = (\sigma_{x_r}^2 - \sigma_{x_i}^2)^2\left( (\sigma_{y_r}^2 - \sigma_{y_i}^2)^2 + 4  \sigma_{y_r,y_i}^2 \right) + 4 \sigma_{x_r,x_i}^2 \left((\sigma_{y_r}^2 - \sigma_{y_i}^2)^2 + \sigma_{y_r,y_i}^2 \right)$ -->

<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^4 + 2 (\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 + (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^4 = (\sigma_{x_r}^2 - \sigma_{x_i}^2)^2\left( (\sigma_{y_r}^2 - \sigma_{y_i}^2)^2 + 4  \sigma_{y_r,y_i}^2 \right) + 4 \sigma_{x_r,x_i}^2 \left((\sigma_{y_r}^2 - \sigma_{y_i}^2)^2 + 4 \sigma_{y_r,y_i}^2 \right) - 12 \sigma_{x_r,x_i}^2 \sigma_{y_r,y_i}^2$ -->

<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^4 + 2 (\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 + (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^4 = \left(\left(\sigma_{x_r}^2 - \sigma_{x_i}^2\right)^2 + 4 \sigma_{x_r,x_i}^2 \right) \left( (\sigma_{y_r}^2 - \sigma_{y_i}^2)^2 + 4  \sigma_{y_r,y_i}^2 \right) - 12 \sigma_{x_r,x_i}^2 \sigma_{y_r,y_i}^2$ -->


### Pearson's correlation
We can also use MDS to analyse the projections of complex variables on x and y axes, similar to how we have done that in Subsection \@ref(correlationVisual). In that case, we can calculate Pearson's correlation coefficient:

```{r}
# Create projections of two complex variables onto axes
xScaled <- cmdscale(dist(complex2vec(x)), k=1)
yScaled <- cmdscale(dist(complex2vec(y)), k=1)
# Calculate the correlation coefficient
cor(xScaled,yScaled)
```

Or using the `ccor()` function with `method="pearson"`, which does exactly the same thing in one line of code:

```{r, eval=FALSE}
ccor(x, y, method="pearson")
```

The interpretation of the coefficient is straightforward and follows the conventional interpretation taught in any Statistics module. Note however that in general in the optimisation phase of MDS, it can reach the local optimum and not being able to get to the global one. As a result, the sign of the correlation might not represent the real relation between the two complex variables and in general should be ignored. Furthermore, inevitably when we move from four dimensions to two, we loose some information, so this approach is prone to possible mistakes and should be used with care. Finally, MDS is computationally expensive, especially on the long series. This means that in some cases it might take plenty of time before we get the Pearson's correlation value. Nonetheless, this is an additional way of analysing relations between complex variables.


### Correlation matrix
```{r}
cov2cor(covar(cbind(x,y)))
```



## Multiple CLR
### Model formulation 

### Estimation

### Inference

### Diagnostics

### Forecasting

### Examples of application (Production functions)
