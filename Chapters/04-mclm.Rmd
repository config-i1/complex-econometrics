# Multiple Complex Linear Regression {#multipleCLR}
We now move to the discussion of the multiple CLR, the model that captures relations between one complex random variable, $y_r + i y_i$ and a set of explanatory complex random variables.

## Model formulation 
Similarly to how the multiple linear regression is formulated for real valued variables, the multiple complex linear regression can be written as:
\begin{equation}
    \underline{y_j} = \underline{\beta_0} + \underline{\beta_1} \underline{x_{1,j}} + \underline{\beta_2} \underline{x_{2,j}} + \dots + \underline{\beta_k} \underline{x_{k,j}} + \underline{\epsilon_j},
    (\#eq:MultipleCLRComplex)
\end{equation}
where $k$ is the number of complex random variables. Similarly to how it was done with SCLR in \@ref(eq:SimpleCLRSystem), we can expand the formula \@ref(eq:MultipleCLRComplex) as a system of two equations, taking that every parameter and every variable in \@ref(eq:MultipleCLRComplex) is complex:
\begin{equation}
    \begin{aligned}
        y_{r,j} = & \beta_{0,r} + \beta_{1,r} x_{1,r,j} - \beta_{1,i} x_{1,i,j} + \dots + \beta_{k,r} x_{k,r,j} - \beta_{k,i} x_{k,i,j} + \epsilon_{r,j} \\
        y_{i,j} = & \beta_{0,i} + \beta_{1,r} x_{1,i,j} + \beta_{1,i} x_{1,r,j} + \dots + \beta_{k,r} x_{k,i,j} + \beta_{k,i} x_{k,r,j} + \epsilon_{i,j} .
    \end{aligned}
    (\#eq:MultipleCLRSystem)
\end{equation}
As can be seen from \@ref(eq:MultipleCLRSystem), the multiple CLR captures more complex dynamics than the conventional multiple linear regression. Both parts of the system use the same set of parameters and explanatory variables, but in different combinations, resulting in a versatile modelling framework.

This system can be represented in a more compact form, similarly to \@ref(eq:SimpleCLRVector):
\begin{equation}
    \underline{\mathbf{y}} = \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\boldsymbol{\epsilon}} ,
    (\#eq:CLRVector)
\end{equation}
where now $\underline{\mathbf{X}} = \begin{pmatrix} 1 & \underline{{x}_{1,1}} & \dots & \underline{{x}_{k,1}} \\ 1 & \underline{{x}_{1,2}} & \dots & \underline{{x}_{k,2}} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & \underline{{x}_{1,n}} & \dots & \underline{{x}_{k,n}} \end{pmatrix}$ and $\underline{\boldsymbol{\beta}} = \begin{pmatrix} \underline{{\beta}_0} \\ \underline{{\beta}_1} \\ \vdots \\ \underline{{\beta}_k} \end{pmatrix}$, where each of the elements in the matrices and vectors above is a complex number.

Furthermore, the system \@ref(eq:MultipleCLRSystem) can also be used to represent the multiple CLR in a simple form using vector and matrix notations, avoiding complex numbers:
\begin{equation}
    \mathbf{y}_j = \underset{\sim}{\mathbf{X}_j} \boldsymbol{\beta} + \boldsymbol{\epsilon}_j ,
    (\#eq:MultipleCLRSystemVector)
\end{equation}
where $\mathbf{y}_j = \begin{pmatrix} y_{r,j} \\ y_{i,j} \end{pmatrix}$, $\underset{\sim}{\mathbf{X}_j} = \begin{pmatrix} 1 & 0 & x_{1,r,j} & -x_{1,i,j} & \dots & x_{k,r,j} & -x_{k,i,j} \\ 0 & 1 & x_{1,i,j} & x_{1,r,j} & \dots & x_{k,i,j} & x_{k,r,j} \end{pmatrix}$, $\boldsymbol{\beta}^\prime = \begin{pmatrix} \beta_{0,r} & \beta_{0,i} & \beta_{1,r} & \beta_{1,i} & \dots & \beta_{1,k} & \beta_{1,k} \end{pmatrix}$ and $\mathbf{\epsilon}_j = \begin{pmatrix} \epsilon_{r,j} \\ \epsilon_{i,j} \end{pmatrix}$. This can be then represented in the even more compact form, using the same principles as discussed in Section \@ref(simpleCLRModel) in formula \@ref(eq:SimpleCLRSystemVectorFinal):
\begin{equation}
    \mathbf{Y} = \underset{\sim}{\mathbf{X}} \boldsymbol{\beta} + \mathbf{E} 
    (\#eq:CLRSystemVectorFinal)
\end{equation}
with where $\mathbf{Y}=\begin{pmatrix}\mathbf{y}_1 \\ \mathbf{y}_2\\ \vdots \\ \mathbf{y}_n \end{pmatrix}$, $\underset{\sim}{\mathbf{X}}=\begin{pmatrix} \underset{\sim}{\mathbf{X}_1} \\ \underset{\sim}{\mathbf{X}_2} \\ \vdots \\ \underset{\sim}{\mathbf{X}_n} \end{pmatrix}$ and $\mathbf{E}=\begin{pmatrix}\boldsymbol{\epsilon}_1 \\ \boldsymbol{\epsilon}_2\\ \vdots \\ \boldsymbol{\epsilon}_n \end{pmatrix}$. Formula \@ref(eq:CLRSystemVectorFinal) becomes especially useful for multiple CLR for the model estimation via OLS, CLS or Likelihood in the matrix form. The form \@ref(eq:CLRSystemVectorFinal) sidesteps complex numbers all together, representing the set of equations in matrices and vectors, containing real numbers only. This is convenient for many purposes and in inference.

The main difference between the form \@ref(eq:CLRVector) and \@ref(eq:CLRSystemVectorFinal) is that the former contains complex numbers inside each of the matrices and vectors.


## Estimation
In order to estimate the parameters of the model \@ref(eq:CLRVector), we can use the same methods as in the Chapter \@ref(simpleCLR): OLS, CLS and Likelihood. We will write the estimated model as:
\begin{equation}
    \underline{\mathbf{y}} = \underline{\mathbf{X}} \underline{\boldsymbol{b}} + \underline{\mathbf{e}} ,
    (\#eq:CLRVectorEstimated)
\end{equation}
where $\underline{\boldsymbol{b}}$ is the estimate of $\underline{\boldsymbol{\beta}}$ and $\underline{\mathbf{e}}$ is the estimate of $\underline{\mathbf{\epsilon}}$. And in case of matrix notations, instead of \@ref(eq:CLRSystemVectorFinal) we will have:
\begin{equation}
    \mathbf{Y} = \underset{\sim}{\mathbf{X}} \boldsymbol{b} + \mathbf{\hat{E}} 
    (\#eq:CLRSystemVectorFinalEstimated)
\end{equation}


### Ordinary Least Squares
The criterion of OLS for multiple CLR can be written as:
\begin{equation}
    \min S(\underline{\boldsymbol{b}}) = \min \left(\underline{\mathbf{e}}^\prime \underline{\mathbf{e}}\right),
    (\#eq:CLROLSCriterion)
\end{equation}
which can be expanded to:
\begin{equation}
    \begin{aligned}
    S(\underline{\boldsymbol{b}}) = & \left( \underline{\mathbf{y}} - \underline{\mathbf{X}} \underline{\boldsymbol{b}} \right)^\prime \left( \underline{\mathbf{y}} - \underline{\mathbf{X}} \underline{\boldsymbol{b}} \right) = \\
    & \underline{\mathbf{y}}^\prime \underline{\mathbf{y}} - \underline{\mathbf{y}}^\prime \underline{\mathbf{X}} \underline{\boldsymbol{b}} - \underline{\boldsymbol{b}}^\prime \underline{\mathbf{X}}^\prime \underline{\mathbf{y}} + \underline{\boldsymbol{b}}^\prime \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \underline{\boldsymbol{b}}
    \end{aligned}. 
    (\#eq:CLROLSCriterionExpanded)
\end{equation}
Taking derivative of \@ref(eq:CLROLSCriterionExpanded) with respect to $\underline{\boldsymbol{b}}$ and equating it to zero, results in the following system of normal equations:
\begin{equation}
    \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \underline{\boldsymbol{b}} = \underline{\mathbf{X}}^\prime \underline{\mathbf{y}} ,
    (\#eq:CLROLSSystemOfNormalEquations)
\end{equation}
which gives the classical formula for the estimation of parameters of the model \@ref(eq:CLRSystemVectorFinal):
\begin{equation}
    \underline{\boldsymbol{b}} = \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{y}}
    (\#eq:MCLROLSEstimate)
\end{equation}
Given that \@ref(eq:MCLROLSEstimate) corresponds to the classical OLS, it will maintain all of its conventional properties, i.e. its estimates being unbiased, efficient and consistent. Note that, as discussed Subsection \@ref(complexVariable), the operator $\prime$ denotes conjugate transition, which means that for the special case of simple CLR, the formula \@ref(eq:MCLROLSEstimate) will become \@ref(eq:SimpleCLROLSLossParametersMoments).

Finally, using the same principles, we can show that the estimates of parameters can be obtained if we use the form \@ref(eq:CLRSystemVectorFinalEstimated) instead of the vectors of complex variables:
\begin{equation}
    \boldsymbol{b} = \left( \underset{\sim}{\mathbf{\tilde{X}}}^\prime \underset{\sim}{\mathbf{X}}\right)^{-1} \underset{\sim}{\mathbf{\tilde{X}}}^\prime {\mathbf{Y}} .
    (\#eq:MCLROLSEstimateComplex)
\end{equation}
The form \@ref(eq:MCLROLSEstimateComplex) becomes especially useful for futher inference.


### Complex Least Squares
As discussed in Section \@ref(SCLREstimation), there is also an alternative approach to estimation of CLR, the Complex Least Squares. In order to get the estimates based on it, we need to apply the same principles as with OLS, but directly to the form \@ref(eq:CLRSystemVectorComplexFinal), i.e. minimise the criterion (which is the same as the one discussed in Subsection \@ref(SCLREstimationCLS)):
\begin{equation}
    \min S(\boldsymbol{b}) = \min \left(\underset{\sim}{\mathbf{\hat{E}}}^\prime \underset{\sim}{\mathbf{\hat{E}}}\right).
    (\#eq:CLRCLSCriterion)
\end{equation}
Using the same logic as with OLS, it can be shown that the minimisation of this criterion implies the solution of the following system of normal equations:
\begin{equation}
    \underset{\sim}{\mathbf{X}}^\prime \underset{\sim}{\mathbf{X}} \underset{\sim}{\boldsymbol{b}} = \underset{\sim}{\mathbf{X}}^\prime \underset{\sim}{\mathbf{Y}} ,
    (\#eq:CLRCLSSystemOfNormalEquations)
\end{equation}
which then results in the following formula for the CLS estimate of parameters:
\begin{equation}
    \boldsymbol{b} = \left( \underset{\sim}{\mathbf{{X}}}^\prime \underset{\sim}{\mathbf{X}}\right)^{-1} \underset{\sim}{\mathbf{{X}}}^\prime \underset{\sim}{\mathbf{Y}} .
    (\#eq:MCLRCLSEstimate)
\end{equation}
For the simple CLR, the formula \@ref(eq:MCLRCLSEstimate) becomes equivalent to \@ref(eq:SimpleCLRCLSLossParameters). The main difference between \@ref(eq:MCLRCLSEstimate) and \@ref(eq:MCLROLSEstimateComplex), as we can see, is that the CLS does not do multiplication by the conjugate complex numbers.

Finally, using the direct multiplication \@ref(eq:complexNumberMatrixMultiDirect), it can be shown that the estimates of parameters according to CLS, can be calculated as:
\begin{equation}
    \boldsymbol{b} = \left( \mathbf{{X}}^\top {\mathbf{X}}\right)^{-1} {\mathbf{{X}}}^\top {\mathbf{Y}} ,
    (\#eq:MCLRCLSEstimateTranspose)
\end{equation}
where $\mathbf{{X}}^\top$ is the complex transposition of the original matrix $\mathbf{{X}}$, as discussed in Subsection \@ref(vectorComplexVariables).

The two formulae \@ref(eq:MCLRCLSEstimate) and \@ref(eq:MCLRCLSEstimateTranspose) result in exactly the same values of parameters, but will be useful for inference in the following sections.


### Likelihood


## Inference
In both OLS and CLS, the individual contributions of real and imaginary parts of regression are ignored, everything is averaged-out. We just get a vector of errors. In likelihood, the split is preserved.

Covariance of parameters of the three approaches.

Difference in covariance matrices... Does OLS - CLS produce positive semidefinite matrix?


## Diagnostics

## Forecasting

## Examples of application (Production functions)
