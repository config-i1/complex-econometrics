#  Assumptions of Complex Linear Models
Similar to how it is done for the conventional real valued regression, we should discuss what assumptions are imposed on complex linear regression estimated using one of the methods discussed in the Section \@ref(mlcrEstimation). We will group all regression assumptions to three parts [similar to how it was done by @SvetunkovAdam]:

1. Model is correctly specified;
2. Residuals are independent and identically distributed (i.i.d.);
3. Explanatory variables are not correlated with anything but the response variable.

While the first two groups directly relate to the so called "true model", the last one refers to the estimation approaches discussed in the previous Chapter: if the latter are violated then the estimation procedure will lead to issues in estimates of parameters. We should also point out that many of the assumptions discussed in this Chapter are very similar to the assumptions in the conventional regression, which is why we do not plan to cover them in this monograph in detail, but rather to focus on their implications to complex-valued models. An reader interested in assumptions for the real-valued models is advised to read Chapter 15 of @SvetunkovSBA.


## Model is correctly specified
This is one of the most general and most important groups of assumptions. It includes the following:

1. Model does not omit any important variables;
2. Model does not have redundant variables;
3. Variables are included in the model with appropriate transformations;
4. Residuals of the model do not contain outliers.

We briefly discuss all of them in this section.


### Model does not omit any important variables
This assumption implies that we have all the variables that can substantially impact our response variable, and that we have included them in the model. If we do not do that then the estimates of parameters are known to be biased. In practice we always omit a lot of different variables that might potentially impact the response one, but do not have a large effect on it. Not including these variables is known not to cause serious issues in the estimation [@econometrics].

Formally speaking, the omitted variables should not be correlated with the ones that we include in the model, because otherwise the impact of the latter will not be captured correctly, causing bias in the estimates of parameters. It is not possible to test this assumption, so it can only be checked based on judgment of an analyst.

When it comes to complex-valued models, the relations become much more complicated and potentially non-linear than in the conventional models. The problem becomes then manifolds: we need to include the correct variables in the model, but also they need to be included correctly, because now any variable can be included in the real or in the imaginary part. Furthermore, the correlation in complex-valued models can be measured differently and implies a variety of effects between two variables.

Practically speaking, we consider two situations with omitted variables:

1. The model is estimated using OLS;
2. The model is estimated using CLS.

For the both cases, we assume that the true model is of the form:
\begin{equation}
    \underline{\mathbf{y}} = \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \underline{\boldsymbol{\epsilon}},
    (\#eq:AssumptionsOmitTrueModel)
\end{equation}
where the matrix $\underline{\mathbf{Z}}$ contains the variables omitted in the applied model and the vector $\underline{\boldsymbol{\gamma}}$ is the vector of the parameters for these variables in the true model.

#### Omitted variables in case of OLS estimation
We refer to the derivations \@ref(eq:MCLROLSExpansion), which given \@ref(eq:AssumptionsOmitTrueModel) can be substituted to:
\begin{equation}
    \begin{aligned}
    \underline{\boldsymbol{b}}^{\text{OLS}} =
        & \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{y}} = \\
        & \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime (\underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \underline{\boldsymbol{\epsilon}}) = \\
        & \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\boldsymbol{\epsilon}} = \\
        & \underline{\boldsymbol{\beta}} + \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\boldsymbol{\epsilon}} .
    \end{aligned}
    (\#eq:AssumptionsOmitOLS01)
\end{equation}


### Model does not have redundant variables
This is the situation opposite to the first one. It implies that we have included something that should not be there. If that happens, the redundant variable will explain the noise and thus our model will overfit the data and lead to inefficient estimates of parameters (i.e. variances of estimates will be larger than needed).


### Variables transformations
While the first two assumptions are universal for any statistical model, the third one has some special implications in case of CLR. This is because of the effect they have on the variables: transformations of separate parts of a complex variable are not equivalent to the transformations of the whole variable. For example, the logarithm of a complex variable $\underline{z}$ as shown in \@ref(eq:complexNumberLogarithm) is:
\begin{equation*}
    \ln \underline{z} = \ln r + i \phi ,
\end{equation*}
which is not equivalent to the complex variable $\ln x_r + i \ln x_i$. In fact, there is no known transformation from $x_r + x_i$ to $\ln x_r + i \ln x_i$ except for transforming separately the real and imaginary parts of the variable. Still, logarithms can be used to linearise a non-linear complex model to simplify its estimation. For example, the following multiplicative model:
\begin{equation*}
    \begin{aligned}
    y_{r} + i y_{i} = & (\beta_{0,r} + i \beta_{0,i}) \times (x_{1,r} + i x_{1,i}) ^{\beta_{1,r} + i \beta_{1,i}} \times \dots \times \\
                      & (x_{k-1,r} + i x_{k-1,i}) ^{\beta_{k-1,r} + i \beta_{k-1,i}} \times (\epsilon_{r} + i \epsilon_{i})
    \end{aligned}
\end{equation*}
can be linearised using natural logarithm to:
\begin{equation*}
    \begin{aligned}
    \ln (y_{r} + i y_{i}) = & \ln (\beta_{0,r} + i \beta_{0,i}) + (\beta_{1,r} + i \beta_{1,i}) \ln(x_{1,r} + i x_{1,i}) + \dots + \\
                            & (\beta_{k-1,r} + i \beta_{k-1,i}) \ln (x_{k-1,r} + i x_{k-1,i}) + \ln (\epsilon_{r} + i \epsilon_{i}),
    \end{aligned}
\end{equation*}
which can then be estimated using OLS, CLS or likelihood maximisation as discussed in Section \@ref(mlcrEstimation). On its own, the multiplicative models of complex variables have useful features, because they allow modelling highly non-linear processes due to the rotation as discussed in Section \@ref(theoryOfComplexNumbers). For example, here how the linear increase of real and imaginary parts of a complex variable $\underline{x}$ leads to a non-linear transform of a variable $\underline{y}=\underline{x}^{0.5+0.5i}$:

```{r fig.width=4, fig.height=4, fig.cap="Non-linear transformation of a complex variable that changes linearly."}
x <- 1:100 + 1:100*1i
y <- x^(0.5+0.5i)
cplot(x, y)
```

Even if the imaginary part of $\underline{x}$ is zero (and we are dealing with a real number, not a complex one), the complex power leads to highly non-linear transformation of the variable. This is a useful feature if the non-linearity is suspected in the data.


### No outliers
Finally, the presence of outliers in the residuals typically implies that either there is an error in recording the data, or the model omits an important variable (e.g. a dummy variable for an external event). In that case the estimates of parameters will be biased. Making sure that there are no outliers provides some assurance that there are no important missing variables in the model.


## Residuals are i.i.d.
The next group of assumptions has 6 elements in it:

1. Residuals are homoscedastic;
2. No autocorrelation in residuals;
3. Expectation of residuals is zero (no matter what);
4. Residuals follow an assumed distribution;
5. Distribution of the residuals is constant (no matter what);
6. Residuals follow a circular distribution.


### Homoscedastic residuals
The first assumption implies for CLR that both direct and conjugate variances of the residuals are constant and do not change with any changes of variables. This also means that the covariance matrix of residuals $\Sigma_\epsilon$ stays the same no matter what. This assumption aligns well with a similar assumption of conventional multivariate models, such as VAR or VETS.




### No autocorrelation in residuals
The second assumption only applies to time series data. In case of the real-valued model, it means that the residuals in the past do not impact the ones in the future. Typically, this effect appears because of the wrong specification of the model (e.g. correct transformations are not done or some autoregressive elements are missing). In case of the CLR, the idea is very similar, but now we are talking about complex relations between the residuals, which could arise, again, because of the wrong transformations or because of missing elements (e.g. Complex Autoregression, discussed later in this book).


### Expectation of residuals is zero
While many other assumptions can be diagnosed in one way or another, the assumption (3) in some cases cannot be. For example, if we use OLS, the unconditional mean of the residuals will be equal to zero by design, which does not provide any useful information about the "true" model. However, this assumption can be analysed conditionally, e.g. conditional on the fitted values. If the conditional mean of the residuals is not constant then we can conclude that some important element was omitted by the model. In a way, the assumption becomes similar to the assumption about the omitted variables and/or autocorrelated residuals.


### Residuals follow an assumed distribution
In the classical real-valued regression, the assumption (4) is not considered to be crucial and if the model is estimated using OLS typically comes to "the residuals follow the Normal distribution". In context of complex variables, the equivalent assumption would be that the complex residuals follow the Complex Normal distribution discussed in Section \@ref(distributionCNorm). If the model is formulated using matrix notations \@ref(eq:CLRSystemVectorFinal), the assumption transforms to "Multivariate Normal" one, discussed in Section \@ref(MVNorm). In anyway, this assumption is complementary to the main ones, even when it does not hold but all the others hold, the estimates of parameters should be unbiased, efficient and consistent. However, it becomes essential when the prediction interval needs to be generated from the model.


### Distribution of residuals us constant
Building upon (4), the fifth assumption is applicable to time series models only and states more generally that we deal with one and the same assumption of residuals over time. This assumption is very difficult to test (if at all possible), so it is typically dropped from the discussion.


### Circular distribution
Finally, in signal processing literature, the assumption (6) means that the covariance between the residuals equals to zero and the variances of the real and the imaginary parts are equal. This implies that the direct variance of residuals is equal to zero. While this might be a suitable assumption in that discipline, there is no good rationale for this to hold universally for all CLR models. So, we will not consider this assumption as an important one in what follows.



## Explanatory variables are not correlated with anything but the response variable
The two assumptions in this group relate to the estimation of model rather than anything else:

1. No endogeneity
2. No multicollinearity

The first assumption comes to the idea that a linear regression model can only capture a one-directional relation, where an explanatory variable impacts the response variable. If the relation in the real life is bi-directional then the estimates of parameters of a regression model will be biased. In that case, the relation needs to be somehow made one-directional, for example by substituting the explanatory variable that causes a problem with something similar that is not impacted by the response variable. In case of the complex linear regression, the situation would be similar to the conventional one. We do not plan to discuss how to solve the problem in case of CLR in this monograph. But we believe that the methods from econometrics of real-valued models should be widely applicable here as well (e.g. using proxies or instrumental variables).

As for the multicollinearity issue, in the real-valued statistics, it arises when least squares or maximum likelihood is used for estimation. It is, in a way, a technical issue: if the explanatory variables are linearly related it becomes difficult to split the impact of each of the explanatory variables on the response variable and thus challenging to correctly capture the relations between the explanatory variables and the response one.

In case of an OLS applied to a conventional real-valued regression model, the issue comes to inverting the matrix $\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1}$. If some of variables in $\mathbf{X}$ are strongly linearly related, the determinant of the matrix becomes close to zero and thus the inversion becomes challenging. Even if it is still possible to invert the matrix, the estimates of parameters become inefficient because small changes in the related explanatory variables may lead to substantial changes in the inversion.

In case of CLR, the situation is similar, but with some specific features. If the OLS is used for parameters estimation then a potential issue might arise from the inversion of $\left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1}$. In one of the special cases, this comes to the conjugate covariance between variables. If it is too high in comparison to the conjugate variances, then we would face the multicollinearity issue, which would make the estimates of parameters inefficient. More generally speaking, if a complex linear regression between an explanatory variable and other explanatory variables estimated using OLS captures the relations between variables well (i.e. having a high coefficient of determination), then we might be facing a multicollinearity problem. Similar problems might appear in case of CLS, but they come to a slightly different inversion, $\left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}}\right)^{-1}$ and thus relates to direct covariances rather than the conjugate ones. Finally, when it comes to the likelihood, due to the model formulation, the conventional real-valued relations between explanatory variables should be considered when multicollinearity is suspected. All of this means that a choice of an estimator can be dictated by the strength of the conventional Pearson's, direct and/or conjugate correlations (and more widely conventional linear model and/or complex linear model for explanatory variables estimated using OLS/CLS).


## Diagnostics

sigma & Sigma

R^2, R^2 - adjusted
R^2 for different parts or for the whole model
R^2 for covariances? Percentage of explained covariance between variables?

```{r}
# 1- cvar(e, method="conj") / cvar(y,method="conj")

# 1-covar(e) / covar(y)
```


Visual analysis


<!-- ## Forecasting -->


<!-- ## Examples of application (Production functions) -->

