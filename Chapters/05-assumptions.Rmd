#  Assumptions of Complex Linear Models {#assumptions}
Similar to how it is done for the conventional real valued regression, we should discuss what assumptions are imposed on complex linear regression estimated using one of the methods discussed in the Section \@ref(mlcrEstimation). We will group all regression assumptions to three parts [similar to how it was done by @SvetunkovAdam]:

1. Model is correctly specified;
2. Residuals are independent and identically distributed (i.i.d.);
3. Explanatory variables are not correlated with anything but the response variable.

While the first two groups directly relate to the so called "true model", the last one refers to the estimation approaches discussed in the previous Chapter: if the latter are violated then the estimation procedure will lead to issues in estimates of parameters. We should also point out that many of the assumptions discussed in this Chapter are very similar to the assumptions in the conventional regression, which is why we do not plan to cover them in this monograph in detail, but rather to focus on their implications to complex-valued models. An reader interested in assumptions for the real-valued models is advised to read Chapter 15 of @SvetunkovSBA.


## Model is correctly specified {#assumptionsSpecification}
This is one of the most general and most important groups of assumptions. It includes the following:

1. Model does not omit any important variables;
2. Model does not have redundant variables;
3. Variables are included in the model with appropriate transformations;
4. Residuals of the model do not contain outliers.

We briefly discuss all of them in this section.


### Model does not omit any important variables {#assumptionsSpecificationOmit}
This assumption implies that we have all the variables that can substantially impact our response variable, and that we have included them in the model. If we do not do that then the estimates of parameters are known to be biased. In practice we always omit a lot of different variables that might potentially impact the response one, but do not have a large effect on it. Not including these variables is known not to cause serious issues in the estimation [@econometrics].

Formally speaking, the omitted variables should not be correlated with the ones that we include in the model, because otherwise the impact of the latter will not be captured correctly, causing bias in the estimates of parameters. It is not possible to test this assumption, so it can only be checked based on judgment of an analyst.

When it comes to complex-valued models, the relations become much more complicated and potentially non-linear than in the conventional models. The problem becomes then manifolds: we need to include the correct variables in the model, but also they need to be included correctly, because now any variable can be included in the real or in the imaginary part. Furthermore, the correlation in complex-valued models can be measured differently and implies a variety of effects between two variables.

Practically speaking, we consider two situations with omitted variables:

1. The model is estimated using OLS;
2. The model is estimated using CLS.

For the both cases, we assume that the true model is of the form:
\begin{equation}
    \underline{\mathbf{y}} = \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \underline{\boldsymbol{\epsilon}},
    (\#eq:AssumptionsOmitTrueModel)
\end{equation}
where the matrix $\underline{\mathbf{Z}}$ contains the variables omitted in the applied model and the vector $\underline{\boldsymbol{\gamma}}$ is the vector of the parameters for these variables in the true model.

We start with the effect of the omitted variables on the estimates of parameters using OLS. We refer to the derivations \@ref(eq:MCLROLSExpansion), which given \@ref(eq:AssumptionsOmitTrueModel) can be substituted to:
\begin{equation}
    \begin{aligned}
    \underline{\boldsymbol{b}}^{\text{OLS}} =
        & \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{y}} = \\
        & \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \left(\underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \underline{\boldsymbol{\epsilon}} \right) = \\
        & \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\boldsymbol{\epsilon}} = \\
        & \underline{\boldsymbol{\beta}} + \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\boldsymbol{\epsilon}} .
    \end{aligned}
    (\#eq:AssumptionsOmitOLS01)
\end{equation}

Taking the expectation of \@ref(eq:AssumptionsOmitOLS01), we get:
\begin{equation}
    \mathrm{E}\left(\underline{\boldsymbol{b}}^{\text{OLS}}\right) = \underline{\boldsymbol{\beta}} + \mathrm{E}\left(\left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} \right) + 0 ,
    (\#eq:AssumptionsOmitOLS02)
\end{equation}
with zero appearing because of the one of fundamental assumptions that the explanatory variables are not correlated with the error term in the true model. The expectation above shows that the estimates of the OLS parameters will be biased in case of omitted variables, and the bias will be proportional to the size of the $\underline{\mathbf{X}}^\prime \underline{\mathbf{Z}}$ matrix, which consists of the covariances between the included and the omitted variables. The higher the correlation between these variables, the higher the bias will be. This agrees with the similar findings in the conventional real-valued econometrics.

When it comes to CLS, the logic is similar to the above. Based on the equations \@ref(eq:MCLRCLSExpansion) and \@ref(eq:AssumptionsOmitTrueModel) we have:
\begin{equation}
    \begin{aligned}
    \underline{\boldsymbol{b}}^{\text{CLS}} =
    & \left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\top \underline{\mathbf{y}} = \\
    & \left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\top \left(\underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \underline{\boldsymbol{\epsilon}}\right) = \\
    & \underline{\boldsymbol{\beta}} + \left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\top \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\top \underline{\boldsymbol{\epsilon}}
    \end{aligned} ,
    (\#eq:AssumptionsOmitCLS01)
\end{equation}
which simplifies (using similar logic as above) to:
\begin{equation}
    \mathrm{E}\left(\underline{\boldsymbol{b}}^{\text{CLS}}\right) = \underline{\boldsymbol{\beta}} + \mathrm{E}\left( \left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\top \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}}\right) ,
    (\#eq:AssumptionsOmitCLS02)
\end{equation}
Which also shows that the bias will increase with the increase of the values of the matrix $\underline{\mathbf{X}}^\top \underline{\mathbf{Z}}$. The main difference between the real-valued and the complex-valued econometrics is that the aforementioned matrices are complex and the respective covariances in OLS and CLS are respectively conjugate and direct. This means that:

1. if an omitted complex variable has high *direct correlation* with the included one but has the zero *conjugate correlation*, the OLS should give unbiased estimates of parameters, while the CLS would give the biased ones;
2. if an omitted complex variable has zero *direct correlation* with the included one but has high *conjugate correlation*, the CLS should give unbiased estimates of parameters, while the OLS would give the biased ones.

This property can be used in deciding which of the estimation methods to use if the researcher has some insights about the relation between the included and the omitted complex variables.

Furthermore, we can also see what happens with the direct and conjugate covariance matrices of parameters in case of omitted variables. In the case of the omitted variables the residuals become $\underline{\boldsymbol{\upsilon}} = \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \underline{\boldsymbol{\epsilon}}$, implying that we estimate the following model instead of \@ref(eq:AssumptionsOmitTrueModel):
\begin{equation}
    \underline{\mathbf{y}} = \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\boldsymbol{\upsilon}} ,
    (\#eq:AssumptionsOmitAppliedTrueModel)
\end{equation}
which means that we then make inference based on:
\begin{equation}
    \underline{\mathbf{y}} = \underline{\mathbf{X}} \underline{\boldsymbol{b}} + \hat{\underline{\boldsymbol{\upsilon}}} .
    (\#eq:AssumptionsOmitAppliedModel)
\end{equation}

The covariance matrix of the estimates of the parameter $\underline{\boldsymbol{\beta}}$ in this case comes to formulae discussed in Section \@ref(MCLRInference):
\begin{equation}
    \begin{aligned}
        & \mathrm{V}\left( \underline{\boldsymbol{b}}^{\text{OLS}} \right) = \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \tilde{\underline{\mathbf{X}}} \left( {\underline{\mathbf{X}}}^\top \tilde{\underline{\mathbf{X}}} \right)^{-1 \prime}  \sigma_{\underline{\upsilon}}^2 \\
        & \mathcal{V}\left( \underline{\boldsymbol{b}}^{\text{OLS}} \right) = \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \varsigma_{\underline{\upsilon}}^2 \\
        & \mathrm{V}\left( \underline{\boldsymbol{b}}^{\text{CLS}} \right) = \left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\top \tilde{\underline{\mathbf{X}}} \left( {\underline{\mathbf{X}}}^\prime \tilde{\underline{\mathbf{X}}} \right)^{-1 \top}  \sigma_{\underline{\upsilon}}^2 \\
        & \mathcal{V}\left( \underline{\boldsymbol{b}}^{\text{CLS}} \right) = \left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}} \right)^{-1} \varsigma_{\underline{\upsilon}}^2 ,
    \end{aligned}
    (\#eq:MCLROLSCLSVariancesOmitted)
\end{equation}
where $\sigma_{\underline{\upsilon}}^2$ is the conjugate and $\varsigma_{\underline{\upsilon}}^2$ is the direct variances of the residuals $\underline{\boldsymbol{\upsilon}}$. Now to better understand the effect of omitted variables on the covariance matrix of parameters, we should consider several situations:

1. Some parts of $\underline{\mathbf{Z}}$ are correlated with $\underline{\mathbf{X}}$, in which case applying the model \@ref(eq:AssumptionsOmitAppliedModel) to the data will lead to the biased estimates of parameters, but would not have a large impact on the variance of the residuals of the model. The higher the correlation is, the lower the variances become in \@ref(eq:MCLROLSCLSVariancesOmitted);
2. $\underline{\mathbf{Z}}$ is uncorrelated with $\underline{\mathbf{X}}$, which means that the effect of the omitted variable is not captured by the model and as a result the variance of the residual $\hat{\underline{\boldsymbol{\upsilon}}}$ will be inflated, increasing the standard errors of the estimates of parameters.

The case (1) corresponds to the classical problem of the omitted variables in econometrics, while the case (2) shows how the uncertainty about the estimates of parameters increases with omitted variables and might lead to the wider confidence intervals for the parameters.



### Model does not have redundant variables {#assumptionsSpecificationRedundant}
This is the situation opposite to the first one. It implies that we have included something that should not be there. Mathematically it can be represented by two sets of equations, for the true and the applied models:
\begin{equation}
    \begin{aligned}
        & \underline{\mathbf{y}} = \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\boldsymbol{\epsilon}} \\
        & \underline{\mathbf{y}} = \underline{\mathbf{X}} \underline{\boldsymbol{b}} + \underline{\mathbf{X}}_{red} \underline{\boldsymbol{b}}_{red} + \underline{\boldsymbol{e}} ,
    \end{aligned}
    (\#eq:AssumptionsRedundantSetting)
\end{equation}
where $\underline{\boldsymbol{b}}_{red}$ is the vector of the estimates of parameters for the redundant variables $\underline{\mathbf{X}}_{red}$ on a sample. It will not be zero in sample, because the values of $\underline{\mathbf{X}}_{red}$ can explain some randomness in $\underline{\mathbf{y}}$, thus reducing the size of variance of the residuals $\underline{\boldsymbol{e}}$ in comparison with the estimation of the correct model. This leads to the effect known as "overfitting" in forecasting.

The second equation in \@ref(eq:AssumptionsRedundantSetting) can also be written in a compact form:
\begin{equation}
    \underline{\mathbf{y}} = \underline{\mathbf{Z}} \underline{\boldsymbol{c}}  + \underline{\boldsymbol{e}} ,
    (\#eq:AssumptionsRedundantAppliedModel)
\end{equation}
where $\underline{\mathbf{Z}} = \begin{pmatrix} \underline{\mathbf{X}} & \underline{\mathbf{X}}_{red} \end{pmatrix}$ is the matrix that contains all variables under consideration, and $\underline{\boldsymbol{c}} = \begin{pmatrix} \underline{\boldsymbol{b}} \\ \underline{\boldsymbol{b}}_{red} \end{pmatrix}$. While it is hard to show explicitly using formulae of OLS and CLS, it is well known in statistics that the estimates of parameters in such model are unbiased, because the true $\underline{\boldsymbol{b}}_{red}$ is zero. At the same time, the inclusion of redundant variables increases the standard errors of parameters, because the covariance matrices from Section \@ref(MCLRInference) would rely on different combinations of $\underline{\mathbf{Z}}$ and its transposition (direct and conjugate), which means that the final variances would be impacted by the variability of redundant variables $\underline{\mathbf{X}}_{red}$. The exact effect is difficult to capture and might depend on the direct/conjugate correlations between variables. We leave this task for future research.


<!-- \begin{equation} -->
<!--     \begin{aligned} -->
<!--     \underline{\boldsymbol{c}}^{\text{OLS}} = -->
<!--         & \left( \underline{\mathbf{Z}}^\prime \underline{\mathbf{Z}} \right)^{-1} \underline{\mathbf{Z}}^\prime \underline{\mathbf{y}} = \\ -->
<!--         & \left( \underline{\mathbf{Z}}^\prime \underline{\mathbf{Z}} \right)^{-1} \underline{\mathbf{Z}}^\prime \left(\underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\boldsymbol{\epsilon}} \right) = \\ -->
<!--         & \left( \underline{\mathbf{Z}}^\prime \underline{\mathbf{Z}} \right)^{-1} \left( \underline{\mathbf{Z}}^\prime \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\mathbf{Z}}^\prime \underline{\boldsymbol{\epsilon}} \right) . -->
<!--     \end{aligned} -->
<!--     (\#eq:AssumptionsRedundantOLS01) -->
<!-- \end{equation} -->
<!-- Substituting $\underline{\mathbf{Z}} = \begin{pmatrix} \underline{\mathbf{X}} & \underline{\mathbf{X}}_{red} \end{pmatrix}$ and $\underline{\mathbf{Z}}^\prime = \begin{pmatrix} \underline{\mathbf{X}}^\prime \\ \underline{\mathbf{X}}_{red}^\prime \end{pmatrix}$ in \@ref(eq:AssumptionsRedundantOLS01), we get: -->
<!-- \begin{equation} -->
<!--     \begin{aligned} -->
<!--     \underline{\boldsymbol{c}}^{\text{OLS}} = -->
<!--         & \left( \begin{pmatrix} \underline{\mathbf{X}}^\prime \\ \underline{\mathbf{X}}_{red}^\prime \end{pmatrix} \begin{pmatrix} \underline{\mathbf{X}} & \underline{\mathbf{X}}_{red} \end{pmatrix} \right)^{-1} \left( \begin{pmatrix} \underline{\mathbf{X}}^\prime \\ \underline{\mathbf{X}}_{red}^\prime \end{pmatrix} \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \begin{pmatrix} \underline{\mathbf{X}}^\prime \\ \underline{\mathbf{X}}_{red}^\prime \end{pmatrix} \underline{\boldsymbol{\epsilon}} \right) = \\ -->
<!--         & \begin{pmatrix} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} & \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}_{red} \\ \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}} & \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}}_{red} \end{pmatrix}^{-1} \left( \begin{pmatrix} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \\ \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}} \end{pmatrix} \underline{\boldsymbol{\beta}} + \begin{pmatrix} \underline{\mathbf{X}}^\prime \\ \underline{\mathbf{X}}_{red}^\prime \end{pmatrix} \underline{\boldsymbol{\epsilon}} \right) -->
<!--     \end{aligned} -->
<!--     (\#eq:AssumptionsRedundantOLS02) -->
<!-- \end{equation} -->

<!-- The inverse block matrix in \@ref(eq:AssumptionsRedundantOLS02) is cumbersome, but each element of the resulting square block matrix can be written as: -->
<!-- \begin{equation} -->
<!--     \begin{pmatrix} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} & \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}_{red} \\ \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}} & \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}}_{red} \end{pmatrix}^{-1} = \begin{pmatrix} \mathbf{A} & \mathbf{B} \\ \mathbf{C} & \mathbf{D} \end{pmatrix} . -->
<!--     (\#eq:AssumptionsRedundantOLS03) -->
<!-- \end{equation} -->
<!-- The first element of that matrix is: -->
<!-- \begin{equation} -->
<!--     \begin{aligned} -->
<!--         A = & \begin{pmatrix} \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} + \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}_{red} \left(\underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}}_{red} - \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}} \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}_{red} \right)^{-1}  \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}} \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} \end{pmatrix} = \\ -->
<!--         & \begin{pmatrix} \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} + \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}_{red} \left(\underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}}_{red} - \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}} \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}_{red} \right)^{-1}  \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}} \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} \end{pmatrix} -->
<!--     \end{aligned} -->
<!--     (\#eq:AssumptionsRedundantOLS04) -->
<!-- \end{equation} -->


### Variables transformations {#assumptionsSpecificationTransformation}
While the first two assumptions are universal for any statistical model, the third one has some special implications in case of CLR. This is because of the effect they have on the variables: transformations of separate parts of a complex variable are not equivalent to the transformations of the whole variable. For example, the logarithm of a complex variable $\underline{z}$ as shown in \@ref(eq:complexNumberLogarithm) is:
\begin{equation*}
    \ln \underline{z} = \ln r + i \phi ,
\end{equation*}
which is not equivalent to the complex variable $\ln x_r + i \ln x_i$. In fact, there is no known transformation from $x_r + x_i$ to $\ln x_r + i \ln x_i$ except for transforming separately the real and imaginary parts of the variable. Still, logarithms can be used to linearise a non-linear complex model to simplify its estimation. For example, the following multiplicative model:
\begin{equation*}
    \begin{aligned}
    y_{r} + i y_{i} = & (\beta_{0,r} + i \beta_{0,i}) \times (x_{1,r} + i x_{1,i}) ^{\beta_{1,r} + i \beta_{1,i}} \times \dots \times \\
                      & (x_{k-1,r} + i x_{k-1,i}) ^{\beta_{k-1,r} + i \beta_{k-1,i}} \times (\epsilon_{r} + i \epsilon_{i})
    \end{aligned}
\end{equation*}
can be linearised using natural logarithm to:
\begin{equation*}
    \begin{aligned}
    \ln (y_{r} + i y_{i}) = & \ln (\beta_{0,r} + i \beta_{0,i}) + (\beta_{1,r} + i \beta_{1,i}) \ln(x_{1,r} + i x_{1,i}) + \dots + \\
                            & (\beta_{k-1,r} + i \beta_{k-1,i}) \ln (x_{k-1,r} + i x_{k-1,i}) + \ln (\epsilon_{r} + i \epsilon_{i}),
    \end{aligned}
\end{equation*}
which can then be estimated using OLS, CLS or likelihood maximisation as discussed in Section \@ref(mlcrEstimation). On its own, the multiplicative models of complex variables have useful features, because they allow modelling highly non-linear processes due to the rotation as discussed in Section \@ref(theoryOfComplexNumbers). For example, here how the linear increase of real and imaginary parts of a complex variable $\underline{x}$ leads to a non-linear transform of a variable $\underline{y}=\underline{x}^{0.5+0.5i}$:

```{r fig.width=4, fig.height=4, fig.cap="Non-linear transformation of a complex variable that changes linearly."}
x <- 1:100 + 1:100*1i
y <- x^(0.5+0.5i)
cplot(x, y)
```

Even if the imaginary part of $\underline{x}$ is zero (and as a result we deal with a real number, not a complex one), the complex power leads to highly non-linear transformation of the variable. This is a useful feature if the non-linearity is suspected in the data.

The diagnostics of correct transformations in CLR is challenging, but can be done by analysing the residuals. Consider the following example in R (using functions from the `complex` package in R):

```{r}
# Set random seed for reproducibility
set.seed(41)
# Sample size
obs <- 1000
# Create the explanatory variable
x <- complex(real=rnorm(obs,100,10), imaginary=rnorm(obs,50,5))
# Generate parameters
b0 <- 1 - 1.5i
b1 <- 2.5 + 1.5i
# Generate error term from the complex normal distribution
e <- complex(real=rnorm(obs, 0, 0.1), imaginary=rnorm(obs, 0, 0.2))
# e <- rcnorm(obs, 0, sigma2=0.25, varsigma2=0.16+0.09i)
# Generate the data using non-linear model
y <- exp(b0 + b1 * log(x) + e)
# Merge it to the matrix
complexData <- cbind(y=y,x=x)
```

For demonstration purposes, we will first use a complex linear regression model on the data that was generated using a non-linear one:
```{r}
# Apply a linear model
complexModel <- clm(y~x, complexData)
```

The issues of the model can be diagnosed using various plots. For example, here how the standardised residuals vs fitted would look for the model above (see Figure \@ref(fig:nonlinearLinStdResid)):
```{r nonlinearLinStdResid, fig.cap="Standardised residuals vs Fitted for the CLR on non-linear data."}
par(mfcol=c(1,2), mar=c(2,2,3,1))
plot(complexModel, which=2, main="")
```

The plots above demonstrate that there is a slight non-linear pattern in the residuals (especially for the real part of the model) and that their variances might not be constant. These are the indicators of a possible non-linearity in the data. In order to capture it correctly, we would need to transform both response and the explanatory complex variables and estimate the model in logarithms:

```{r}
# Transform the data
complexDataLogs <- log(complexData)
# Apply a model to log data
complexModelLogs <- clm(y~x, complexDataLogs)
```

After which the same plot will look more reasonable, with residuals not exhibiting the u-shape anymore (see Figure \@ref(fig:nonlinearStdResid)):

```{r nonlinearStdResid, fig.cap="Standardised residuals vs Fitted for the log-log CLR on non-linear data."}
par(mfcol=c(1,2), mar=c(2,2,3,1))
plot(complexModelLogs, which=2, main="")
```


### No outliers {#assumptionsSpecificationOutlier}
Finally, the presence of outliers in the residuals typically implies that either there is an error in recording the data, or the model omits an important variable (e.g. a dummy variable for an external event). In that case the effect on estimates of parameters will be similar to the one discussed in Subsection \@ref(assumptionsSpecificationOmit): the estimates of parameters will be in general biased with some specific effects depending on the direct/conjugate correlation and the estimation method used.

The simplest way to detect outliers is to produce a diagnostic plot of fitted vs standardised residuals (similar to the one in Figure \@ref(fig:nonlinearStdResid)) and analyse those values that lie outside of the constructed confidence interval. If the value of an outlier cannot be explained (e.g. this is not a calendar event, this is not a promotion etc) then it can be removed or interpolated. In the latter case, creating a dummy variable (see Section \@ref(multipleCLRDummy)) that equals to one on that specific observation and to zero on all the others is one of the simplest way of dealing with the outlier.


## Residuals are i.i.d. {#assumptionsResiduals}
The next group of assumptions has five elements in it:

1. Residuals are homoscedastic;
2. No autocorrelation in residuals;
3. Expectation of residuals is zero (no matter what);
4. Residuals follow an assumed distribution;
5. Residuals follow a circular distribution.

Some of these assumptions are universal for statistical models, while the others (e.g. the last one) are specific to CLR.

### Homoscedastic residuals {#assumptionsResidualsHomoscedastic}
The first assumption implies that both direct and conjugate variances of the residuals are constant and do not change with any changes of variables. This follows directly from the formulae for the direct and conjugate covariances from Section \@ref(MCLRInference): if either of the covariances is not constant, and we use the formulae for calculating the standard errors of parameters, we will obtain averaged-out values, that would be lower than needed for some observations and higher than needed for the others.

In case of the formulation of CLR as a vector model and the likelihood estimation, the assumption implies that the covariance matrix of residuals $\Sigma_\epsilon$ stays the same no matter what. This assumption aligns well with a similar assumption of conventional multivariate models, such as VAR [@Lutkepohl] or VETS [@SvetunkovVETS].

The homoscedasticity assumption can be typically diagnosed visually by producing the plot of absolute residuals vs fitted. In case of the CLR, this can be modified to producing plots for each part of the complex residuals. Figure \@ref(fig:heteroDiagnostics) shows how the residuals of the linear model applied to the non-linear data (example from Subsection \@ref(assumptionsSpecificationTransformation)) look:

```{r heteroDiagnostics, fig.cap="Absolute residuals vs fitted"}
par(mfcol=c(1,2), mar=c(2,2,3,1))
plot(complexModel, which=4, main="")
```

The plots in Figure \@ref(fig:heteroDiagnostics) show that the variability of residuals (and the local mean) increases with the increase of fitted values. This is a signal of the heteroscedasticity in the data. Some non-linear transformations (e.g. taking logarithm) typically resolve the issue.


### No autocorrelation in residuals
The second assumption only applies to time series data. In case of the real-valued model, it means that the residuals in the past do not impact the ones in the future. Typically, this effect appears because of the wrong specification of the model (e.g. correct transformations are not done or some autoregressive elements are missing). In case of the CLR, the idea is very similar, but now we are talking about complex relations between the residuals, which could arise, again, because of the wrong transformations or because of missing elements (e.g. Complex Autoregression, discussed later in this book).


### Expectation of residuals is zero
While many other assumptions can be diagnosed in one way or another, the assumption (3) in some cases cannot be. For example, if we use OLS, the unconditional mean of the residuals will be equal to zero by design, which does not provide any useful information about the "true" model. However, this assumption can be analysed conditionally, e.g. conditional on the fitted values. If the conditional mean of the residuals is not constant then we can conclude that some important element was omitted by the model. In a way, the assumption becomes similar to the assumption about the omitted variables and/or autocorrelated residuals.


### Residuals follow an assumed distribution
In the classical real-valued regression, the assumption (4) is not considered to be crucial and if the model is estimated using OLS typically comes to "the residuals follow the Normal distribution". In context of complex variables, the equivalent assumption would be that the complex residuals follow the Complex Normal distribution discussed in Section \@ref(distributionCNorm). If the model is formulated using matrix notations \@ref(eq:CLRSystemVectorFinal), the assumption transforms to "Multivariate Normal" one, discussed in Section \@ref(MVNorm). In anyway, this assumption is complementary to the main ones, even when it does not hold but all the others hold, the estimates of parameters should be unbiased, efficient and consistent. However, it becomes essential when the prediction interval needs to be generated from the model.


### Distribution of residuals us constant
Building upon (4), the fifth assumption is applicable to time series models only and states more generally that we deal with one and the same assumption of residuals over time. This assumption is very difficult to test (if at all possible), so it is typically dropped from the discussion.


### Circular distribution
Finally, in signal processing literature, the assumption (6) means that the covariance between the residuals equals to zero and the variances of the real and the imaginary parts are equal. This implies that the direct variance of residuals is equal to zero. While this might be a suitable assumption in that discipline, there is no good rationale for this to hold universally for all CLR models. So, we will not consider this assumption as an important one in what follows.



## Explanatory variables are not correlated with anything but the response variable
The two assumptions in this group relate to the estimation of model rather than anything else:

1. No endogeneity
2. No multicollinearity

The first assumption comes to the idea that a linear regression model can only capture a one-directional relation, where an explanatory variable impacts the response variable. If the relation in the real life is bi-directional then the estimates of parameters of a regression model will be biased. In that case, the relation needs to be somehow made one-directional, for example by substituting the explanatory variable that causes a problem with something similar that is not impacted by the response variable. In case of the complex linear regression, the situation would be similar to the conventional one. We do not plan to discuss how to solve the problem in case of CLR in this monograph. But we believe that the methods from econometrics of real-valued models should be widely applicable here as well (e.g. using proxies or instrumental variables).

As for the multicollinearity issue, in the real-valued statistics, it arises when least squares or maximum likelihood is used for estimation. It is, in a way, a technical issue: if the explanatory variables are linearly related it becomes difficult to split the impact of each of the explanatory variables on the response variable and thus challenging to correctly capture the relations between the explanatory variables and the response one.

In case of an OLS applied to a conventional real-valued regression model, the issue comes to inverting the matrix $\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1}$. If some of variables in $\mathbf{X}$ are strongly linearly related, the determinant of the matrix becomes close to zero and thus the inversion becomes challenging. Even if it is still possible to invert the matrix, the estimates of parameters become inefficient because small changes in the related explanatory variables may lead to substantial changes in the inversion.

In case of CLR, the situation is similar, but with some specific features. If the OLS is used for parameters estimation then a potential issue might arise from the inversion of $\left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1}$. In one of the special cases, this comes to the conjugate covariance between variables. If it is too high in comparison to the conjugate variances, then we would face the multicollinearity issue, which would make the estimates of parameters inefficient. More generally speaking, if a complex linear regression between an explanatory variable and other explanatory variables estimated using OLS captures the relations between variables well (i.e. having a high coefficient of determination), then we might be facing a multicollinearity problem. Similar problems might appear in case of CLS, but they come to a slightly different inversion, $\left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}}\right)^{-1}$ and thus relates to direct covariances rather than the conjugate ones. Finally, when it comes to the likelihood, due to the model formulation, the conventional real-valued relations between explanatory variables should be considered when multicollinearity is suspected. All of this means that a choice of an estimator can be dictated by the strength of the conventional Pearson's, direct and/or conjugate correlations (and more widely conventional linear model and/or complex linear model for explanatory variables estimated using OLS/CLS).


## Diagnostics

sigma & Sigma

R^2, R^2 - adjusted
R^2 for different parts or for the whole model
R^2 for covariances? Percentage of explained covariance between variables?

```{r}
# 1- cvar(e, method="conj") / cvar(y,method="conj")

# 1-covar(e) / covar(y)
```


Visual analysis


<!-- ## Forecasting -->


<!-- ## Examples of application (Production functions) -->

