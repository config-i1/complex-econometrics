#  Assumptions for Complex Linear Models
Similar to how it is done for the conventional real valued regression, we should discuss what assumptions are imposed on complex linear regression estimated using one of the methods discussed in the previous Section \@ref(mlcrEstimation). We will split all regression assumptions to three groups [similar to how it was done by @SvetunkovSBA]:

1. Model is correctly specified;
2. Residuals are independent and identically distributed (i.i.d.);
3. Explanatory variables are not correlated with anything but the response variable.

While the first two groups directly relate to the so called "true model", the last one refers to the estimation approaches discussed above: if the latter are violated then the estimation procedure will lead to issues in estimates of parameters. We should also point out that many of the assumptions discussed in this Chapter are very similar to the assumptions in the conventional regression, which is why we do not plan to cover them in this monograph in detail. An interested reader is advised to read Chapter 15 of @SvetunkovSBA. Instead, we will focus on some of the assumptions that are specific for complex-valued models.


## Model is correctly specified
This is one of the most general and most important groups of assumptions. It includes the following:

1. Model does not omit any important variables;
2. Model does not have redundant variables;
3. Variables are included in the model with appropriate transformations;
4. Residuals of the model do not contain outliers.

We briefly discuss them in this section.


### Model does not omit any important variables
This assumption implies that we have all the variables that can impact our response variable and that we have included them in the model. However, we always omit a lot of different variables that might impact the response one, but do not have a large effect on it. Still, if they do not play an important role in regression, they do not tend to cause issues. Formally speaking, the omitted variables should not be correlated with the ones that we include in the model, because otherwise the impact of the latter will not be captured correctly, causing bias in the estimates of parameters. It is not possible to test this assumption, so it can only be checked based on judgment of an analyst. If we violate this assumption, the estimates of parameters will typically be biased.


### Model does not have redundant variables
This is the situation opposite to the first one. It implies that we have included something that should not be there. If that happens, the redundant variable will explain the noise and thus our model will overfit the data and lead to inefficient estimates of parameters (i.e. variances of estimates will be larger than needed).


While the first two assumptions are universal for any statistical model, the third one has some special implications in case of CLR. This is because even simple transformations (such as taking logarithm of a variable) might lead to highly non-linear results in case of complex variables. Furthermore, transformations of separate parts of a complex variable are not equivalent to the transformations of the whole variable. For example, the logarithm of a complex variable $\underline{z}$ as shown in \@ref(eq:complexNumberLogarithm) is:
\begin{equation*}
    \ln \underline{z} = \ln r + i \phi ,
\end{equation*}
which is not equivalent to the complex variable $\ln x_r + \ln x_i$. This means that transformations of complex variables should be taken with additional care, and the task of finding the correct ones becomes more complicated than in the case of the conventional real-valued regression.

Finally, the presence of outliers in the residuals typically implies that either there is an error in recording the data, or the model omits an important variable (e.g. a dummy variable for an external event). Making sure that there are no outliers provides some assurance that there are no important missing variables in the model.


## Residuals are i.i.d.
The next group of assumptions has 6 elements in it:

1. Residuals are homoscedastic;
2. No autocorrelation in residuals;
3. Expectation of residuals is zero (no matter what);
4. The residuals follow an assumed distribution;
5. The distribution of the residuals does not change over time;
6. Residuals follow a circular distribution.

The first assumption implies for CLR that both direct and conjugate variances of the residuals are constant and do not change with any changes of variables. This also means that the covariance matrix of residuals $\Sigma$ stays the same no matter what. This assumption aligns well with a similar assumption of multivariate models, such as VAR or VETS, which will be discussed later in this book.

The second assumption only applies to time series data. In case of the real-valued model, it means that the residuals in the past do not impact the ones in the future. Typically, this effect appears because of the wrong specification of the model (e.g. correct transformations are not done or some autoregressive elements are missing). In case of the CLR, the idea is very similar, but now we are talking about complex relations between the residuals, which could arise, again, because of the wrong transformations or because of missing elements (e.g. Complex Autoregression, discussed later in this book).

While many other assumptions can be diagnosed in one way or another, the assumption (3) in some cases cannot be. For example, if we use OLS, the unconditional mean of the residuals will be equal to zero by design, which does not provide any useful information about the "true" model. However, this assumption can be analysed conditionally, e.g. conditional on the fitted values. If the conditional mean of the residuals is not constant then we can conclude that some important element was omitted by the model. In a way, the assumption becomes similar to the assumption about the omitted variables and/or autocorrelated residuals.

In the classical real-valued regression, the assumption (4) is not considered to be crucial and if the model is estimated using OLS typically comes to "the residuals follow the Normal distribution". In context of complex variables, the equivalent assumption would be that the complex residuals follow the Complex Normal distribution discussed in Section \@ref(distributionCNorm). If the model is formulated using matrix notations \@ref(eq:CLRSystemVectorFinal), the assumption transforms to "Multivariate Normal" one, discussed in Section \@ref(MVNorm). In anyway, this assumption is complementary to the main ones, even when it does not hold but all the others hold, the estimates of parameters should be unbiased, efficient and consistent. However, it becomes essential when the prediction interval needs to be generated from the model.

Building upon (4), the fifth assumption is applicable to time series models only and states more generally that we deal with one and the same assumption of residuals over time. This assumption is very difficult to test (if at all possible), so it is typically dropped from the discussion.

Finally, in signal processing literature, the assumption (6) means that the covariance between the residuals equals to zero and the variances of the real and the imaginary parts are equal. This implies that the direct variance of residuals is equal to zero. While this might be a suitable assumption in that discipline, there is no good rationale for this to hold universally for all CLR models. So, we will not consider this assumption as an important one in what follows.


## Explanatory variables are not correlated with anything but the response variable
The two assumptions in this group relate to the estimation of model rather than anything else:

1. No endogeneity
2. No multicollinearity

The first assumption comes to the idea that a linear regression model can only capture a one-directional relation, where an explanatory variable impacts the response variable. If the relation in the real life is bi-directional then the estimates of parameters of a regression model will be biased. In that case, the relation needs to be somehow made one-directional, for example by substituting the explanatory variable that causes a problem with something similar that is not impacted by the response variable. In case of the complex linear regression, the situation would be similar to the conventional one. We do not plan to discuss how to solve the problem in case of CLR in this monograph. But we believe that the methods from econometrics of real-valued models should be widely applicable here as well (e.g. using proxies or instrumental variables).

As for the multicollinearity issue, in the real-valued statistics, it arises when least squares or maximum likelihood is used for estimation. It is, in a way, a technical issue: if the explanatory variables are linearly related it becomes difficult to split the impact of each of the explanatory variables on the response variable and thus challenging to correctly capture the relations between the explanatory variables and the response one.

In case of an OLS applied to a conventional real-valued regression model, the issue comes to inverting the matrix $\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1}$. If some of variables in $\mathbf{X}$ are strongly linearly related, the determinant of the matrix becomes close to zero and thus the inversion becomes challenging. Even if it is still possible to invert the matrix, the estimates of parameters become inefficient because small changes in the related explanatory variables may lead to substantial changes in the inversion.

In case of CLR, the situation is similar, but with some specific features. If the OLS is used for parameters estimation then a potential issue might arise from the inversion of $\left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1}$. In one of the special cases, this comes to the conjugate covariance between variables. If it is too high in comparison to the conjugate variances, then we would face the multicollinearity issue, which would make the estimates of parameters inefficient. More generally speaking, if a complex linear regression between an explanatory variable and other explanatory variables estimated using OLS captures the relations between variables well (i.e. having a high coefficient of determination), then we might be facing a multicollinearity problem. Similar problems might appear in case of CLS, but they come to a slightly different inversion, $\left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}}\right)^{-1}$ and thus relates to direct covariances rather than the conjugate ones. Finally, when it comes to the likelihood, due to the model formulation, the conventional real-valued relations between explanatory variables should be considered when multicollinearity is suspected. All of this means that a choice of an estimator can be dictated by the strength of the conventional Pearson's, direct and/or conjugate correlations (and more widely conventional linear model and/or complex linear model for explanatory variables estimated using OLS/CLS).


## Diagnostics

sigma & Sigma

R^2, R^2 - adjusted
R^2 for different parts or for the whole model
R^2 for covariances? Percentage of explained covariance between variables?

```{r}
# 1- cvar(e, method="conj") / cvar(y,method="conj")

# 1-covar(e) / covar(y)
```


Visual analysis


<!-- ## Forecasting -->


<!-- ## Examples of application (Production functions) -->

