# Simple Complex Linear Regression {#simpleCLR}

```{r echo=FALSE}
options(scipen = 100)
```

We start with the simplest complex-valued model, a special case of a Complex Linear Regression (cLR), which we call "simple", analogous to how it is done in the real-valued domain, because it captures the relation between two variables. The main difference from the conventional Simple Linear Regression is that each of the variables is complex. This makes the model more complicated than the one in real-valued domain.

## Model formulation {#simpleCLRModel}
The simple Complex Linear Regression can be written as:
\begin{equation}
    \underline{y}_j = \underline{\beta}_0 + \underline{\beta}_1 \underline{x}_j + \underline{\epsilon}_j,
    (\#eq:SimpleCLRComplex)
\end{equation}
where $j$ is the index for the observation and every parameter and variable is a complex number, i.e. $\underline{y}_j = y_{r,j}+i y_{i,j}$ is the complex response variable, $\underline{x}_j = x_{r,j}+i x_{i,j}$ is the complex explanatory variable, $\underline{\beta}_{l} = \beta_{l,r} + i \beta_{l,i}$ is the $l$-th complex parameter and $\underline{\epsilon}_j = \epsilon_{r,j} + i \epsilon_{i,j}$ is the error term. For now, we do not make any specific assumptions about the distribution of the complex error term, we will come back to this later in this chapter. Inserting the values above in \@ref(eq:SimpleCLRComplex) leads to:
\begin{equation}
    y_{r,j}+i y_{i,j} = (\beta_{0,r} + i \beta_{0,i}) + (\beta_{1,r} + i \beta_{1,i}) (x_{r,j}+i x_{i,j}) + (\epsilon_{r,j} + i \epsilon_{i,j}),
    (\#eq:SimpleCLR)
\end{equation}
which is in fact a multivariate model, capturing how the change of values in pair of variables $x_r$, $x_i$ leads to the change in the pair of variables $y_r$ and $y_i$. Given that any complex-valued equation can be represented as a system of two equations, the model \@ref(eq:SimpleCLR) can be represented as a system of two linear regressions:
\begin{equation}
    \begin{aligned}
        y_{r,j} = & \beta_{0,r} + \beta_{1,r} x_{r,j} - \beta_{1,i} x_{i,j} + \epsilon_{r,j} \\
        y_{i,j} = & \beta_{0,i} + \beta_{1,r} x_{i,j} + \beta_{1,i} x_{r,j} + \epsilon_{i,j}
    \end{aligned}
    (\#eq:SimpleCLRSystem)
\end{equation}
This model captures a very specific dynamics between the real and imaginary parts, given that they share the same set of parameters for the slope. But most importantly it shows how each complex variable $\underline{y}$ relates to variable $\underline{x}$ in a four dimensional space. Note that the real and imaginary parts of $\underline{y}$ can be exchanged without a serious impact on the model: in that case the values of parameters would change, but the relation between $\underline{x}$ and $\underline{y}$ would stay the same. Similarly, changing $x_r$ with $x_i$ would only lead to different estimates of parameters, the general relation would hold.

Given that we deal with a sample of values, the model \@ref(eq:SimpleCLRComplex) can be represented in a vector form for all observations $j$ from 1 to $n$ (based on discussion in Subsection \@ref(vectorComplexVariables)):
\begin{equation}
    \underline{\mathbf{y}} = \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\boldsymbol{\epsilon}} ,
    (\#eq:SimpleCLRVector)
\end{equation}
where $\underline{\mathbf{y}}=\begin{pmatrix} \underline{y}_1 \\ \underline{y}_2 \\ \vdots \\ \underline{y}_n \end{pmatrix}$, $\underline{\mathbf{X}} = \begin{pmatrix} 1 & \underline{x}_1 \\ 1 & \underline{x}_2 \\ \vdots \\ 1 & \underline{x}_n \end{pmatrix}$, $\underline{\boldsymbol{\beta}} = \begin{pmatrix} \underline{\beta}_0 \\ \underline{\beta}_1 \end{pmatrix}$ and $\underline{\boldsymbol{\epsilon}} = \begin{pmatrix} \underline{\epsilon}_1 \\ \underline{\epsilon}_2 \\ \vdots \\ \underline{\epsilon}_n \end{pmatrix}$ and each element of these objects is a complex number.

Going even further, using the vector and matrix representations of complex variables, the same system of equations \@ref(eq:SimpleCLRSystem) can be rewritten as:
\begin{equation}
    \begin{pmatrix} y_{r,j} \\ y_{i,j} \end{pmatrix} = \begin{pmatrix} \beta_{0,r} \\ \beta_{0,i} \end{pmatrix} + \begin{pmatrix} x_{r,j} & -x_{i,j} \\ x_{i,j} & x_{r,j} \end{pmatrix} \begin{pmatrix} \beta_{1,r} \\ \beta_{1,i} \end{pmatrix} + \begin{pmatrix} \epsilon_{r,j} \\ \epsilon_{i,j} \end{pmatrix} ,
    (\#eq:SimpleCLRSystemVector01)
\end{equation}
or uniting all the parameters in one vector:
\begin{equation}
    \begin{pmatrix} y_{r,j} \\ y_{i,j} \end{pmatrix} = \begin{pmatrix} 1 & 0 & x_{r,j} & -x_{i,j} \\ 0 & 1 & x_{i,j} & x_{r,j} \end{pmatrix} \begin{pmatrix} \beta_{0,r} \\ \beta_{0,i} \\ \beta_{1,r} \\ \beta_{1,i} \end{pmatrix} + \begin{pmatrix} \epsilon_{r,j} \\ \epsilon_{i,j} \end{pmatrix} .
    (\#eq:SimpleCLRSystemVector02)
\end{equation}

This form can then be represented in a classical matrix notations:
\begin{equation}
    \mathbf{y}_j = \underset{\sim}{\mathbf{X}_j} {\boldsymbol{\beta}} + \boldsymbol{\epsilon}_j .
    (\#eq:SimpleCLRSystemVector03)
\end{equation}
And if we stack each of the vectors and matrices for each $j=1 \dots n$ we get even more compact form:
\begin{equation}
    \mathbf{Y} = \underset{\sim}{\mathbf{X}} \boldsymbol{\beta} + \mathbf{E} ,
    (\#eq:SimpleCLRSystemVectorFinal)
\end{equation}
where $\mathbf{Y}=\begin{pmatrix}\mathbf{y}_1 \\ \mathbf{y}_2\\ \vdots \\ \mathbf{y}_n \end{pmatrix}$, $\underset{\sim}{\mathbf{X}}=\begin{pmatrix} \underset{\sim}{\mathbf{X}_1} \\ \underset{\sim}{\mathbf{X}_2} \\ \vdots \\ \underset{\sim}{\mathbf{X}_n} \end{pmatrix}$ and $\mathbf{E}=\begin{pmatrix}\boldsymbol{\epsilon}_1 \\ \boldsymbol{\epsilon}_2\\ \vdots \\ \boldsymbol{\epsilon}_n \end{pmatrix}$. The form \@ref(eq:SimpleCLRSystemVectorFinal) shows the connection between the complex regression and the conventional real valued one and can be used in model estimation using the conventional methods.

Both representations \@ref(eq:SimpleCLRVector) and \@ref(eq:SimpleCLRSystemVectorFinal) can be used for model estimation with the only difference that the former uses complex variables explicitly, while the latter avoids them, substituting them with real-valued matrices.


## Estimation {#SCLREstimation}
Whenever we estimate a model, its formula needs to be amended to "true" parameters by their sample estimates. So, for the cLR, the model applied to the data should be written as:
\begin{equation}
    \underline{y}_j = \underline{b}_0 + \underline{b}_1 \underline{x}_j + \underline{e}_j,
    (\#eq:SimpleCLRComplexEstimated)
\end{equation}
or
\begin{equation}
    y_{r,j}+i y_{i,j} = (b_{0,r} + i b_{0,i}) + (b_{1,r} + i b_{1,i}) (x_{r,j}+i x_{i,j}) + (e_{r,j} + i e_{i,j}),
    (\#eq:SimpleCLREstimated)
\end{equation}
or
\begin{equation}
    \begin{aligned}
        y_{r,j} = & b_{0,r} + b_{1,r} x_{r,j} - b_{1,i} x_{i,j} + e_{r,j} \\
        y_{i,j} = & b_{0,i} + b_{1,r} x_{i,j} + b_{1,i} x_{r,j} + e_{i,j}
    \end{aligned}
    (\#eq:SimpleCLRSystemEstimated)
\end{equation}
or equivalently in matrix notations:
\begin{equation}
    \mathbf{y}_j = \underset{\sim}{\mathbf{X}_j} \boldsymbol{b} + \boldsymbol{e}_j ,
    (\#eq:SimpleCLRSystemVector03Estimated)
\end{equation}
where $\underline{b_{l}}$, $b_{l,r}+ib_{l,i}$ and $\boldsymbol{b}$ are the estimates of respective $\underline{\beta_l}$, $\beta_{l,r} + i \beta_{l,i}$ and $\boldsymbol{\beta}$ and $\underline{e}_j$, $e_{r,j} + i e_{i,j}$ and $\boldsymbol{e}_j$ are the residuals of the model. There are many approaches for estimation of parameters of mathematical models. In this monograph, we focus only on the three of them: Ordinary Least Squares (OLS), Complex Least Squares (CLS), and Likelihood.


### Ordinary Least Squares {#SCLREstimationOLS}
We start with the conventional Ordinary Least Squares (OLS), which relies on the multiplication by conjugate number. To our knowledge, @VandenBos1994 was the first paper that discussed it in the complex-valued domain. For the simple cLR, it comes to minimising the following loss based on the residuals:
\begin{equation}
    \sum_{j=1}^n (\underline{e}_j \underline{\tilde{e}}_j) = \sum_{j=1}^n (e_{r,j}^2 + e_{i,j}^2).
    (\#eq:SimpleCLROLSLoss)
\end{equation}
The residuals can be substituted as: $\underline{e}_j = \underline{y}_j - \underline{b}_0 - \underline{b}_1 \underline{x}_j$ and $\underline{\tilde{e}}_j = \underline{\tilde{y}}_j - \underline{\tilde{b}}_0 - \underline{\tilde{b}}_1 \underline{\tilde{x}}_j$ to get:
\begin{equation}
    \sum_{j=1}^n (\underline{y}_j - \underline{b}_0 - \underline{b}_1 \underline{x}_j) (\underline{\tilde{y}}_j - \underline{\tilde{b}}_0 - \underline{\tilde{b}}_1 \underline{\tilde{x}}_j),
    (\#eq:SimpleCLROLSLoss01)
\end{equation}
which after opening the brackets becomes:
\begin{equation}
    \sum_{j=1}^n \left(\underline{y}_j \underline{\tilde{y}}_j - \underline{y}_j \underline{\tilde{b}}_0 - \underline{y}_j \underline{\tilde{b}}_1 \underline{\tilde{x}}_j - \underline{b}_0\underline{\tilde{y}}_j + \underline{b}_0 \underline{\tilde{b}}_0 + \underline{b}_0 \underline{\tilde{b}}_1 \underline{\tilde{x}}_j - \underline{b}_1 \underline{x}_j \underline{\tilde{y}}_j + \underline{b}_1 \underline{x}_j \underline{\tilde{b}}_0 + \underline{b}_1 \underline{x}_j \underline{\tilde{b}}_1 \underline{\tilde{x}}_j \right) .
    (\#eq:SimpleCLROLSLoss02)
\end{equation}
In order to minimise the sum of squared errors \@ref(eq:SimpleCLROLSLoss), we need to take derivative of \@ref(eq:SimpleCLROLSLoss02) with respect to each of the parameters $b_{0,r}$, $b_{0,i}$, $b_{1,r}$ and $b_{1,i}$ and equate each of the resulting formulae to zero to find the extrema:
\begin{equation}
    \begin{aligned}
        & \frac{d \sum_{j=1}^n (\underline{e}_j \underline{\tilde{e}}_j)}{d b_{0,r}} = 0 \\
        & \frac{d \sum_{j=1}^n (\underline{e}_j \underline{\tilde{e}}_j)}{d b_{0,i}} = 0 \\
        & \frac{d \sum_{j=1}^n (\underline{e}_j \underline{\tilde{e}}_j)}{d b_{1,r}} = 0 \\
        & \frac{d \sum_{j=1}^n (\underline{e}_j \underline{\tilde{e}}_j)}{d b_{1,i}} = 0
    \end{aligned}
    (\#eq:SimpleCLROLSLossSystem01)
\end{equation}
to get:
\begin{equation}
    \begin{aligned}
        & - \sum_{j=1}^n \underline{y}_j - \sum_{j=1}^n \underline{\tilde{y}}_j + 2 n b_{0,r} + \underline{\tilde{b}}_1 \sum_{j=1}^n \underline{\tilde{x}}_j + \underline{b}_1 \sum_{j=1}^n \underline{x}_j = 0 \\
        & i \sum_{j=1}^n \underline{y}_j - i \sum_{j=1}^n \underline{\tilde{y}}_j + 2 n b_{0,i} + i \underline{\tilde{b}}_1 \sum_{j=1}^n \underline{\tilde{x}}_j - i \underline{b}_1 \sum_{j=1}^n \underline{x}_j = 0 \\
        & - \sum_{j=1}^n \underline{y}_j \underline{\tilde{x}}_j + \underline{b}_0 \sum_{j=1}^n \underline{\tilde{x}}_j - \sum_{j=1}^n \underline{x}_j \underline{\tilde{y}}_j + \underline{\tilde{b}}_0 \sum_{j=1}^n \underline{x}_j + 2 b_{1,r} \sum_{j=1}^n \underline{x}_j \underline{\tilde{x}}_j = 0 \\
        & i \sum_{j=1}^n \underline{y}_j \underline{\tilde{x}}_j - i \underline{b}_0 \sum_{j=1}^n \underline{\tilde{x}}_j - i \sum_{j=1}^n \underline{x}_j \underline{\tilde{y}}_j + i \underline{\tilde{b}}_0 \sum_{j=1}^n \underline{x}_j + 2 b_{1,i} \sum_{j=1}^n \underline{x}_j \underline{\tilde{x}}_j = 0 .
    \end{aligned}
    (\#eq:SimpleCLROLSLossSystem02)
\end{equation}
Solving the system of equations \@ref(eq:SimpleCLROLSLossSystem02) gives the following formulae for the parameters of the model [@Svetunkov2012]:
\begin{equation}
    \begin{aligned}
        & \underline{b}_1 = \frac{\sum_{j=1}^n (\underline{y}_j-\underline{\hat{\mu}}_{\underline{y}}) (\underline{\tilde{x}}_j-\hat{\tilde{\mu}}_{\underline{x}})}{\sum_{j=1}^n (\underline{x}_j-\underline{\hat{\mu}}_{\underline{x}}) (\underline{\tilde{x}}_j-\hat{\tilde{\mu}}_{\underline{x}})} \\
        & \underline{b}_0 = \frac{1}{n} \sum_{j=1}^n \underline{y}_j - \underline{b}_1 \sum_{j=1}^n \underline{x}_j ,
    \end{aligned}
    (\#eq:SimpleCLROLSLossParameters)
\end{equation}
which can also be written in terms of conjugate moments as:
\begin{equation}
    \begin{aligned}
        & \underline{b}_1 = \frac{\hat{\sigma}_{\underline{x},\underline{y}}}{\hat{\sigma}_{\underline{x}}^2} \\
        & \underline{b}_0 = \underline{\hat{\mu}}_{\underline{y}} - \underline{b}_1 \underline{\hat{\mu}}_{\underline{x}} ,
    \end{aligned}
    (\#eq:SimpleCLROLSLossParametersMoments)
\end{equation}
or after expanding the moments (based on formulae from Section \@ref(crvSecondMoment)) as:
\begin{equation}
        \underline{b}_1 = \frac{\hat{\sigma}_{x_r, y_r} + \hat{\sigma}_{x_i, y_i} + i (\hat{\sigma}_{x_r, y_i} - \hat{\sigma}_{x_i, y_r})}{\hat{\sigma}_{x_r}^2 + \hat{\sigma}_{x_i}^2} .
    (\#eq:SimpleCLROLSLossParametersMomentsExpanded)
\end{equation}

As we can see, the formula \@ref(eq:SimpleCLROLSLossParametersMoments) is similar to the one that is typically used for the conventional simple linear regression with the only difference that the parameters in \@ref(eq:SimpleCLROLSLossParametersMoments) are complex and that each of the moments in \@ref(eq:SimpleCLROLSLossParametersMoments) is a moment for respective complex variable.

It is apparent what the estimates of parameters \@ref(eq:SimpleCLROLSLossParametersMoments) give: they minimise the loss \@ref(eq:SimpleCLROLSLoss), thus in the case, when $\mathrm{E}(\underline{e}_j)=0$ they minimise variances of real and imaginary parts of the complex residuals. Coming back to Figure \@ref(fig:crvMomentSecondVariance) from Subsection \@ref(crvSecondMoment), this also implies that they minimise the hypotenuse in the figure, thus moving the residuals closer to the middle line. So, if the real and imaginary parts of the residuals are correlated in the "true" model, the OLS will automatically take care of this situation, emphasising the correlation. However, it is worth noting that the OLS ignores the potential covariance between the real and imaginary parts, which in some cases might be a desirable property, but in the others might cause issues.

In terms of properties of the estimator, it can be shown that $\underline{b}_1$ equals to:
\begin{equation}
        \underline{b}_1 = \frac{\mathrm{cov}(\underline{\tilde{x}}, \underline{y})}{\mathrm{cov}(\underline{\tilde{x}},\underline{x})} = \frac{\mathrm{cov}(\underline{\tilde{x}}, \underline{\beta}_0 + \underline{\beta}_1 \underline{x} + \underline{\epsilon})}{\mathrm{cov}(\underline{\tilde{x}},\underline{x})} = \frac{\mathrm{cov}(\underline{\tilde{x}}, \underline{\beta}_0) + \mathrm{cov}(\underline{\tilde{x}}, \underline{\beta}_1 \underline{x}) + \mathrm{cov}(\underline{\tilde{x}}, \underline{\epsilon})}{\mathrm{cov}(\underline{\tilde{x}},\underline{x})},
    (\#eq:SimpleCLROLSb1Value01)
\end{equation}
which then simplifies to:
\begin{equation}
        \underline{b}_1 = \underline{\beta}_1 + \frac{\mathrm{cov}(\underline{\tilde{x}}, \underline{\epsilon})}{\mathrm{cov}(\underline{\tilde{x}},\underline{x})} = \underline{\beta}_1 + \frac{\hat{\sigma}_{\underline{x},\underline{\epsilon}}}{\hat{\sigma}_{\underline{x}}^2} ,
    (\#eq:SimpleCLROLSb1Value02)
\end{equation}
or:
\begin{equation}
        \underline{b}_1 = \underline{\beta}_1 + \frac{\hat{\sigma}_{x_r, \epsilon_r} + \hat{\sigma}_{x_i, \epsilon_i} + i (\hat{\sigma}_{x_r, \epsilon_i} - \hat{\sigma}_{x_i, \epsilon_r})}{\hat{\sigma}_{x_r}^2 + \hat{\sigma}_{x_i}^2} .
    (\#eq:SimpleCLROLSb1Value03)
\end{equation}
On small samples, the covariance between the true error and the available $\underline{x}$ can be non-zero, implying that the value of the estimated parameter would differ from the true one. If the basic regression assumptions hold, in the population estimated covariances will converge to their true values and then ${\sigma}_{x_r, \epsilon_r} = {\sigma}_{x_i, \epsilon_i} = {\sigma}_{x_r, \epsilon_i} = {\sigma}_{x_i, \epsilon_r}=0$, which implies that the expectation of $\underline{b}_1$ is:
\begin{equation}
        \mathrm{E}(\underline{b}_1) = \underline{\beta}_1 .
    (\#eq:SimpleCLROLSb1Expectation)
\end{equation}
This implies that OLS gives unbiased estimates of the slope parameter. The same property can be shown to hold for the intercept $\underline{b}_0$.

The same formula \@ref(eq:SimpleCLROLSb1Value03) can be rewritten separately for the real and imaginary parts of the parameter:
\begin{equation}
    \begin{aligned}
        b_{1,r} = & \beta_{1,r} + \frac{\hat{\sigma}_{x_r, \epsilon_r} + \hat{\sigma}_{x_i, \epsilon_i}}{\hat{\sigma}_{x_r}^2 + \hat{\sigma}_{x_i}^2} \\
        b_{1,i} = & \beta_{1,i} + \frac{\hat{\sigma}_{x_r, \epsilon_i} - \hat{\sigma}_{x_i, \epsilon_r}}{\hat{\sigma}_{x_r}^2 + \hat{\sigma}_{x_i}^2} ,
    \end{aligned}
    (\#eq:SimpleCLROLSb1Value04)
\end{equation}
which shows what specific covariances impact different parts of the slope parameter.

As for the variance of $\underline{b}_1$, for the real part of the parameter, it can be shown to be equal to:
\begin{equation}
    \mathrm{V}(b_{1,r}) = \mathrm{V}\left(\frac{\hat{\sigma}_{x_r, \epsilon_r} + \hat{\sigma}_{x_i, \epsilon_i}}{\hat{\sigma}_{x_r}^2 + \hat{\sigma}_{x_i}^2}\right) .
    (\#eq:SimpleCLROLSb1Variance)
\end{equation}
The variance for the imaginary part will be similar. We will need this variance to understand how efficient the estimators are. But we do not expand it further, because the formula \@ref(eq:SimpleCLROLSb1Variance) is sufficient for the comparison of OLS with other estimators.


### Complex Least Squares {#SCLREstimationCLS}
An alternative estimation technique involves the minimisation of a rather exotic loss function:
\begin{equation}
    \sum_{j=1}^n (\underline{e}_j \underline{e}_j) = \sum_{j=1}^n (e_{r,j}^2 - e_{i,j}^2 + i 2 e_{r,j} e_{i,j}),
    (\#eq:SimpleCLRCLSLoss)
\end{equation}
for which in case of $\mathrm{E}(\underline{e}_j)=0$, the real part corresponds to the difference between the variances of the real and imaginary parts of the residuals, while the imaginary part corresponds to the covariance between them. This loss can be considered exotic, because its value is a complex number. It is difficult to explain how one can minimise a complex number, but from what follows, we show that the estimation technique has some meaning, works and gives adequate estimates of parameters.

The logic for the derivation of CLS is similar to OLS. We expand \@ref(eq:SimpleCLRCLSLoss) to:
\begin{equation}
    \sum_{j=1}^n (\underline{e}_j \underline{e}_j) = \sum_{j=1}^n \underline{y}_j^2 + n \underline{b}_0^2 + \underline{b}_1^2 \sum_{j=1}^n \underline{x}_j^2 - 2 \underline{b}_0 \sum_{j=1}^n \underline{y}_j - 2 \underline{b}_1 \sum_{j=1}^n \underline{x}_j \underline{y}_j + 2 \underline{b}_0 \underline{b}_1 \sum_{j=1}^n \underline{x}_j 
    (\#eq:SimpleCLRCLSLoss02)
\end{equation}
and then take derivatives of \@ref(eq:SimpleCLRCLSLoss02) with respect to parameter $\underline{b}_0$ and $\underline{b}_1$ and then equate the resulting formulae to zero:
\begin{equation}
    \begin{aligned}
        & \frac{d \sum_{j=1}^n (\underline{e}_j^2)}{d \underline{b}_0} = 0 \\
        & \frac{d \sum_{j=1}^n (\underline{e}_j^2)}{d \underline{b}_1} = 0 .
    \end{aligned}
    (\#eq:SimpleCLRCLSLossSystem01)
\end{equation}
The derivatives \@ref(eq:SimpleCLRCLSLossSystem01) give the following system of complex equations:
\begin{equation}
    \begin{aligned}
        & 2 n \underline{b}_0 - 2 \sum_{j=1}^n \underline{y}_j + 2 \underline{b}_1 \sum_{j=1}^n \underline{x}_j = 0 \\
        & 2 \underline{b}_1 \sum_{j=1}^n \underline{x}_j^2 - 2 \sum_{j=1}^n \underline{x}_j \underline{y}_j + 2 \underline{b}_0 \sum_{j=1}^n \underline{x}_j = 0 .
    \end{aligned}
    (\#eq:SimpleCLRCLSLossSystem02)
\end{equation}
The solution for this system of equations, as it was shown by @Svetunkov2012, is:
\begin{equation}
    \begin{aligned}
        & \underline{b}_1 = \frac{\sum_{j=1}^n (\underline{y}_{j}-\underline{\hat{\mu}}_{\underline{y}}) (\underline{x}_j-\underline{\hat{\mu}}_{\underline{x}})}{\sum_{j=1}^n (\underline{x}_j-\underline{\hat{\mu}}_{\underline{x}})^2} \\
        & \underline{b}_0 = \frac{1}{n} \sum_{j=1}^n \underline{y}_j - \underline{b}_1 \sum_{j=1}^n \underline{x}_j ,
    \end{aligned}
    (\#eq:SimpleCLRCLSLossParameters)
\end{equation}
or in terms of direct moments for complex random variables:
\begin{equation}
    \begin{aligned}
        & \underline{b}_1 = \frac{\hat{\varsigma}_{\underline{x},\underline{y}}}{\hat{\varsigma}_{\underline{x}}^2} \\
        & \underline{b}_0 = \underline{\hat{\mu}}_{\underline{y}} - \underline{b}_1 \underline{\hat{\mu}}_{\underline{x}} ,
    \end{aligned}
    (\#eq:SimpleCLRCLSLossParametersMoments)
\end{equation}
or after inserting the values for direct variance and covariance (from Section \@ref(crvSecondMoment)):
\begin{equation}
    \begin{aligned}
        & \underline{b}_1 = \frac{\hat{\sigma}_{x_r, y_r} - \hat{\sigma}_{x_i, y_i} + i (\hat{\sigma}_{x_i, y_r} + \hat{\sigma}_{x_r, y_i})}{\hat{\sigma}_{x_r}^2 - \hat{\sigma}_{x_i}^2 + i2 \hat{\sigma}_{x_r,x_i}} \\
        & \underline{b}_0 = \underline{\hat{\mu}}_{\underline{y}} - \underline{b}_1 \underline{\hat{\mu}}_{\underline{x}} .
    \end{aligned}
    (\#eq:SimpleCLRCLSLossParametersMomentsExpanded)
\end{equation}
Similarly to how it was done with OLS estimate of the slope, we can consider the estimate $\underline{b}_1$, expanding the covariance in the numerator of \@ref(eq:SimpleCLRCLSLossParametersMoments):
\begin{equation}
        \underline{b}_1 = \frac{\mathrm{cov}(\underline{x},\underline{y})}{V(\underline{x})} = \frac{\mathrm{cov}(\underline{x},\underline{\beta}_0 + \underline{\beta}_1 \underline{x} + \underline{\epsilon})}{V(\underline{x})},
    (\#eq:SimpleCLRCLSb1Expansion01)
\end{equation}
which after some simplifications becomes:
\begin{equation}
        \underline{b}_1 = \underline{\beta}_1 + \frac{\mathrm{cov}(\underline{x}, \underline{\epsilon})}{V(\underline{x})} ,
    (\#eq:SimpleCLRCLSb1Expansion02)
\end{equation}
which can be expanded to:
\begin{equation}
    \underline{b}_1 = \underline{\beta}_1 + \frac{\mathrm{cov}(x_r, \epsilon_r) - \mathrm{cov}(x_i, \epsilon_i) + i (\mathrm{cov}(x_r, \epsilon_i) + \mathrm{cov}(x_i, \epsilon_r))}{V(x_r) - V(x_i) + 2i \mathrm{cov}(x_r, x_i)} . 
    (\#eq:SimpleCLRCLSb1Expansion03)
\end{equation}
Note that both numerator and denominator of \@ref(eq:SimpleCLRCLSb1Expansion03) are complex numbers. In order to have a proper split into real and imaginary parts we need to multiply the ratio by the complex number conjugate to the denominator:
\begin{equation}
\resizebox{0.9\textwidth}{!}{$
        \underline{b}_1 = \underline{\beta}_1 + \frac{\left(\mathrm{cov}(x_r, \epsilon_r) - \mathrm{cov}(x_i, \epsilon_i) + i (\mathrm{cov}(x_r, \epsilon_i) + \mathrm{cov}(x_i, \epsilon_r))\right)\left(V(x_r) - V(x_i) - 2i \mathrm{cov}(x_r, x_i)\right)}{\left(V(x_r) - V(x_i)\right)^2 + 4 \mathrm{cov}(x_r, x_i)^2} $}.
    (\#eq:SimpleCLRCLSb1Expansion04)
\end{equation}
This then can be split into two parts:
\begin{equation}
\resizebox{0.9\textwidth}{!}{$
    \begin{aligned}
        b_{1,r} = & \beta_{1,r} + \frac{\left(\mathrm{cov}(x_r, \epsilon_r) - \mathrm{cov}(x_i, \epsilon_i)\right) \left(V(x_r) - V(x_i)\right) - 2 \mathrm{cov}(x_r, x_i) \left(\mathrm{cov}(x_r, \epsilon_i) + \mathrm{cov}(x_i, \epsilon_r)\right)}{\left(V(x_r) - V(x_i)\right)^2 + 4 \mathrm{cov}(x_r, x_i)^2} \\
        b_{1,i} = & \beta_{1,i} + \frac{\left( \mathrm{cov}(x_r, \epsilon_i) + \mathrm{cov}(x_i, \epsilon_r)\right)\left(V(x_r) - V(x_i) \right) + 2 \mathrm{cov}(x_r, x_i) \left(\mathrm{cov}(x_i, \epsilon_i) - \mathrm{cov}(x_r, \epsilon_r)\right)}{\left(V(x_r) - V(x_i)\right)^2 + 4 \mathrm{cov}(x_r, x_i)^2} 
    \end{aligned}$}
    (\#eq:SimpleCLRCLSb1Expansion05)
\end{equation}
or
\begin{equation}
    \begin{aligned}
        b_{1,r} = & \beta_{1,r} + \frac{\left(\hat{\sigma}_{x_r, \epsilon_r} - \hat{\sigma}_{x_i, \epsilon_i}\right) \left(\hat{\sigma}_{x_r}^2 - \hat{\sigma}_{x_i}^2 \right) - 2 \hat{\sigma}_{x_r, x_i} \left(\hat{\sigma}_{x_r, \epsilon_i} + \hat{\sigma}_{x_i, \epsilon_r}\right)}{\left(\hat{\sigma}_{x_r}^2 - \hat{\sigma}_{x_i}^2\right)^2 + 4 \hat{\sigma}_{x_r, x_i}^2} \\
        b_{1,i} = & \beta_{1,i} + \frac{\left( \hat{\sigma}_{x_r, \epsilon_i} + \hat{\sigma}_{x_i, \epsilon_r}\right)\left(\hat{\sigma}_{x_r}^2 - \hat{\sigma}_{x_i}^2 \right) + 2 \hat{\sigma}_{x_r, x_i} \left(\hat{\sigma}_{x_i, \epsilon_i} - \hat{\sigma}_{x_r, \epsilon_r}\right)}{\left(\hat{\sigma}_{x_r}^2 - \hat{\sigma}_{x_i}^2\right)^2 + 4 \hat{\sigma}_{x_r, x_i}^2}
    \end{aligned}
    (\#eq:SimpleCLRCLSb1Expansion06)
\end{equation}
If we now consider the properties of $b_{1,r}$, we can see that it is an asymptotically unbiased estimate of $\beta_{1,r}$ when the basic regression assumptions hold (i.e. ${\sigma}_{x_r, \epsilon_r} = {\sigma}_{x_i, \epsilon_i} = {\sigma}_{x_r, \epsilon_i} = {\sigma}_{x_i, \epsilon_r}=0$). Similar holds for the $b_{1,i}$.

The variance of $b_{1,r}$ can be written as:
\begin{equation}
    \mathrm{V}(b_{1,r}) = \mathrm{V}\left(\frac{\left(\hat{\sigma}_{x_r, \epsilon_r} - \hat{\sigma}_{x_i, \epsilon_i}\right) \left(\hat{\sigma}_{x_r}^2 - \hat{\sigma}_{x_i}^2 \right) - 2 \hat{\sigma}_{x_r, x_i} \left(\hat{\sigma}_{x_r, \epsilon_i} + \hat{\sigma}_{x_i, \epsilon_r}\right)}{\left(\hat{\sigma}_{x_r}^2 - \hat{\sigma}_{x_i}^2\right)^2 + 4 \hat{\sigma}_{x_r, x_i}^2}\right) .
    (\#eq:SimpleCLRCLSb1Variance)
\end{equation}
We will come back to this variance, when comparing the efficiency of OLS and CLS estimators in Section \@ref(SCLREstimatorsComparison).

In order to better understand what is specifically minimised, when the formulae \@ref(eq:SimpleCLRCLSLossParametersMoments) are used for the estimation of parameters, we conduct a small experiment in R with the following code:

```{r echo=FALSE}
load("./data/CLSMDS.Rdata")
```

```{r eval=FALSE}
# Random seed for reproducibility
set.seed(41)
# Create real part of a c.r.v. x
xr <- rnorm(1000,0,10)
# Create a c.r.v. x
x <- complex(real=xr, imaginary=1.5*xr+rnorm(1000,0,10))
# Create a c.r.v. y
y <- (1.5 + 1.2i) * x +
    complex(real=rnorm(1000,0,10), imaginary=rnorm(1000,0,10))

# Define number of iterations and the matrix with the values
nsim <- 10000
clsValues <- matrix(NA, nsim, 4,
                    dimnames=list(NULL,
                                  c("b1r","b1i","CLSr","CLSi")))

# CLS loss function
clsLoss <- function(y, yHat){
    return(sum((y - yHat)**2))
}

# Loop for values of b1r from 0.52 to 2.5 and
# for b1i from 0.22 to 2.2
l <- 1
for(i in 1:(nsim/100)){
    for(j in 1:(nsim/100)){
        b <- complex(real=i/100*2+0.5, imaginary=j/100*2+0.2)
        yHat <- (10 + 15i) + b * x
        clsResult <- clsLoss(y, yHat)
        clsValues[l,1] <- Re(b);
        clsValues[l,2] <- Im(b);
        clsValues[l,3] <- Re(clsResult);
        clsValues[l,4] <- Im(clsResult);
        l <- l+1;
    }
}

# Estimate b1 using CLS
bOptimal <- ccov(x, y, method="direct") /
            cvar(x, method="direct")
# Produce fitted values
yHat <- bOptimal * x
# Calculate the loss
clsResult <- clsLoss(y, yHat)
```

In this experiment we generate complex variables $\underline{x}$ and $\underline{y}$ and then calculate values of the complex loss function based on a combination of values of the complex slope $\underline{b}_1$ (which we call just `b` in the code). After running the code above we end up with 10,000 values of the loss function and one additional, which corresponds to the optimal point according to \@ref(eq:SimpleCLRCLSLossParametersMoments). We can then produce several plots to better understand what is happening.

```{r clsScatter, fig.cap="Variety of CLS loss function values for different values of $\\underline{b}_1$.", echo=FALSE}
plot(clsValues[,3:4], type="p",
     xlab="Re(CLS loss)", ylab="Im(CLS loss)")
abline(h=0, col="grey", lty=2, lwd=2)
abline(v=0, col="grey", lty=2, lwd=2)
points(Re(clsResult), Im(clsResult), col="red", pch=19)
```

Figure \@ref(fig:clsScatter) shows a scatterplot of values of the complex loss \@ref(eq:SimpleCLRCLSLoss). The red point close to the origin corresponds to the estimate obtained using \@ref(eq:SimpleCLRCLSLossParametersMoments). As we can see, it is close to the origin, which implies that the minimisation of the loss \@ref(eq:SimpleCLRCLSLoss) is equivalent to making both real and imaginary parts of it close to zero. This means that CLS makes variances of real and imaginary residuals similar and shrinks the covariance between them to zero.

Another way of looking at how CLS works is to reduce its dimensionality via the Multidimensional Scaling [see, for example, @Borg2005] and then visualise it. The R code below is relatively slow but produces a solution for this task.

```{r eval=FALSE}
# Calculate distance matrix for all losses
# (including the CLS point)
clsMDSDistance <- dist(rbind(clsValues[,3:4],
                             c(Re(clsResult),Im(clsResult))))

# Do Multidimensional scaling
clsMDS <- cmdscale(clsMDSDistance, k=1)

# Create a data frame with the coordinates
clsMDSValues <- data.frame(z=clsMDS,
                           x=c(clsValues[,1],Re(bOptimal)),
                           y=c(clsValues[,2],Im(bOptimal)))

# Create a matrix with loss values for 3d plotting
clsMDSValuesZ <- matrix(clsMDSValues$z[-10001], 100, 100)
```

Then, to produce the 3d surface on the plane of (loss, $b_{1,r}$, $b_{1,i}$), we can use functions from the `plotly` package in R as shown in the code below:

```{r eval=FALSE}
plot_ly(z=clsMDSValuesZ,
        x=unique(clsMDSValues$x[-10001]),
        y=unique(clsMDSValues$y[-10001])) |>
    plotly::layout(scene=list(xaxis = list(title = "Re(b1)"),
                      yaxis = list(title = "Im(b1)"),
                      zaxis = list(title = "MDS of CLS loss"))) |>
    add_surface() |>
    add_trace(data = tail(clsMDSValues,1),
              x = ~x,
              y = ~y,
              z = ~z,
              mode = "markers",
              type = "scatter3d",
              marker = list(size = 10))
```

```{r plotlyCLS, echo=FALSE, fig.cap="Plot of the 3d surface for the MDS of the CLS loss."}
knitr::include_graphics("images/CLSPlot.png")
```

Figure \@ref(fig:plotlyCLS) demonstrates how the projection of the two-dimensional loss behaves for a variety of parameters of the regression. The orange dot in the middle corresponds to the one obtained via CLS. We can see that it corresponds to the loss being close to zero, and represents a 2-dimensional inflection point, where the surface bends in different directions.

Concluding this brief demonstration, we can see that despite being counter-intuitive, the CLS produces the estimates of parameters that lead to the residuals being close to spherical (normal if we assume normality), because the variances of real and imaginary parts of the residuals become closer to each other, while the covariance between them becomes close to zero.


### Likelihood {#SCLREstimationLikelihood}
Finally, another way of estimating the simple cLR is by assuming a distribution of the residuals and maximising the respective likelihood [@VandenBos1994]. Given the connection between the linear and the vector forms of the complex regression, we can assume that the error term follows a bivariate normal distribution with a covariance matrix:
\begin{equation}
    \boldsymbol{\Sigma}_{\epsilon} = \begin{pmatrix} \sigma_{\epsilon_r}^2 & \sigma_{\epsilon_r, \epsilon_i} \\ \sigma_{\epsilon_r, \epsilon_i} & \sigma_{\epsilon_i}^2 \end{pmatrix} .
    (\#eq:SimpleCLRLikelihoodSigma)
\end{equation}
The log-likelihood in this case can be written as:
\begin{equation}
	\ell(\boldsymbol{\theta}, \boldsymbol{\Sigma}_\epsilon | \mathbf{Y}) = -\frac{n}{2} \left( 2 \log(2 \pi) + \log | \boldsymbol{\Sigma}_\epsilon| \right) -\frac{1}{2} \sum_{j=1}^n \left( \boldsymbol{\epsilon}_j^\prime \boldsymbol{\Sigma}_\epsilon^{-1} \boldsymbol{\epsilon}_j \right) ,
    (\#eq:additiveLogLik)
\end{equation}
where $\boldsymbol{\theta}$ is the vector of estimated parameters and $\boldsymbol{\epsilon}_j = \begin{pmatrix} \epsilon_{r,j} \\  \epsilon_{i,j} \end{pmatrix}$ is the two dimensional error term. This likelihood is maximised when the covariance matrix \@ref(eq:SimpleCLRLikelihoodSigma) is estimated via:
\begin{equation}
	\hat{\boldsymbol{\Sigma}}_\epsilon = \frac{1}{n} \sum_{j=1}^{n} \boldsymbol{e}_j \boldsymbol{e}_j^\prime .
    (\#eq:Sigmaest)
\end{equation}
It can be shown that if the \@ref(eq:Sigmaest) is inserted in \@ref(eq:additiveLogLik) then the following concentrated log-likelihood can be obtained [see, for example, @Snyder2017; @Svetunkov2021a]:
\begin{equation}
	\ell^*(\boldsymbol{\theta}, \hat{\boldsymbol{\Sigma}}_\epsilon | \mathbf{Y}) = -\frac{n}{2} \left( 2 \log(2 \pi e) + \log | \hat{\boldsymbol{\Sigma}}_\epsilon | \right) .
	(\#eq:additiveLogLikConcentrated)
\end{equation}
It is obvious that the maximisation of the log-likelihood \@ref(eq:additiveLogLikConcentrated) is equivalent to minimising the generalised variance of the complex error term (as discussed in Subsection \@ref(crvSecondMoment)):
\begin{equation}
    \mathrm{GV} = |\hat{\boldsymbol{\Sigma}}_\epsilon| = \sigma_{\epsilon_r}^2 \sigma_{\epsilon_i}^2 - \sigma_{\epsilon_r, \epsilon_i}^2 .
    (\#eq:additiveLogLikConcentratedGV)
\end{equation}
The minimisation of GV in its turn implies the joint minimisation of variances of the real and imaginary parts of the residuals and a maximisation of the square of the covariance between them, thus making the residuals more linearly related. This loss function might be especially useful if we indeed can assume that the real and imaginary parts of the residuals are linearly related, and we want to use this information in the estimation.

The main difference between the Likelihood and the other two estimators discussed in this section is that the former can only be maximised via a numeric optimisation - there is no analytical solution for estimates of parameters via likelihood.


## Comparing different estimators for CLR {#SCLREstimatorsComparison}
Arguably, all three estimators discussed in this Section give adequate estimates of parameters, but inevitably they will have different efficiency and would be appropriate in different situations. In this subsection, we will explore the performance of estimators with the increase of sample size.

First, comparing variances of OLS and CLS estimates of $b_{1,r}$ \@ref(eq:SimpleCLRCLSb1Variance) with \@ref(eq:SimpleCLROLSb1Variance):
\begin{equation*}
    \begin{aligned}
        \mathrm{V}(b_{1,r}^{\mathrm{OLS}}) = & \mathrm{V}\left(\frac{\hat{\sigma}_{x_r, \epsilon_r} + \hat{\sigma}_{x_i, \epsilon_i}}{\hat{\sigma}_{x_r}^2 + \hat{\sigma}_{x_i}^2}\right) \\
        \mathrm{V}(b_{1,r}^{\mathrm{CLS}}) = & \mathrm{V}\left(\frac{\left(\hat{\sigma}_{x_r, \epsilon_r} - \hat{\sigma}_{x_i, \epsilon_i}\right) \left(\hat{\sigma}_{x_r}^2 - \hat{\sigma}_{x_i}^2 \right) - 2 \hat{\sigma}_{x_r, x_i} \left(\hat{\sigma}_{x_r, \epsilon_i} + \hat{\sigma}_{x_i, \epsilon_r}\right)}{\left(\hat{\sigma}_{x_r}^2 - \hat{\sigma}_{x_i}^2\right)^2 + 4 \hat{\sigma}_{x_r, x_i}^2}\right)
    \end{aligned}
\end{equation*}
we can conclude that there are some situations, when the estimates of parameters via CLS are less efficient than the estimates via OLS. There are some cases, when the variance \@ref(eq:SimpleCLRCLSb1Variance) is much higher than \@ref(eq:SimpleCLROLSb1Variance). For example, when the real and imaginary parts of $x$ are not correlated (i.e. $\hat{\sigma}_{x_r, x_i}=0$) and when the variances of the real and imaginary parts of $x$ are equal, the variance of $b_{1,r}$ explodes. However, there are also some cases, when CLS estimates are more efficient than the OLS ones. For example, when $\hat{\sigma}_{x_r, \epsilon_i}=\hat{\sigma}_{x_i, \epsilon_i}$ and $\hat{\sigma}_{x_r, x_i}=0$, the variance \@ref(eq:SimpleCLROLSb1Variance) will be equal to zero, while the variance \@ref(eq:SimpleCLRCLSb1Variance) will be greater than zero. So, in general, we cannot conclude which of the estimators will be more efficient, but we know that in different circumstances, they will have different properties.

We cannot compare the efficiency of the likelihood estimator directly with the OLS and CLS, but we know from the statistics literature [@referenceLikelihoodPaper] that likelihood gives consistent and asymptotically efficient estimates of parameters.

In terms of consistency, in one specific situation, when $\hat{\sigma}_{x_r}^2 = \hat{\sigma}_{x_i}^2$ and $\hat{\sigma}_{x_r, x_i}=0$, CLS might produce non-consistent estimates of parameters. This means that if we deal with explanatory variables with these properties, we should use either OLS or Likelihood.

Similar analysis can be done for the coefficient $b_{1,i}$, with the conclusions similar to the above, so we skip this discussion.

```{r echo=FALSE}
load("./data/clmExperimentsParameters.Rdata")
```

In order to do a more thorough comparison of the three estimators, we set up an experiment based on the following simple cLR:
\begin{equation*}
    y_r + i y_i = 10+15i + (2-1.5i) (x_r + i x_i) + (\epsilon_r + i \epsilon_i)
\end{equation*}
with several scenarios, parameters for which are shown in Table \@ref(tab:scenariosEstimators). They covered several important situations: when $x_r$ and $x_i$ are not correlated, have medium correlation and perfectly correlated, when their variances are similar or different and then when the real and imaginary parts of the error term are not correlated, have medium correlation, are equal to zero and when their variances have high or low values.

<!-- 1. $x_r$ and $x_i$ are **uncorrelated** and have **different variances** ($\sigma_{x_r}=10$, while $\sigma_{x_i}=20$). $\epsilon_r$ and $\epsilon_i$ are **uncorrelated** and have **similar variances** ($\sigma_{\epsilon_r}=\sigma_{\epsilon_i}=1.5$). -->

<!-- 2. $x_r$ and $x_i$ are **perfectly correlated** ($\sigma_{x_r}=10$, while $\sigma_{x_i}=15$). $\epsilon_r$ and $\epsilon_i$ are **uncorrelated** and have **similar variances** ($\sigma_{\epsilon_r}=\sigma_{\epsilon_i}=1.5$). -->

<!-- 3. $x_r$ and $x_i$ are **uncorrelated** and have **the same variances** ($\sigma_{x_r}=\sigma_{x_i}=20$). $\epsilon_r$ and $\epsilon_i$ are **uncorrelated** and have **the same variances** ($\sigma_{\epsilon_r}=\sigma_{\epsilon_i}=1.5$). -->

<!-- 4. $x_r$ and $x_i$ are **correlated** with **different variances** ($\sigma_{x_r}=10$, while $\sigma_{x_i}=15$). $\epsilon_r$ and $\epsilon_i$ are **equal to zero** (i.e. $x$ and $y$ have functional relation). -->

<!-- 5. $x_r$ and $x_i$ are **correlated** with **different variances** ($\sigma_{x_r}=10$, while $\sigma_{x_i}=15$). $\epsilon_r$ and $\epsilon_i$ are **correlated** with **different variances** ($\sigma_{\epsilon_r}=10$, $\sigma_{\epsilon_i}=8$). -->

<!-- 6. $x_r$ and $x_i$ are **correlated** with **different variances** ($\sigma_{x_r}=10$, while $\sigma_{x_i}=15$). $\epsilon_r$ and $\epsilon_i$ are **uncorrelated** and have **similar high variances** ($\sigma_{\epsilon_r}=\sigma_{\epsilon_i}=100$). -->

```{r scenariosEstimators, echo=FALSE}
scenarios <- matrix(c("0","1","0","medium","medium","medium",
                      # "different variances","different variances","same variance","different variances","different variances","different variances",
                      "$\\sigma_{x_r}=10$, $\\sigma_{x_i}=20$", "$\\sigma_{x_r}=10$, while $\\sigma_{x_i}=15$", "$\\sigma_{x_r}=\\sigma_{x_i}=20$", "$\\sigma_{x_r}=10$, $\\sigma_{x_i}=15$", "$\\sigma_{x_r}=10$, $\\sigma_{x_i}=15$", "$\\sigma_{x_r}=10$, $\\sigma_{x_i}=15$",
                      "0","0","0","NA","medium","0",
                      # "same variance","same variance","same variance","zero variance","different variances","same variance",
                      "$\\sigma_{\\epsilon_r}=\\sigma_{\\epsilon_i}=1.5$", "$\\sigma_{\\epsilon_r}=\\sigma_{\\epsilon_i}=1.5$", "$\\sigma_{\\epsilon_r}=\\sigma_{\\epsilon_i}=1.5$", "$\\sigma_{\\epsilon_r}=\\sigma_{\\epsilon_i}=0$", "$\\sigma_{\\epsilon_r}=10$, $\\sigma_{\\epsilon_i}=8$", "$\\sigma_{\\epsilon_r}=\\sigma_{\\epsilon_i}=100$"),
                    nrow=6, dimnames=list(paste0("Scenario ",1:6),
                                          c("cor($x_r$, $x_i$)", "std.dev. of $x_r$ and $x_i$", "cor($\\epsilon_r$, $\\epsilon_i$)", "std.dev. of $\\epsilon_r$ and $\\epsilon_i$")))
kableTable <- kableExtra::kable(scenarios, escape=FALSE, caption="Several scenarios for the comparison of estimators.")
kableExtra::kable_styling(kableTable, font_size=12, protect_latex=TRUE, latex_options="scale_down")
```

These six scenarios in Table \@ref(tab:scenariosEstimators) cover different theoretically possible situations and show how the three estimators behave in these conditions. The sample size was first set to 20 observations and then was increased iteratively by one observation until reaching 10,000. This should give us an understanding of how the estimators behave both on small samples and asymptotically. While we recorded both values of estimated intercept and slope, we are mainly interested in the latter, and the plots shown below focus on how the complex $\underline{b}_1$ is estimated. The experiments were done using `clm()` function from the `complex` package with `method` parameter equal to "OLS", "CLS" and "likelihood" for each of the respective estimators. A sample of the script used in the experiments is shown below (Scenario 1):

```{r eval=FALSE}
# Random seed for reproducibility
set.seed(41)
# Number of observations
obs <- 10000
x0 <- rnorm(obs,10,10)
x <- complex(real=x0,imaginary=rnorm(obs,0,20))

# Parameters of the model
b0 <- 10 + 15i
b1 <- 2-1.5i
# Response variable
y <- b0 + b1 * x + 1.5*complex(real=rnorm(obs,0,1),
                               imaginary=rnorm(obs,0,1))

# Form a matrix of variables
complexData <- cbind(y,x)

# Number of iterations and an array for parameters
nsim <- 9980
parametersValues <-
    array(NA, c(nsim,3,2),
          dimnames=list(NULL,
                        c("CLS","OLS","Likelihood"),
                        c("b0","b1")))

# The loop with cLR estimated with CLS, OLS and Likelihood
for(i in 1:nsim){
    test <- clm(y~x, complexData, loss="CLS",
                subset=sample(c(1:obs),20+i))
    parametersValues[i,1,] <- coef(test)
    test <- clm(y~x, complexData, loss="OLS",
                subset=sample(c(1:obs),20+i))
    parametersValues[i,2,] <- coef(test)
    test <- clm(y~x, complexData, loss="likelihood",
                subset=sample(c(1:obs),20+i))
    parametersValues[i,3,] <- coef(test)
}
```

```{r echo=FALSE}
# Function produces plots based on the provided data
parametersPlots <- function(data){
    lineColour <- rgb(255/255,165/255,0,0.75)
    methods <- c("CLS","OLS","Likelihood")
    par(mfcol=c(3,2), mar=rep(0.1,4), oma=c(5,5,2,2), xaxt="s",yaxt="s",cex.main=1.5)
    plot(Re(data[,2,2]), xlab="Sample size", ylab=TeX("Re($\\underline{b}_1$)"),
         ylim=c(1.5,2.5), type="l", axes=F)
    abline(h=2, col=lineColour, lwd=2)
    axis(side=2)
    box(col="black", lwd=1)
    mtext(methods[2], side=2, line=3, at=2.5/3, outer=TRUE)
    
    plot(Re(data[,1,2]), xlab="Sample size", ylab=TeX("Re($\\underline{b}_1$)"),
         ylim=c(1.5,2.5), type="l", axes=F)
    abline(h=2, col=lineColour, lwd=2)
    axis(side=2)
    box(col="black", lwd=1)
    mtext(methods[1], side=2, line=3, outer=TRUE)
    
    plot(Re(data[,3,2]), xlab="Sample size", ylab=TeX("Re($\\underline{b}_1$)"),
         ylim=c(1.5,2.5), type="l", axes=F)
    abline(h=2, col=lineColour, lwd=2)
    axis(side=2)
    axis(side=1)
    box(col="black", lwd=1)
    mtext(methods[3], side=2, line=3, at=0.5/3, outer=TRUE)
    mtext(TeX("Re($\\underline{b}_1$)"), side=1, line=3, at=0.5/2, outer=TRUE);
    
    plot(Im(data[,2,2]), xlab="Sample size", ylab=TeX("Im($\\underline{b}_1$)"),
         ylim=c(-3,0), type="l", axes=F)
    abline(h=-1.5, col=lineColour, lwd=2)
    axis(side=4)
    box(col="black", lwd=1)
    plot(Im(data[,1,2]), xlab="Sample size", ylab=TeX("Im($\\underline{b}_1$)"),
         ylim=c(-3,0), type="l", axes=F)
    abline(h=-1.5, col=lineColour, lwd=2)
    axis(side=4)
    box(col="black", lwd=1)
    plot(Im(data[,3,2]), xlab="Sample size", ylab=TeX("Im($\\underline{b}_1$)"),
         ylim=c(-3,0), type="l", axes=F)
    abline(h=-1.5, col=lineColour, lwd=2)
    axis(side=4)
    axis(side=1)
    box(col="black", lwd=1)
    mtext(TeX("Im($\\underline{b}_1$)"), side=1, line=3, at=1.5/2, outer=TRUE)
}
```

Figure \@ref(fig:parametersUCDV) demonstrates how estimates of parameters change with the increase of sample size in Scenario 1.

```{r parametersUCDV, fig.cap="Estimation of parameters using OLS, CLS and likelihood and their convergence to the true value of $\\underline{\\beta_1}=2-1.5i$ (orange horizontal lines on the plot). Scenario 1.", echo=FALSE, fig.height=3, fig.width=8}
parametersPlots(parametersUCDV)
```

Apparently, all three estimators produce very similar estimates of parameters in this case and converge to the true values quite fast. A similar behaviour is observed for the Scenario 2, shown in Figure \@ref(fig:parametersPC). The difference between the three estimators does not look substantial.

```{r parametersPC, fig.cap="Estimation of parameters using OLS, CLS and likelihood and their convergence to the true value of $\\underline{\\beta_1}=2-1.5i$ (orange horizontal lines on the plot). Scenario 2.", echo=FALSE, fig.height=3, fig.width=8}
parametersPlots(parametersPC)
```

In both Scenarios 1 and 2 the error term has a small variance, in Scenario 2 $x_r$ and $x_i$ are perfectly correlated, transforming the original model into: $y_r + i y_i = 10+15i + (2-1.5i) (1 + 1.5 i) x_r + (\epsilon_r + i \epsilon_i)$.

The Scenario 3 demonstrates an exotic case, when the variances of $x_r$ and $x_i$ are the same (as shown in Figure \@ref(fig:parametersUCSV)).

```{r parametersUCSV, fig.cap="Estimation of parameters using OLS, CLS and likelihood and their convergence to the true value of $\\underline{\\beta_1}=2-1.5i$ (orange horizontal lines on the plot). Scenario 3.", echo=FALSE, fig.height=3, fig.width=8}
parametersPlots(parametersUCSV)
```

In this scenario, OLS and Likelihood produce efficient, unbiased and consistent estimates of parameters, which cannot be said about the CLS. The behaviour of CLS is explainable because for this specific scenario the direct variance of the complex variable $x_r + i x_i$ becomes close to zero. As a result, the direct covariance in \@ref(eq:SimpleCLRCLSLossParametersMomentsExpanded) is divided by zero, and the estimate of the parameter becomes unstable. Scenarios 1 and 3 could be considered as two special cases of the spectrum of values, showing that the closer the variances of the real and imaginary parts of $x$ are, the less consistent, efficient and unbiased estimates of parameters are produced by CLS. Scenario 2 is complimentary, because it shows that if the real and imaginary parts are correlated, the CLS estimates of parameters become as good (in statistical terms) as the estimates of OLS and/or Likelihood. So, the CLS becomes unreliable in a special case, when $\sigma_{x_r}^2 = \sigma_{x_i}^2$ and $\sigma_{x_r,x_i}=0$.

Next, the three estimators give the same estimates of parameters in Scenario 4 of functional relation between $x$ and $y$, which is shown in Figure \@ref(fig:parametersFR).

```{r parametersFR, fig.cap="Estimation of parameters using OLS, CLS and likelihood and their convergence to the true value of $\\underline{\\beta_1}=2-1.5i$ (orange horizontal lines on the plot). Scenario 4.", echo=FALSE, fig.height=3, fig.width=8}
parametersPlots(parametersFR)
```

The main difficulties for the estimators appear, when the variance of the error term increases. Figure \@ref(fig:parametersCorError) shows how the three perform in case of a higher variance of the error term in Scenario 5.

```{r parametersCorError, fig.cap="Estimation of parameters using OLS, CLS and likelihood and their convergence to the true value of $\\underline{\\beta_1}=2-1.5i$ (orange horizontal lines on the plot). Scenario 5.", echo=FALSE, fig.height=3, fig.width=8}
parametersPlots(parametersCorError)
```

It becomes apparent that OLS and Likelihood produce more efficient estimates of parameters than CLS on small samples. This is because the variability of estimates of parameter is higher for CLS than for the other two on small samples.

The situations worsens for the three estimators when we consider Scenario 6, when the variances of the error term become much higher than before, which is shown in Figure \@ref(fig:parametersHVError).

```{r parametersHVError, fig.cap="Estimation of parameters using OLS, CLS and likelihood and their convergence to the true value of $\\underline{\\beta_1}=2-1.5i$ (orange horizontal lines on the plot). Scenario 6.", echo=FALSE, fig.height=3, fig.width=8}
parametersPlots(parametersHVError)
```

In addition to being less efficient, the real parts of estimates of parameters exhibit a bias, which is diminished with the increase of sample size. Comparing the three estimators, it appears that CLS produces less efficient and more biased estimates of parameters than OLS or Likelihood in this Scenario.

These six scenarios show a variety of situations in which the three estimators produce different estimates of parameters. Arguably, OLS and Likelihood could be considered as robust alternatives, but all the three estimators seem to perform well in several sensible situations.
