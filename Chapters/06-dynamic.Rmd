# Complex Dynamic Models {#Dynamic}
In this chapter we consider the application of the complex linear model to time series data. There are different ways how to do that, and we only discuss a few of the straight forward options of how to construct dynamic models. We start with a model with a trend, which has its own features, then discuss complex Autoregression (cAR), then move to the complex Moving Average (cMA) and complex Autoregression with Moving Average (cARMA) and then finish the discussion with the model applied to differences of the data, cARIMA. cARIMA and its sub-models have a lot in common with the vector ARIMA models applied to two time series. The main difference between them is in the matrix of the parameters: in case of the vector models, it is estimated in full (four parameters for two time series), while in case of the complex ARIMA, it contains two times fewer parameters to estimate. Many of the properties of the vector models can be transferred to cARIMA, and many of them have been explained in detail by @Lutkepohl. Still, there are some of them that are specific for the complex models.

In this chapter, we will use index $t$ instead of $j$ to denote that we are applying the model to time series, not to the cross-sectional data.

## Complex model with trend {#DynamicTrend}
One of the simplest ways to introduce dynamics in a complex model is to include a trend. In the real-valued domain this means adding an explanatory variable with the index of the observation:

\begin{equation}
    {y}_t = {\beta}_0 + {\beta}_1 {x}_{1,t} + {\beta}_2 {x}_{2,t} + \dots + {\beta}_{k-1} {x}_{k-1,t} + \gamma t + {\epsilon}_t,
    (\#eq:trendInRealModel)
\end{equation}
where $\gamma$ is the parameter for the trend and the rest of the equation is just a conventional real-valued regression. The trend in this situation is deterministic and linear, meaning that it starts from the beginning of the sample and increases or decreases (depending on the value of the parameter $\gamma$) indefinitely. Adding such trend to the model allows capturing the deterministic trend and neglecting its effect on the values of other parameters in the model. In cLR, the addition of the trend in the additive model is similar and has the same meaning - it will increase linearly indefinitely. Mathematically, cLR with trend can be written as:
\begin{equation}
    \underline{y}_t = \underline{\beta}_0 + \underline{\beta}_1 \underline{x}_{1,t} + \dots + \underline{\beta}_{k-1} \underline{x}_{k-1,t} + \underline{\gamma} t + \underline{\epsilon}_t .
    (\#eq:trendInCLR)
\end{equation}
The more interesting case arises when a multiplicative cLR is considered, i.e. the model with logarithmic transform (as discussed in Subsection \@ref(complexVariable) and Section \@ref(assumptionsSpecificationTransformation)):
\begin{equation}
    \ln \underline{y}_t = \underline{\beta}_0 + \underline{\beta}_1 \ln \underline{x}_{1,t} + \dots + \underline{\beta}_{k-1} \ln \underline{x}_{k-1,t} + \underline{\gamma} \ln t + \underline{\epsilon}_t ,
    (\#eq:trendInCLRLogs)
\end{equation}
which is equivalent to:
\begin{equation}
    \underline{y}_t = \exp \underline{\beta}_0 \times \underline{x}_{1,t}^{\underline{\beta}_1} \times \dots \times \underline{x}_{k-1,t}^{\underline{\beta}_{k-1}} \times t^{\underline{\gamma}} \times \exp \underline{\epsilon}_t .
    (\#eq:trendInCLRNonlinear)
\end{equation}
The trend in \@ref(eq:trendInCLRNonlinear) is now non-linear and depending on the specific values of the complex parameter $\underline{\gamma}$ can exhibit exponential, trigonometric or inverse trajectories. Figure \@ref(fig:trendTrajectories) shows several trajectories of the real and the imaginary parts of the response variable with different values of the parameter $\underline{\gamma}$.

```{r trendTrajectories, fig.cap="Different trajectories of trend with different values of the parameter $\\underline{\\gamma}$: 0.5-1.5i, 1.5-0.5i, and 0.5+0.2i.", echo=FALSE}
t <- c(1:100)
par(mfcol=c(2,3), mar=c(2,4,2,1))
t1 <- t^{0.5-1.5i}
plot(Re(t1),main="",type="l",ylab="Re(y)")
title(main=TeX("$\\underline\\gamma=0.5-1.5i$"))
plot(Im(t1),main="",type="l",ylab="Im(y)")
t1 <- t^{1.5-0.5i}
plot(Re(t1),main="",type="l",ylab="Re(y)")
title(main=TeX("$\\underline\\gamma=1.5-0.5i$"))
plot(Im(t1),main="",type="l",ylab="Im(y)")
t1 <- t^{0.5+0.2i}
plot(Re(t1),main="",type="l",ylab="Re(y)")
title(main=TeX("$\\underline\\gamma=0.5+0.2i$"))
plot(Im(t1),main="",type="l",ylab="Im(y)")
```

As we can see, the non-linear transformation gives the cLR flexibility in capturing various types of trends in the data, making it a much more flexible instrument for modelling than the real-valued regressions. Furthermore, this flexibility in the trend can be used in the real-valued models as well, if we substitute all the complex variables and parameters (except for $\underline{\gamma}$) by their real-valued counterparts. The model will then produce a complex response variable, the imaginary part of which can be dropped.

To better understand how the cLR model with the trend works, we can represent it in the exponential form:
\begin{equation}
    t^{\underline{\gamma}} = \exp (\underline{\gamma} \ln t) = t^{\gamma_r} \exp (i \gamma_i \ln t),
    (\#eq:trendExp)
\end{equation}
where $\underline{\gamma}=\gamma_r + i \gamma_i$. The form \@ref(eq:trendExp) shows that the real part of the parameter $\underline{\gamma}$ controls the magnitude of the resulting complex variable, while the imaginary one controls how fast the argument (angle) changes. So, the more stable trajectories (closer to the linear ones) will be obtained with $\gamma_i$ close to zero, while the speed of the change in the trajectory is regulated by $\gamma_r$.

Another way to look at the complex-valued trend is to use the trigonometric form of complex variable:
\begin{equation}
    t^{\underline{\gamma}} = t^{\gamma_r} \left(\cos (\gamma_i \ln t) + i \sin (\gamma_i \ln t) \right) .
    (\#eq:trendTrig)
\end{equation}
This form tells us that fundamentally any trajectory from the complex-valued trend is trigonometric, but $\gamma_i$ regulates the strength of the cosine/sine waves for the respective real and imaginary parts of the complex response variable.

### Example in R
In R, the trend is supported in the `clm()` function from the `complex` package via the formula. Here is an example:
```{r}
# Set random seed for reproducibility
set.seed(41)
# Sample size
obs <- 100
# Create an explanatory variable
x <- complex(real=rnorm(obs,100,10), imaginary=rnorm(obs,50,5))
# Generate parameters
b0 <- 2 + 1.5i
b1 <- 0.8 - 0.4i
b2 <- 0.5 + 0.2i
# Generate error term from the complex normal distribution
e <- rcnorm(obs, 0, sigma2=0.25, varsigma2=0.16+0.09i)
# Generate the data using non-linear model
y <- exp(b0 + b1 * log(x) + b2 * log(c(1:obs)) + e)
# Merge it to the matrix
complexData <- data.frame(y=y,x=x)
```

The code above will generate data with a slowly increasing trend (similar to the one shown in Figure \@ref(fig:trendTrajectories)), which is shown in Figure \@ref(fig:dataWithTrend)

```{r dataWithTrend, fig.cap="Generated time series with a trend.", echo=FALSE}
par(mfcol=c(2,1), mar=c(4,4,1,1))
plot(Re(complexData$y), typ="l", main="",
     xlab="Time", ylab="Real part")
plot(Im(complexData$y), typ="l", main="",
     xlab="Time", ylab="Imaginary part")
```

Applying a complex linear regression to the data with the same formula as in the DGP gives us the following estimates of parameters:

```{r}
# Apply a model with the trend in logarithms
clrModel <- clm(log(y)~log(x)+log(trend), complexData, subset=1:80)
summary(clrModel)
```

The resulting model fit and the forecast for the next 20 observations are shown in Figure \@ref(fig:dataWithTrendForecast):

```{r dataWithTrendForecast, fig.cap="cLR applied to the generated data with a trend."}
# Extract fitted values and transform to the original scale
fitted(clrModel) |> exp() -> yFitted
yForecast <- predict(clrModel, newdata=tail(complexData, 20))

# Setup the canvas
par(mfcol=c(2,1), mar=c(4,4,1,1))
# Plot the real part
plot(Re(complexData$y), type="l",
     xlab="Time", ylab="Real part")
lines(Re(yFitted), col="purple", lwd=2, lty=2)
exp(yForecast$mean) |> Re() |>
    lines(x=c(81:100), col="blue", lwd=2, lty=2)

# Plot the imaginary part
plot(Im(complexData$y), type="l",
     xlab="Time", ylab="Imaginary part")
lines(Im(yFitted), col="purple", lwd=2, lty=2)
exp(yForecast$mean) |> Im() |>
    lines(x=c(81:100), col="blue", lwd=2, lty=2)
```

As we can see, this example shows how a non-linear trend can be estimated and then used for forecasting using a complex linear regression model.


## Complex AR
One of the possible ways of modelling dynamics is by saying that future values of the variable depend on its past ones. @Box1976, who have developed a theory and methodology of so called "AutoRegressive Integrated Moving Average" (or ARIMA), provide an example of a series of CO$_2$ output by a furnace with variable gas rate. In that situation the future amount of carbon dioxide will depend on the amount the furnace produced on the previous observations. This is one of example of an AR process. An interested reader is directed to Chapter 8 of @SvetunkovAdam, where the ARIMA model is discussed in more detail.

Mathematically the cAR model can be represented in a vector form, which will make it look more similar to vector AR rather than to the univariate one. However, the cAR model is restricted with just two time series and thus can be represented using complex variables:
\begin{equation}
    \underline{y}_t = \underline{\beta}_0 + \underline{\phi}_1 \underline{y}_{t-1} + \dots + \underline{\phi}_p \underline{y}_{t-p} + \underline{\epsilon}_t ,
    (\#eq:ComplexAR)
\end{equation}
where $\phi_j$ is the $j$-th parameter, and $p$ is the order of the model, complex autoregression, cAR(p). The same model can be rewritten in the conventional polynomial form using a backshift operator $B$:
\begin{equation}
    \left(1 - \underline{\phi}_1 B - \dots - \underline{\phi}_p B^p \right) \underline{y}_t  = \underline{\beta}_0 + \underline{\epsilon}_t .
    (\#eq:ComplexARPolynomial)
\end{equation}
This form is convenient for what follows, e.g. for the discussion of unit roots calculation. But also, if we add the already discussed static elements to the right-hand side of \@ref(eq:ComplexAR) or \@ref(eq:ComplexARPolynomial), namely $\underline{\beta}_1 \underline{x}_{1,t} + \dots + \underline{\beta}_{k-1} \underline{x}_{k-1,t}$, then the model will transform into cARX(p), combining the properties of the cLR and cAR.

Similarly to the conventional AR, it can be shown how complex ACF/PACF (discussed in Subsection \@ref(assumptionsResidualsAuto)) behave for different orders of cAR. In this monograph, for simplicity, we only provide an example of cAR(1) without an intercept, noting that the logic for the higher orders is similar [@SvetunkovAdam has similar derivations for the real-valued AR(p) in Section 8.3].

The cAR(1) model is defined as:
\begin{equation}
    \underline{y}_t = \underline{\phi}_1 \underline{y}_{t-1} + \underline{\epsilon}_t .
    (\#eq:ComplexAR1Step1)
\end{equation}
The covariance between the most recent and the previous observations can then be calculated as (note here that we provide the formula for the conjugate covariance, but the calculations for the direct one will be similar):
\begin{equation}
    \mathrm{cov} \left(\underline{y}_t, \underline{y}_{t-1} \right) = \mathrm{cov} \left(\underline{\phi}_1 \underline{y}_{t-1} + \underline{\epsilon}_t, \underline{y}_{t-1} \right) ,
    (\#eq:ComplexAR1Step2)
\end{equation}
which given the basic assumptions of models (Section \@ref(assumptionsResiduals)) equals to:
\begin{equation}
    \mathrm{cov} \left(\underline{y}_t, \underline{y}_{t-1} \right) = \underline{\phi}_1 \mathrm{cov} \left( \underline{y}_{t-1}, \underline{y}_{t-1} \right) .
    (\#eq:ComplexAR1Step3)
\end{equation}
Now, depending on the type of covariance we need, we get:

1. For the conjugate one:
\begin{equation}
    \mathrm{cov}\left(\underline{y}_t, \underline{y}_{t-1} \right) = \underline{\phi}_1 \sigma^2_{y_t} ;
    (\#eq:ComplexAR1ConjCov)
\end{equation}
2. For the direct one:
\begin{equation}
    cov \left(\underline{y}_t, \underline{y}_{t-1} \right) = \underline{\phi}_1 \varsigma^2_{y_t} .
    (\#eq:ComplexAR1DirCov)
\end{equation}

If we now insert \@ref(eq:ComplexAR1ConjCov) and \@ref(eq:ComplexAR1DirCov) into the respective formulae for the conjugate and direct correlations (Section \@ref(correlationTypes), equations \@ref(eq:correlationConjugate) and \@ref(eq:correlationDirect)), we will get the values of the cACF for the first lag according to:

1. Conjugate correlation:
\begin{equation}
    \rho(1) = \frac{\sqrt{\underline{\phi}_1 \sigma^2_{y_t} \underline{\phi}_1 \sigma^2_{y_t}}}{\sigma_{y_t}^2} = | \underline{\phi}_1 |;
    (\#eq:ComplexAR1ConjCor)
\end{equation}

2. Direct correlation:
\begin{equation}
    \varrho(1) = \frac{\underline{\phi}_1 \varsigma^2_{y_t}}{\varsigma^2_{y_t}} = \underline{\phi}_1.
    (\#eq:ComplexAR1DirCor)
\end{equation}

As we can see from the formula \@ref(eq:ComplexAR1ConjCor), the specific value of the cAR(1) parameter is lost because of the geometric mean in the formula: we end up with a magnitude of the complex variable instead of it real and imaginary parts. On the other hand, the direct cACF \@ref(eq:ComplexAR1DirCor) keeps the information about the cAR(1) parameter. However, the two formulae above are calculated in terms of expectations and their sample estimates might be slightly different. Specifically, the direct correlation \@ref(eq:ComplexAR1DirCor) relies on the sample estimate of the direct variance of the response variable $y_t$, which might become equal to zero if the variances of the real and the imaginary parts of the complex variable are similar (as discussed in Section \@ref(correlationDirect)). This means that in some situations the values of the direct cACF might become greater than one, leading to potential losses in information. But this also means that both conjugate and direct cACFs should be used for the analysis of complex time series: each of them has issues, but both of them give much clearer information about the potential process.

If we calculate the covariance for the cAR(1) between the values $y_{t}$ and $y_{t-2}$, we will get the following (the reader is encouraged to do the calculations manually to check the correctness of the final result):

1. Conjugate correlation:
\begin{equation}
    \rho(2) = | \underline{\phi}_1^2 |;
    (\#eq:ComplexAR1ConjCor)
\end{equation}

2. Direct correlation:
\begin{equation}
    \varrho(2) =  \underline{\phi}_1^2.
    (\#eq:ComplexAR1DirCor)
\end{equation}

In principles, it can be shown that the main properties of real-valued AR discussed by @Box1976 hold widely for the cAR processes: cACF will decline exponentially starting from the lag p. Note however that the direct cACF involves the power of a complex number, which means that in some situations, for some values of the complex parameter $\underline{\phi}_1$, the cACF will decline harmonically rather than exponentially.

The more useful (at least potentially) property for cAR(p) is the behaviour of the cPACF. This function shows the relations between specific lags without the effect of all the interim lags, i.e. cleaned from the autocorrelations between the neighbouring lags. One of ways of calculating the cPACF for a lag $\tau$ is to estimate the following model:
\begin{equation}
    \underline{y}_t = \underline{\phi}_1 \underline{y}_{t-1} + \dots + \underline{\phi}_\tau \underline{y}_{t-\tau} + \underline{\epsilon}_t
    (\#eq:cPACFModel)
\end{equation}
and getting the coefficient $\underline{\phi}_\tau$. Repeating this procedure for all lags of interest, we get a cPACF. The model \@ref(eq:cPACFModel) can be estimated using OLS or CLS, which will result in respective conjugate and direct cPACF.

It can be shown that for the cAR(p) process, both conjugate and direct cPACF will drop to zero abruptly after the lag p and that both of them will produce complex numbers, reflecting the respective parts of the coefficients of the cAR(p) model. However, the direct cPACF might have issues similar to the ones the direct cACF has if the variances of the real and the imaginary parts of the response variable are close to each other.

As we see, the complex AR(p) has similar properties to the real-valued one, but it should be analysed using both conjugate and direct cACF/cPACF.

Finally, in Box-Jenkins methodology, it is important for the AR(p) processes to be stationary, otherwise they become explosive and cannot be efficiently identified at the model building stage. The stationarity condition for AR(p) in @Box1976 is that the roots of the polynomial formed from the parameters of the AR model should all lie outside the unit circle. For AR(1) this simplifies to $|\phi_1|<1$. The same conditions hold for the cAR(p) processes, meaning that the magnitudes of the complex parameters are considered. For the cAR(1), the stationarity condition is $|\underline{\phi}_1|<1$. But more widely, for cAR(p), it is that the roots of the following polynomial:
\begin{equation}
    1 - \underline{\phi}_1 x - \underline{\phi}_2 x^2 - \dots - \underline{\phi}_p x^p = 0
    (\#eq:ComplexARPolyRoots)
\end{equation}
should all lie outside the unit circle (be greater than one by absolute value).


### Example in R
For demonstration purposes we consider the cAR(1) process to see how cACF/cPACF and the data will look
```{r}
# Sample size
set.seed(41)
# Number of observations
obs <- 110
# Parameters
b0 <- 100+50i
phi1 <- 0.5 + 0.2i
# Complex white noise
e <- rcnorm(obs, 0, sigma2=25, varsigma2=16+9i)
y <- vector("complex", obs)
# Initial value
y[1] <- b0 + e[1]
# The cAR(1)
for(i in 2:obs){
    y[i] <- phi1 * y[i-1] + b0 + e[i]
}
# Drop the first 10 observations as a burn-in period
y <- y[-c(1:10)]
```

The conjugate cACF/cPACF are shown in Figures \@ref(fig:complexAR1cACF) and \@ref(fig:complexAR1cPACF):

```{r complexAR1cACF, fig.cap="Conjugate cACF of the complex AR(1)."}
cacf(y, method="conjugate", main="")
```

As we see from Figure \@ref(fig:complexAR1cACF), the cACF declines after the lag 1. Notably, the cACF for lag one equals to `r round(cacf(y, method="conjugate", plot=FALSE)$acf[1],3)`, which is close to the true value of the magnitude of the parameter $\phi_1=0.5+0.2i$ is `r abs(phi1)`.

```{r complexAR1cPACF, fig.cap="Conjugate cPACF of the complex AR(1)."}
cpacf(y, method="conjugate", main="")
```

The cPACF in Figure \@ref(fig:complexAR1cPACF) demonstrates that both the real and the imaginary values drop to zero after the first observation, as expected for the AR(1) process. Notably, the value of the first cPACF is `r round(cpacf(y, method="conjugate", plot=FALSE)$acf[1],3)`, which is close to the true value of the parameter.

The direct cACF and cPACF are shown in Figures \@ref(fig:complexAR1cACFDir) and \@ref(fig:complexAR1cpACFDir), roughly giving the same information as their conjugate counterparts. We should note however that the direct cACF has the more detailed information about the dynamic relations in the data than the conjugate one.
```{r complexAR1cACFDir, fig.cap="Direct cACF of the complex AR(1).", echo=FALSE}
cacf(y, method="direct", main="")
```

Although, both plots contain values outside of the 95% non-rejection region, these could be considered as happening at random and can be neglected.

```{r complexAR1cpACFDir, fig.cap="Direct cPACF of the complex AR(1).", echo=FALSE}
cpacf(y, method="direct", main="")
```

It is worth noting that in both Figures \@ref(fig:complexAR1cACFDir) and \@ref(fig:complexAR1cpACFDir) the first values of the cACF/cPACF are close to the true value of $0.5+0.2i$.

Finally, the plot in Figure \@ref(fig:complexAR1Plot) shows the generated data on the complex plane and the dynamics of both the real and the imaginary parts.

```{r complexAR1Plot, fig.cap="Visualisation of the cAR(1) process.", echo=FALSE}
layout(matrix(c(1,2,1,3),2,2))
par(mar=c(4,4,1,1))
plot(y, type="b", main="")
plot(Re(y), type="l", xlab="Time", main="")
plot(Im(y), type="l", xlab="Time", main="")
```

When it comes to applying a model and to forecasting, the same `clm()` function from the `complex` package in R can be used:

```{r}
# Fit the model with intercept and AR(1)
cAR1Model <- clm(y~1, orders=c(1,0,0), subset=c(1:80))
# Generate the summary
summary(cAR1Model)
```

The output above shows the estimates of parameters of the model, demonstrating that the OLS produced estimates close to the true values of parameters (as expected). After that we can generate forecast for the next 20 observations:

```{r}
# Dummy data. 20 rows tells the function what the horizon is
complexData20 <- matrix(rep(1,20),20,1)
# Produce forecasts for the next 20 steps
yForecast <- predict(cAR1Model, newdata=complexData20)
```

The only catch in the code above is to create a dummy `newdata` with the number of rows equal to the forecast horizon. This is a fix, needed because the `clm()` function focuses on explanatory variables. The forecasts from the function are shown in Figure \@ref(fig:complexAR1Forecast).

```{r complexAR1Forecast, fig.cap="Forecasts for the cAR(1) process."}
# Produce the plot of the data and the forecasts
par(mfcol=c(2,1),mar=c(4,4,1,1))
plot(yForecast)
```

As we can see, the forecast trajectory from the cAR(1) model corresponds to the dampening line, which is a behaviour similar to the conventional real-valued AR(1) model [e.g. discussed in Subsection 8.1.1 of @SvetunkovAdam].


## Complex MA
Another type of the dynamic model discussed by @Box1976, is called "Moving Average". In case of complex variables, it can be formulated as:
\begin{equation}
    \underline{y}_t = \underline{\beta}_0 + \underline{\theta}_1 \underline{\epsilon}_{t-1} + \dots + \underline{\theta}_q \underline{\epsilon}_{t-q} + \underline{\epsilon}_t ,
    (\#eq:ComplexMA)
\end{equation}
where $\theta_j$ is the $j$-th parameter, and $q$ is the order of the model complex MA(q) model. Similar to the cAR, it can be written via polynomials:
\begin{equation}
    \underline{y}_t  = \underline{\beta}_0 + \left(1 + \underline{\theta}_1 B + \dots + \underline{\theta}_q B^q \right) \underline{\epsilon}_t .
    (\#eq:ComplexMAPolynomial)
\end{equation}
The idea of this model is that the new actual observation is formed as a linear combination of the past white noise. @Box1976 continue the example with a CO$_2$ output, showing that some random disturbances in the system in the past can impact the volume of the carbon dioxide at the current moment. 

Using similar logic as in case of cAR, we will show how cACF and cPACF behave for cMA(1) without intercept, and then we will generalise their behaviour for cMA(q).

The cMA(1) model can be defined as:
\begin{equation}
    \underline{y}_t = \underline{\theta}_1 \underline{\epsilon}_{t-1} + \underline{\epsilon}_t .
    (\#eq:ComplexMA1)
\end{equation}
For the cACF, we need to calculate the covariance between the values on observations $t$ and $t-1$. Again, the formulae for the conjugate and the direct covariances will be similar at this stage:
\begin{equation}
    \mathrm{cov} \left(\underline{y}_t, \underline{y}_{t-1} \right) = \mathrm{cov}\left( \underline{\theta}_1 \underline{\epsilon}_{t-1} + \underline{\epsilon}_t, \underline{\theta}_1 \underline{\epsilon}_{t-2} + \underline{\epsilon}_{t-1} \right).
    (\#eq:ComplexMA1Step1)
\end{equation}
Given that the basic assumptions discussed in Chapter \@ref(assumptions) hold for the "true model", the covariance is simplified to:
\begin{equation}
    \mathrm{cov} \left(\underline{y}_t, \underline{y}_{t-1} \right) = \underline{\theta}_1 \mathrm{cov}\left( \underline{\epsilon}_{t-1}, \underline{\epsilon}_{t-1} \right).
    (\#eq:ComplexMA1Step2)
\end{equation}

Based on this, the conjugate and the direct covariances will be equal to respectively:
\begin{equation}
    \mathrm{cov} \left(\underline{y}_t, \underline{y}_{t-1} \right) = \underline{\theta}_1 \sigma^2_{y_t}.
    (\#eq:ComplexMA1ConjCov)
\end{equation}
and
\begin{equation}
    {cov} \left(\underline{y}_t, \underline{y}_{t-1} \right) = \underline{\theta}_1 \varsigma^2_{y_t}.
    (\#eq:ComplexMA1DirCov)
\end{equation}
Note that in the formulae above, we use the fact that the variance of the error term will coincide with the variance of the response variable $\underline{y}_t$, which becomes apparent from the original model formulation \@ref(eq:ComplexMA1), assuming that the error term has a zero expectation. Inserting \@ref(eq:ComplexMA1ConjCov) and \@ref(eq:ComplexMA1DirCov) in the formulae for the conugate and direct correlations, we get that for the cMA(1), the first lag of the cACF equals to:

1. Conjugate correlation:
\begin{equation}
    \rho(1) = | \underline{\theta}_1 |;
    (\#eq:ComplexMA1ConjCor)
\end{equation}

2. Direct correlation:
\begin{equation}
    \varrho(1) =  \underline{\theta}_1.
    (\#eq:ComplexMA1DirCor)
\end{equation}

Similarly to how it was with the cAR(1) process, we see that the conjugate cACF is based on the magnitude of the complex parameter, while the direct one has the value itself in it. This comes with the same in-sample limitations as above with the direct correlation being sensitive to the similarity of the variances of the real and the imaginary parts of the response variable.

However, when it comes to the second lag of the cACF, the situation differs from the one with the cAR(1):
\begin{equation}
    \mathrm{cov} \left(\underline{y}_t, \underline{y}_{t-2} \right) = \mathrm{cov}\left( \underline{\theta}_1 \underline{\epsilon}_{t-1} + \underline{\epsilon}_t, \underline{\theta}_1 \underline{\epsilon}_{t-3} + \underline{\epsilon}_{t-2} \right) = 0 .
    (\#eq:ComplexMA1Lag2)
\end{equation}
This agrees with the behaviour of the cACF in case of the conventional real-valued MA(q) processes [@Box1976]: it falls to zero abruptly right after the lag q.

When it comes to the cPACF of cMA(1), its behaviour becomes similar to the behaviour of cACF for AR(1). This is because the model \@ref(eq:ComplexMA1) can be represented as an infinite cAR:
\begin{equation}
    \underline{y}_t = \sum_{j=1}^\infty -1^{j-1} \underline{\theta}_1^j \underline{y}_{t-j} + \underline{\epsilon}_t ,
    (\#eq:ComplexMA1Infinite)
\end{equation}
which is based on the fact that $\underline{\epsilon}_t = \underline{y}_t - \underline{\theta}_1 \underline{\epsilon}_{t-1}$. In this situation, both conjugate and direct cPACF will decline either exponentially or harmonically, depending on the specific value of the complex parameter. In general, this ones again agrees with the behaviour of the conventional PACF in case of the real-valued MA(q) process.

As we see, the behaviour of both cACF and cPACF in case of cMA(q) model agrees with their behaviour in the real-valued model. Still, similarly to the cAR(p) process, both conjugate and direct cPACF should be used for time series analysis.

Continuing the similarities between the complex and the real-valued models, it can be shown that the property of the invertibility is easily transferable to the cMA(q) model from the conventional MA(q). The model will be invertible if all the roots of the following polynomial equation lie outside of the unit circle:
\begin{equation}
    1 + \underline{\theta}_1 x + \underline{\theta}_2 x^2 + \dots + \underline{\theta}_p x^p = 0 .
    (\#eq:ComplexMAPolyRoots)
\end{equation}

When it comes to estimating the model on a sample of data, we face several restrictions: the cMA part needs to be constructed recursively, because the new error from observation $t$ is used on the next observations. This means that the analytical solutions according to OLS or CLS cannot be applied anymore, and we need to revert to the numeric optimisation. To that extent, CLS can no longer be used, because the standard solvers require the real-valued loss function, while the one from CLS is a complex one. So, when estimating cMA(q) model, we have to use either OLS or likelihood.


### Example in R
Similarly to the cAR(p), we consider an R example, analysing how the cACF and cPACF behave in a special case of cMA(1) with an intercept:
```{r}
# Sample size
set.seed(41)
# Number of observations
obs <- 110
# Parameters
b0 <- 100 + 50i
theta1 <- -0.8 + 0.2i
# Complex white noise
e <- rcnorm(obs, 0, sigma2=25, varsigma2=16+9i)
y <- vector("complex", obs)
# Initial value
y[1] <- b0 + e[1]
# The cAR(1)
for(i in 2:obs){
    y[i] <- theta1 * e[i-1] + b0 + e[i]
}
# Drop the first 10 observations as a burn-in period
y <- y[-c(1:10)]
```

Because we use the same random seed value as before and the same value of the parameter as in case of cAR(1), we get time series, which look similar to the one generated from cAR(1) (see Figure \@ref(fig:complexMA1Plot), compared to Figure \@ref(fig:complexAR1Plot)).

```{r complexMA1Plot, fig.cap="Visualisation of the cMA(1) process.", echo=FALSE}
layout(matrix(c(1,2,1,3),2,2))
par(mar=c(4,4,1,1))
plot(y, type="b", main="")
plot(Re(y), type="l", xlab="Time", main="")
plot(Im(y), type="l", xlab="Time", main="")
```

However, when we analyse cACF of the time series, we will spot an important difference - its values drop to zero abruptly right after the first lag.

```{r complexMA1cACF, fig.cap="Conjugate cACF of the complex MA(1)."}
cacf(y, method="conjugate", main="")
```

As we see from Figure \@ref(fig:complexMA1cACF), the only statistically significant lag (on 5%) is the first one, all the others lie in the non-rejection region. We observe a similar behaviour in case of the direct cACF (Figure \@ref(fig:complexMA1cACFDir)), although it is worth noting that the imaginary part of the direct cACF has all lags lying in the non-rejection region.

```{r complexMA1cACFDir, fig.cap="Direct cACF of the complex MA(1).", echo=FALSE}
cacf(y, method="direct", main="")
```

On the other hand, the conjugate and direct cPACF will decline exponentially, as discussed earlier. Figure \@ref(fig:complexMA1cpACFDir) shows how the Direct cPACF looks for the complex MA(1), while Figure \@ref(fig:complexMA1cPACF) shows the conjugate one. While the specific values are different for the two functions, they give roughly the same message: the cPACFs decline over time.

```{r complexMA1cpACFDir, fig.cap="Direct cPACF of the complex MA(1).", echo=FALSE}
cpacf(y, method="direct", main="")
```


```{r complexMA1cPACF, fig.cap="Conjugate cPACF of the complex MA(1)."}
cpacf(y, method="conjugate", main="")
```

There are also some values outside of the 95% non-rejection region on the direct cPACF. However, they can be considered as happening at random (we expect them to lie outside in 5% cases anyway) and can be neglected.

Finally, we can apply the model to the data:
```{r}
# Fit the model with intercept and MA(1)
cMA1Model <- clm(y~1, orders=c(0,0,1),
                 subset=c(1:80), loss="likelihood")
# Generate the summary
summary(cMA1Model)
```

In the code above, we specify the `loss="likelihood"` because the parameters of cMA(q) cannot be estimated analytically, and likelihood, having good statistical properties (efficiency and consistency), is appropriate. Note that the estimates of parameters for the complex moving average are far from the true ones, which is due to the sample size and the specific random sample we selected. Asymptotically, the estimator should give parameters closer to the "true" ones.

Similarly to how we did it with cAR(1), we can produce forecasts from the model using the dummy data:

```{r}
# Dummy data. 20 rows tells the function what the horizon is
complexData20 <- matrix(1,20,1)
# Produce forecasts for the next 20 steps
yForecast <- predict(cMA1Model, newdata=complexData20)
```

These forecasts are shown in Figure \@ref(fig:complexMA1Forecast).

```{r complexMA1Forecast, fig.cap="Forecasts for the cMA(1) process."}
# Produce the plot of the data and the forecasts
par(mfcol=c(2,1),mar=c(4,4,1,1))
plot(yForecast)
```

As expected, the forecasts from the cMA(1) converge to the intercept value abruptly right after the $h=1$.


## Complex ARMA and ARIMA {#DynamicARIMA}
Uniting the cAR(p) with cMA(q) gives us the CARMA(p,q) model, which can be written as (in the polynomial form):
\begin{equation}
    \left(1 - \underline{\phi}_1 B - \dots - \underline{\phi}_p B^p \right) \underline{y}_t  = \underline{\beta}_0 + \left(1 + \underline{\theta}_1 B + \dots + \underline{\theta}_q B^q \right) \underline{\epsilon}_t .
    (\#eq:ComplexARMAPolynomial)
\end{equation}
This model combines the properties of the both models, making it even more flexible. However, the cACF/cPACF become even more complicated than before for some cARMA models, because they show complex interactions between the cAR and cMA. We do not discuss specific examples here and leave them as a home task for an interested reader.

Furthermore, in order for cARMA model to be identifiable, we need to apply it to the stationary data - all the examples of cAR and cMA above assumed that implicitly. There are several ways of achieving this, including the addition of trend component as in Section \@ref(DynamicTrend) and taking differences of the original data. The latter implies the introduction of the difference polynomial giving us cARIMA(p,d,q), which is analogous to the real-valued ARIMA:
\begin{equation}
    \left(1 - \underline{\phi}_1 B - \dots - \underline{\phi}_p B^p \right) \left(1 - B \right)^d \underline{y}_t  = \underline{\beta}_0 + \left(1 + \underline{\theta}_1 B + \dots + \underline{\theta}_q B^q \right) \underline{\epsilon}_t .
    (\#eq:ComplexARIMAPolynomial)
\end{equation}
There are several ways how to construct such a model. First one is to take the differences of the original data and then apply cARMA. But this implies that we estimate the following model:
\begin{equation}
    \Delta_{\underline{y}_t} = \left(1 - B \right)^d \underline{y}_t  = \underline{\beta}_0 + \left(\underline{\phi}_1 B + \dots + \underline{\phi}_p B^p \right) \Delta_{\underline{y}_t} + \left(\underline{\theta}_1 B + \dots + \underline{\theta}_q B^q \right) \underline{\epsilon}_t + \underline{\epsilon}_t .
    (\#eq:ComplexARIMADiffs)
\end{equation}
But working with this model might be challenging because in order to produce forecasts from it, one needs to take the inverse of differences to get back to the original units. Instead, we argue that the following form (in principle) is easier to implement and use:
\begin{equation}
    \underline{y}_t  = \underline{\beta}_0^\prime + \frac{\left(1 + \underline{\theta}_1 B + \dots + \underline{\theta}_q B^q \right)}{\left(1 - \underline{\phi}_1 B - \dots - \underline{\phi}_p B^p \right) \left(1 - B \right)^d} \underline{\epsilon}_t ,
    (\#eq:ComplexARIMADiffsCorrect)
\end{equation}
where we now can produce future $\hat{\underline{y}}_t$ directly instead of $\hat{\Delta}_{\underline{y}_t}$. This formulation is implemented and used in the `clm()` function from the `complex` package in R via the `orders` parameter.

When it comes to deciding whether to take the differences of the time series or not, the standard ADF [@Dickey1979] and KPSS [@Kwiatkowski1992] tests can be applied independently to the real and imaginary parts to see whether they are stationary or not. This approach assumes that both parts contain variables that have similar dynamics in principle. But there might be some situations, when one part is stationary, while the other is not. In that case, the decision about the order of differencing d might not be straight forward. A solution in this case is to take differences, although this might lead to the overdifferencing one of the parts of the complex variable. The alternative approach is to apply the test to the two parts jointly and a relatively simple solution in this case is to use the MDS to scale the complex variable into the real-valued one. After that the conventional ADF/KPSS can be applied to determine whether the differences are needed to make the variable stationary "overall".

When it comes to the selection of orders of cARIMA(p,d,q), using cACF/cPACF for this purpose becomes extremely difficult, because of all the possibilities for the behaviour of these functions depending on the order of the model and/or specific values of parameters. For example, in some situations cACF will decline harmonically for an AR(p) process, while cPACF will do the same for an MA(q) one. The combination of cAR and cMA might produce really puzzling cACF/cPACF, which might become useless for diagnostics or order selection. Furthermore, as discuss in Section 8.3 of @SvetunkovAdam, the fact that some specific orders of cARMA(p,q) generate some specific shapes of cACF/cPACF does not mean that if we observe that shape then the data is generated from that specific model. After all, there can be many reasons why we observe the specific cACF/cPACF, and conclusions about the order of the appropriate cARMA model just based on them become unreliable. As a result, we recommend using other principles for the selection of the appropriate p and q. One of those is to do that based on the information criteria, such as AIC [@Akaike1974] or BIC [@Schwarz1978]. But in order to be able to do that, one needs to use likelihood for the model estimation.


<!-- ## Other possible models -->
<!-- There are many possibilities how complex dynamic model can be expanded. In this section we briefly discuss some of them pointing out their advantages and limitations. -->

<!-- ### cARIMA in logarithms -->
<!-- The first and relatively simple one is applying cARIMA to the complex variables after the logarithmic transformation. As discussed in Section \@ref(assumptionsSpecificationTransformation), this leads to modelling complex non-linear dynamic relations. This might become especially useful on long time series, for example, in energy domain. -->

<!-- log cARIMA -->

<!-- Other stuff from the original monograph -->

