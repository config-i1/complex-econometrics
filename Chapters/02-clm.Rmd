# Complex Linear Regression {#CLR}

## Correlations between complex random variables
When it comes to measuring associations between variables, most frequently analysts use coefficient of correlation. While it is straightforward for real-valued variables, for complex variables this become challenging, because each c.r.v. has two parts, so the correlation needs to take them both into account.

For modelling purposes, it might be useful to have the information about all possible correlations involved relation of two c.r.v. This comes to analysing the following covariances:

1. $\sigma_{x_r,x_i}$,
2. $\sigma_{y_r,y_i}$,
3. $\sigma_{x_r,y_r}$,
4. $\sigma_{x_i,y_i}$,
5. $\sigma_{x_r,y_i}$,
6. $\sigma_{y_r,x_i}$.

In order to better understand what correlations between c.r.v. imply, we need to understand how to produce scatterplots for them. While in case of two real variables it is straightforward (a variable per axes), in our situation, this becomes challenging. We propose considering a set of scatterplots shown in Figure \@ref(fig:crvScatterplots) for two generated complex random variables $x$ and $y$, created using `cplot()` function from `complex` package in R.

```{r crvScatterplots, fig.width=8, fig.height=8, fig.cap="Visualisation of relations between two complex variables"}
# Create real part of a c.r.v. x
xr <- rnorm(1000,0,10)
# Create a c.r.v. x
x <- complex(real=xr, imaginary=1.5*xr+rnorm(1000,0,10))
# Create a c.r.v. y
y <- (10 + 15i) + (1.5 + 1.2i) * x +
    complex(real=rnorm(1000,0,10), imaginary=rnorm(1000,0,10))
# Produce the plot
cplot(x, y)
```

This scatterplot has several important elements in it:

- It shows relations between real and imaginary parts of each variable (e.g. the two scatterplots in the bold dark red frame),
- It shows cross-relations between parts of one variable and parts of the other one (e.g. the rest four plots),
- The colour shows ordering of the original variable $x$ with dark values corresponding to point with higher magnitude and light ones being closer to zero. This way, we can see what the original points in $x$ correspond to in $y$.

The plots are positioned to satisfy two rules:

1. When a scatterplot for a c.r.v. is produced, the real part should be in x-axis, while the imaginary should be in the y-axis.
2. When parts of variables $x$ and $y$ are compared, the part for $x$ should be in x-axis, while the part for $y$ should be in y-axis, which should the reflect the idea that $x$ could be an explanatory variable for $y$.

While a simple scatterplot matrix could have been constructed instead of Figure \@ref(fig:crvScatterplots), we argue that the latter has a logical grouping and should be preferred for analysis of complex variables. For example, based on the plots in Figure \@ref(fig:crvScatterplots) we can conclude that:

- There is a positive relation between the real and imaginary parts of $x$,
- There is a negative relation between the real and imaginary parts of $y$,
- Real parts of $x$ and $y$ do not exhibit a strong linear relation,
- But the respective imaginary parts of $x$ and $y$ have the positive relation between them,
- Finally, we see that with the increase of real and imaginary parts of $x$, the real part of $y$ decreases, while the imaginary one increases. This sort of behaviour implies positive complex slope in the potential linear regression (discussed in Section \@ref(simpleCLR)).

We think that this visualisation is useful when analysing relations between two complex random variables. But it also shows how complicated it is to capture the relation between them and how many aspects need to be considered.

The literature knows two correlation coefficients for complex variables [@ref]: the conjugate and the direct correlation (the latter is known in the literature as "pseudo-correlation"). Their formula are based on the respective covariances and variances (conjugate and direct discussed in Subsection \@ref(crvSecondMoment)):

1. Conjugate correlation
\begin{equation}
    \rho_{x,y} = \frac{\sigma_{x,y}}{\sigma_x \sigma_y},
    (\#eq:correlationConventional)
\end{equation}

2. Direct correlation
\begin{equation}
    \varrho_{x,y} = \frac{\varsigma_{x,y}}{\varsigma_x \varsigma_y}.
    (\#eq:correlationPseudo)
\end{equation}

They both can be derives based on the same principles used for the derivation of the original Pearson's correlation coefficient [@refPearson]. In that case, the correlation coefficient equals to the geometric mean of slopes of two regression lines:
\begin{equation}
    \begin{aligned}
        &y = \beta_0 + \beta_1 x + \epsilon \\
        &x = \alpha_0 + \alpha_1 y + \upsilon ,
    \end{aligned}
    (\#eq:twoRegressions)
\end{equation}
where $\alpha_0$ and $\beta_0$ are the intercepts, $\alpha_1$ and $\beta_1$ are the slopes of the regression lines and $e$ and $u$ are the residuals of the models. As we show in Section \@ref(simpleCLR), the parameters of slope can be estimated using Ordinary Least Squares (OLS) or the Complex Least Squares (CLS) to get respectively $a_1$ and $b_1$. For the OLS, the formulae for the slopes are:
\begin{equation}
    \begin{aligned}
        &b_1 = \frac{\hat{\sigma}_{x,y}}{\hat{\sigma}_x} \\
        &a_1 = \frac{\hat{\sigma}_{x,y}}{\hat{\sigma}_y} .
    \end{aligned}
    (\#eq:twoRegressionsOLS)
\end{equation}
Taking their geometric means leads to:
\begin{equation}
    \hat{\rho}_{x,y} = \sqrt{a_1 b_1},
    (\#eq:correlationConventionalEstimate)
\end{equation}
which then leads to the formula \@ref(eq:correlationConventional). Similarly, for CLS estimated regressions, we have:
\begin{equation}
    \begin{aligned}
        &b_1 = \frac{\hat{\varsigma}_{x,y}}{\hat{\varsigma}_x} \\
        &a_1 = \frac{\hat{\varsigma}_{x,y}}{\hat{\varsigma}_y} ,
    \end{aligned}
    (\#eq:twoRegressionsCLS)
\end{equation}
which after taking the same geometric means leads to \@ref(eq:correlationPseudo). Note that the estimates of the slope parameters will differ between the OLS and the CLS. This is discussed further in Section \@ref(simpleCLR).

When it comes to the interpretation of the coefficients, both of them are complex numbers, but they show different things. The conjugate correlation $\rho_{x,y}$ can be written as:
\begin{equation}
    {\rho}_{x,y} = \frac{\sigma_{x_r, y_r} + \sigma_{x_i, y_i} + i (\sigma_{x_i, y_r} - \sigma_{x_r, y_i})}{\sqrt{(\sigma_{x_r}^2 + \sigma_{x_i}^2)(\sigma_{y_r}^2 + \sigma_{y_i}^2)}} .
    (\#eq:correlationConventionalExpanded)
\end{equation}
As can be seen from the formula \@ref(eq:correlationConventionalExpanded), the coefficient becomes a real number only when the cross-covariances between the real and imaginary parts of $x$ and $y$ are equal. In practice, this exotic condition can be met, when the parts for both variables are not correlated. In all the other cases, the resulting correlation coefficient will be a complex number. In general, the real part of the complex correlation $\rho_{x,y}$ shows the average strength of linear relation between respective real and imaginary parts of two complex variables, while the imaginary part of $\rho_{x,y}$ shows the difference between linear relations between the cross values of real and imaginary parts of $x$ and $y$.

The coefficient will be equal to zero, when there are no linear relations between the parts of complex variables $x$ and $y$. On the other hand, its real part will be close to one by absolute value if the complex relation between variables $y$ and $x$ is close to linear, i.e. $a_1 = \frac{1}{b_1}$.

In order to better understand what the real and imaginary parts of the conjugate correlation mean, we expand the formula \@ref(eq:correlationConventionalEstimate) by substituting $b_1 = b_{1,r} + i b_{1,i}$ and $a_1 = a_{1,r} + i a_{1,i}$:
\begin{equation}
    \begin{aligned}
        \hat{\rho}_{x,y} = & \sqrt{(a_{1,r} + i a_{1,i}) (b_{1,r}+ib_{1,i})} = \\
        & \sqrt{a_{1,r} b_{1,r} - a_{1,i} b_{1,i} + i(a_{1,r} b_{1,i} + a_{1,i} b_{1,r})},
    \end{aligned}
    (\#eq:correlationConjugateExpanded01)
\end{equation}
or in the exponential form:
\begin{equation}
    \hat{\rho}_{x,y} = R^{\frac{1}{2}} e^{i \frac{1}{2} \phi} ,
    (\#eq:correlationConjugateExpanded02)
\end{equation}
where
\begin{equation}
    \begin{aligned}
        & R = \sqrt{(a_{1,r} b_{1,r} - a_{1,i} b_{1,i})^2 + (a_{1,r} b_{1,i} + a_{1,i} b_{1,r})^2} \\
        & \phi=\arctan\left(\frac{a_{1,r} b_{1,i} + a_{1,i} b_{1,r}}{a_{1,r} b_{1,r} - a_{1,i} b_{1,i}}\right),
    \end{aligned}
    (\#eq:correlationConjugateExpanded03)
\end{equation}
As can be seen from \@ref(eq:correlationConjugateExpanded03), there is a multitude of combinations of parameters of the model that can give the unity magnitude $R$. For example, if $a_{1,r} = 0.5$, $b_{1,r} = 1$, $a_{1,i} = -0.5$ and $b_{1,i} = 1$, $R$ would be equal to one. Similarly, there is a multitude of values of parameters that would give angles of $0$, $\frac{\pi}{2}$, $\pi$ and $\frac{3\pi}{2}$ leading to pure real or imaginary numbers. In fact, for the example above, $\phi=0$, implying that the correlation coefficient will be equal to one, no matter how strong the relation between variable $x$ and $y$ is. This means that it is difficult to make any solid conclusions based on the conjugate correlation coefficient, but it can still be used to get some preliminary understanding of the relation.

So, the conjugate correlation coefficient has some similarities with the conventional one for the real numbers, but similarly to the conjugate variance, it might obscure the real situation by mixing different things in one number.

The direct correlation can be expanded to:
\begin{equation}
    {\varrho}_{x,y} = \frac{\sigma_{x_r, y_r} - \sigma_{x_i, y_i} + i (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})}{\sqrt{(\sigma_{x_r}^2 - \sigma_{x_i}^2 + i2 \sigma_{x_r,x_i})(\sigma_{y_r}^2 - \sigma_{y_i}^2 + i2 \sigma_{y_r,y_i})}}.
    (\#eq:correlationPseudoExpanded)
\end{equation}
Given that it contains a complex number in the denominator, it is more complicated than the conjugate one.

Father's stuff here.

To get other insights about the direct correlation coefficient, we express its denominator in the exponential form:
\begin{equation}
    {\sqrt{(\sigma_{x_r}^2 - \sigma_{x_i}^2 + i2 \sigma_{x_r,x_i})(\sigma_{y_r}^2 - \sigma_{y_i}^2 + i2 \sigma_{y_r,y_i})}} = R_1^{\frac{1}{2}} e^{i \frac{\phi_1}{2}},
    (\#eq:correlationPseudoExpandedExp01)
\end{equation}
where $R_1 = \sqrt{\left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) - 4 \sigma_{x_r,x_i} \sigma_{y_r,y_i} \right)^2+ 4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2}$ and $\phi_1=\arctan \left(\frac{2 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)}{(\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) - 4 \sigma_{x_r,x_i} \sigma_{y_r,y_i}}\right)$, which are obtained by opening the brackets inside the square root. If we now insert \@ref(eq:correlationPseudoExpandedExp01) in \@ref(eq:correlationPseudoExpanded) and multiply both numerator and denominator by conjugate number to the \@ref(eq:correlationPseudoExpandedExp01) we will get:
\begin{equation}
    {\varrho}_{x,y} = \frac{\left(\sigma_{x_r, y_r} - \sigma_{x_i, y_i} + i (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})\right)e^{-i \frac{\phi_1}{2}}}{R_1^{\frac{1}{2}}}.
    (\#eq:correlationPseudoExpandedExp02)
\end{equation}
Representing the $\sigma_{x_r, y_r} - \sigma_{x_i, y_i} + i (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})$ as $R_2 e^{i \phi_2}$, where $R_2= \sqrt{(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 +  (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2}$ and $\phi_2= \arctan \left( \frac{\sigma_{x_i, y_r} + \sigma_{x_r, y_i}}{\sigma_{x_r, y_r} - \sigma_{x_i, y_i}} \right)$, and inserting these values in \@ref(eq:correlationPseudoExpandedExp02) we get:
\begin{equation}
    {\varrho}_{x,y} = \frac{R_2}{\sqrt{R_1}} e^{i \left(\phi_2 - \frac{\phi_1}{2} \right)}.
    (\#eq:correlationPseudoExpandedExp03)
\end{equation}
or in an even shorter exponential form ${\varrho}_{x,y} = |{\varrho}_{x,y}| e ^{i \arg({\varrho}_{x,y})}$, where:
\begin{equation}
    |{\varrho}_{x,y}|=\sqrt{\frac{(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 +  (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2}{\sqrt{\left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) - 4 \sigma_{x_r,x_i} \sigma_{y_r,y_i} \right)^2+ 4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2}}}
    (\#eq:correlationPseudoExpandedExpR)
\end{equation}
and 
\begin{equation}
    \arg({\varrho}_{x,y}) = \arctan \left( \frac{\sigma_{x_i, y_r} + \sigma_{x_r, y_i}}{\sigma_{x_r, y_r} - \sigma_{x_i, y_i}} \right) - \frac{1}{2}\arctan \left(\frac{2 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)}{(\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) - 4 \sigma_{x_r,x_i} \sigma_{y_r,y_i}}\right) .
    (\#eq:correlationPseudoExpandedExpPhi)
\end{equation}
Analysing \@ref(eq:correlationPseudoExpandedExpR) \@ref(eq:correlationPseudoExpandedExpPhi), we can identify several conditions that lead to specific values of the direct correlation coefficient:

1. The magnitude of the coefficient will be equal to zero (and thus the coefficient will be equal to zero) only when $\sigma_{x_i,y_r}=\sigma_{x_r,y_i}=0$ and $\sigma_{x_r,y_r}=\sigma_{x_i,y_i}$. One of the special cases of this is when all cross-covariances between the real and imaginary parts of $x$ and $y$ are equal to zero.

2. The magnitude of the coefficient will be equal to one, when its numerator and denominator are equal, which can be obtained with a multitude of values of covariances and variances of real and imaginary parts of complex variables $x$ and $y$.

3. One of the special cases for $\varrho_{x,y}=1$ comes to the definition of the conventional correlation coefficient [@refPearson]. $\sqrt{a_1 b_1}=1$



<!-- Trying to derive |rho|=1 -->
<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 +  (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 = \sqrt{\left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) - 4 \sigma_{x_r,x_i} \sigma_{y_r,y_i} \right)^2+ 4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2}$ -->

<!-- $\left((\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 +  (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 \right)^2 = \left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) - 4 \sigma_{x_r,x_i} \sigma_{y_r,y_i} \right)^2+ 4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2$ -->

<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^4 + 2 (\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 + (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^4 = \left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) - 4 \sigma_{x_r,x_i} \sigma_{y_r,y_i} \right)^2+ 4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2$ -->

<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^4 + 2 (\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 + (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^4 = \left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) \right)^2 - 8 (\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \sigma_{y_r,y_i} + \left(4 \sigma_{x_r,x_i} \sigma_{y_r,y_i}\right)^2 +    4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} + (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2$ -->

<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^4 + 2 (\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 + (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^4 = \left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) \right)^2 - 8 (\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \sigma_{y_r,y_i} + \left(4 \sigma_{x_r,x_i} \sigma_{y_r,y_i}\right)^2 + 4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} \right)^2  + 8(\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} (\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} + 4\left((\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2$ -->

<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^4 + 2 (\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 + (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^4 = \left((\sigma_{x_r}^2 - \sigma_{x_i}^2)(\sigma_{y_r}^2 - \sigma_{y_i}^2) \right)^2 + \left(4 \sigma_{x_r,x_i} \sigma_{y_r,y_i}\right)^2 + 4 \left( (\sigma_{x_r}^2 - \sigma_{x_i}^2) \sigma_{y_r,y_i} \right)^2 + 4\left((\sigma_{y_r}^2 - \sigma_{y_i}^2) \sigma_{x_r,x_i} \right)^2$ -->


<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^4 + 2 (\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 + (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^4 = (\sigma_{x_r}^2 - \sigma_{x_i}^2)^2\left( (\sigma_{y_r}^2 - \sigma_{y_i}^2)^2 + 4  \sigma_{y_r,y_i}^2 \right) + 4 \sigma_{x_r,x_i}^2 \left((\sigma_{y_r}^2 - \sigma_{y_i}^2)^2 + \sigma_{y_r,y_i}^2 \right)$ -->

<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^4 + 2 (\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 + (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^4 = (\sigma_{x_r}^2 - \sigma_{x_i}^2)^2\left( (\sigma_{y_r}^2 - \sigma_{y_i}^2)^2 + 4  \sigma_{y_r,y_i}^2 \right) + 4 \sigma_{x_r,x_i}^2 \left((\sigma_{y_r}^2 - \sigma_{y_i}^2)^2 + 4 \sigma_{y_r,y_i}^2 \right) - 12 \sigma_{x_r,x_i}^2 \sigma_{y_r,y_i}^2$ -->

<!-- $(\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^4 + 2 (\sigma_{x_r, y_r} - \sigma_{x_i, y_i})^2 (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^2 + (\sigma_{x_i, y_r} + \sigma_{x_r, y_i})^4 = \left(\left(\sigma_{x_r}^2 - \sigma_{x_i}^2\right)^2 + 4 \sigma_{x_r,x_i}^2 \right) \left( (\sigma_{y_r}^2 - \sigma_{y_i}^2)^2 + 4  \sigma_{y_r,y_i}^2 \right) - 12 \sigma_{x_r,x_i}^2 \sigma_{y_r,y_i}^2$ -->






## Simple CLR {#simpleCLR}
### Model formulation
The simple Complex Linear Regression can be written as:
\begin{equation}
    \mathbf{z} = \mathbf{a} + \mathbf{b} \mathbf{x} + \boldsymbol{\epsilon}
    (\#eq:SimpleCLRComplex)
\end{equation}
or
\begin{equation}
    y_r + i y_i = a_0 + i a_1 + (b_0 + i b_1) (x_r + i x_i) + \epsilon_r + i \epsilon_i
    (\#eq:SimpleCLR)
\end{equation}
Given that any complex equation can be represented as a system of two equations, this model can be represented as a system of two linear regressions:
\begin{equation}
    \begin{aligned}
        y_r = & a_0 + b_0 x_r - b_1 x_i + \epsilon_r \\
        y_i = & a_1 + b_0 x_i + b_1 x_r + \epsilon_i
    \end{aligned}
    (\#eq:SimpleCLRSystem)
\end{equation}
This model captures a very specific dynamics between the real and imaginary parts, given that they share the same set of parameters for the slope.

### Estimation
Number of estimated parameters is k, number of degrees of freedom per series is k/2.


### Inference


### Forecasting


## Multiple CLR
### Model formulation 

### Estimation

### Inference

### Diagnostics

### Forecasting

### Examples of application (Production functions)
