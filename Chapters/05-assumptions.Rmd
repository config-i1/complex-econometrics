#  Assumptions of Complex Linear Models {#assumptions}
Similar to how it is done for the conventional real valued regression, we should discuss what assumptions are imposed on complex linear regression estimated using one of the methods discussed in the Section \@ref(mlcrEstimation). We will group all regression assumptions to three parts [similar to how it was done by @SvetunkovAdam]:

1. Model is correctly specified;
2. Residuals are independent and identically distributed (i.i.d.);
3. Explanatory variables are not correlated with anything but the response variable.

While the first two groups directly relate to the so called "true model", the last one refers to the estimation approaches discussed in the previous Chapter: if the latter are violated then the estimation procedure will lead to issues in estimates of parameters. We should also point out that many of the assumptions discussed in this Chapter are very similar to the assumptions in the conventional regression, which is why we do not plan to cover them in this monograph in detail, but rather to focus on their implications to complex-valued models. An reader interested in assumptions for the real-valued models is advised to read Chapter 15 of @SvetunkovSBA.

**Disclaimer**: while in conventional econometrics, statistical tests are often used to check hypotheses of violation of specific assumptions, in this monograph we focus on the visual diagnostics. While tests might give ambiguous results and rely on a selected significance level, the visual inspection comes to expert judgment of an analyst. Yes, in some situations the subjectivity impacts the diagnostics, but at least it is not hidden behind p-values and a variety of outputs that might give contradictory results.


## Model is correctly specified {#assumptionsSpecification}
This is one of the most general and most important groups of assumptions. It includes the following:

1. Model does not omit any important variables;
2. Model does not have redundant variables;
3. Variables are included in the model with appropriate transformations;
4. Residuals of the model do not contain outliers.

We briefly discuss all of them in this section.


### Model does not omit any important variables {#assumptionsSpecificationOmit}
This assumption implies that we have all the variables that can substantially impact our response variable, and that we have included them in the model. If we do not do that then the estimates of parameters are known to be biased. In practice we always omit a lot of different variables that might potentially impact the response one, but do not have a large effect on it. Not including these variables is known not to cause serious issues in the estimation [@econometrics].

Formally speaking, the omitted variables should not be correlated with the ones that we include in the model, because otherwise the impact of the latter will not be captured correctly, causing bias in the estimates of parameters. It is not possible to test this assumption, so it can only be checked based on judgment of an analyst.

When it comes to complex-valued models, the relations become much more complicated and potentially non-linear than in the conventional models. The problem becomes then manifolds: we need to include the correct variables in the model, but also they need to be included correctly, because now any variable can be included in the real or in the imaginary part. Furthermore, the correlation in complex-valued models can be measured differently and implies a variety of effects between two variables.

Practically speaking, we consider two situations with omitted variables:

1. The model is estimated using OLS;
2. The model is estimated using CLS.

For the both cases, we assume that the true model is of the form:
\begin{equation}
    \underline{\mathbf{y}} = \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \underline{\boldsymbol{\epsilon}},
    (\#eq:AssumptionsOmitTrueModel)
\end{equation}
where the matrix $\underline{\mathbf{Z}}$ contains the variables omitted in the applied model and the vector $\underline{\boldsymbol{\gamma}}$ is the vector of the parameters for these variables in the true model.

We start with the effect of the omitted variables on the estimates of parameters using OLS. We refer to the derivations \@ref(eq:MCLROLSExpansion), which given \@ref(eq:AssumptionsOmitTrueModel) can be substituted to:
\begin{equation}
    \begin{aligned}
    \underline{\boldsymbol{b}}^{\text{OLS}} =
        & \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{y}} = \\
        & \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \left(\underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \underline{\boldsymbol{\epsilon}} \right) = \\
        & \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\boldsymbol{\epsilon}} = \\
        & \underline{\boldsymbol{\beta}} + \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\boldsymbol{\epsilon}} .
    \end{aligned}
    (\#eq:AssumptionsOmitOLS01)
\end{equation}

Taking the expectation of \@ref(eq:AssumptionsOmitOLS01), we get:
\begin{equation}
    \mathrm{E}\left(\underline{\boldsymbol{b}}^{\text{OLS}}\right) = \underline{\boldsymbol{\beta}} + \mathrm{E}\left(\left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} \right) + 0 ,
    (\#eq:AssumptionsOmitOLS02)
\end{equation}
with zero appearing because of the one of fundamental assumptions that the explanatory variables are not correlated with the error term in the true model. The expectation above shows that the estimates of the OLS parameters will be biased in case of omitted variables, and the bias will be proportional to the size of the $\underline{\mathbf{X}}^\prime \underline{\mathbf{Z}}$ matrix, which consists of the covariances between the included and the omitted variables. The higher the correlation between these variables, the higher the bias will be. This agrees with the similar findings in the conventional real-valued econometrics.

When it comes to CLS, the logic is similar to the above. Based on the equations \@ref(eq:MCLRCLSExpansion) and \@ref(eq:AssumptionsOmitTrueModel) we have:
\begin{equation}
    \begin{aligned}
    \underline{\boldsymbol{b}}^{\text{CLS}} =
    & \left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\top \underline{\mathbf{y}} = \\
    & \left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\top \left(\underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \underline{\boldsymbol{\epsilon}}\right) = \\
    & \underline{\boldsymbol{\beta}} + \left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\top \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\top \underline{\boldsymbol{\epsilon}}
    \end{aligned} ,
    (\#eq:AssumptionsOmitCLS01)
\end{equation}
which simplifies (using similar logic as above) to:
\begin{equation}
    \mathrm{E}\left(\underline{\boldsymbol{b}}^{\text{CLS}}\right) = \underline{\boldsymbol{\beta}} + \mathrm{E}\left( \left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\top \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}}\right) ,
    (\#eq:AssumptionsOmitCLS02)
\end{equation}
Which also shows that the bias will increase with the increase of the values of the matrix $\underline{\mathbf{X}}^\top \underline{\mathbf{Z}}$. The main difference between the real-valued and the complex-valued econometrics is that the aforementioned matrices are complex and the respective covariances in OLS and CLS are respectively conjugate and direct. This means that:

1. if an omitted complex variable has high *direct correlation* with the included one but has the zero *conjugate correlation*, the OLS should give unbiased estimates of parameters, while the CLS would give the biased ones;
2. if an omitted complex variable has zero *direct correlation* with the included one but has high *conjugate correlation*, the CLS should give unbiased estimates of parameters, while the OLS would give the biased ones.

This property can be used in deciding which of the estimation methods to use if the researcher has some insights about the relation between the included and the omitted complex variables.

Furthermore, we can also see what happens with the direct and conjugate covariance matrices of parameters in case of omitted variables. In the case of the omitted variables the residuals become $\underline{\boldsymbol{\upsilon}} = \underline{\mathbf{Z}} \underline{\boldsymbol{\gamma}} + \underline{\boldsymbol{\epsilon}}$, implying that we estimate the following model instead of \@ref(eq:AssumptionsOmitTrueModel):
\begin{equation}
    \underline{\mathbf{y}} = \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\boldsymbol{\upsilon}} ,
    (\#eq:AssumptionsOmitAppliedTrueModel)
\end{equation}
which means that we then make inference based on:
\begin{equation}
    \underline{\mathbf{y}} = \underline{\mathbf{X}} \underline{\boldsymbol{b}} + \hat{\underline{\boldsymbol{\upsilon}}} .
    (\#eq:AssumptionsOmitAppliedModel)
\end{equation}

The covariance matrix of the estimates of the parameter $\underline{\boldsymbol{\beta}}$ in this case comes to formulae discussed in Section \@ref(MCLRInference):
\begin{equation}
    \begin{aligned}
        & \mathrm{V}\left( \underline{\boldsymbol{b}}^{\text{OLS}} \right) = \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\prime \tilde{\underline{\mathbf{X}}} \left( {\underline{\mathbf{X}}}^\top \tilde{\underline{\mathbf{X}}} \right)^{-1 \prime}  \sigma_{\underline{\upsilon}}^2 \\
        & \mathcal{V}\left( \underline{\boldsymbol{b}}^{\text{OLS}} \right) = \left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \right)^{-1} \varsigma_{\underline{\upsilon}}^2 \\
        & \mathrm{V}\left( \underline{\boldsymbol{b}}^{\text{CLS}} \right) = \left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}} \right)^{-1} \underline{\mathbf{X}}^\top \tilde{\underline{\mathbf{X}}} \left( {\underline{\mathbf{X}}}^\prime \tilde{\underline{\mathbf{X}}} \right)^{-1 \top}  \sigma_{\underline{\upsilon}}^2 \\
        & \mathcal{V}\left( \underline{\boldsymbol{b}}^{\text{CLS}} \right) = \left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}} \right)^{-1} \varsigma_{\underline{\upsilon}}^2 ,
    \end{aligned}
    (\#eq:MCLROLSCLSVariancesOmitted)
\end{equation}
where $\sigma_{\underline{\upsilon}}^2$ is the conjugate and $\varsigma_{\underline{\upsilon}}^2$ is the direct variances of the residuals $\underline{\boldsymbol{\upsilon}}$. Now to better understand the effect of omitted variables on the covariance matrix of parameters, we should consider several situations:

1. Some parts of $\underline{\mathbf{Z}}$ are correlated with $\underline{\mathbf{X}}$, in which case applying the model \@ref(eq:AssumptionsOmitAppliedModel) to the data will lead to the biased estimates of parameters, but would not have a large impact on the variance of the residuals of the model. The higher the correlation is, the lower the variances become in \@ref(eq:MCLROLSCLSVariancesOmitted);
2. $\underline{\mathbf{Z}}$ is uncorrelated with $\underline{\mathbf{X}}$, which means that the effect of the omitted variable is not captured by the model and as a result the variance of the residual $\hat{\underline{\boldsymbol{\upsilon}}}$ will be inflated, increasing the standard errors of the estimates of parameters.

The case (1) corresponds to the classical problem of the omitted variables in econometrics, while the case (2) shows how the uncertainty about the estimates of parameters increases with omitted variables and might lead to the wider confidence intervals for the parameters.



### Model does not have redundant variables {#assumptionsSpecificationRedundant}
This is the situation opposite to the first one. It implies that we have included something that should not be there. Mathematically it can be represented by two sets of equations, for the true and the applied models:
\begin{equation}
    \begin{aligned}
        & \underline{\mathbf{y}} = \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\boldsymbol{\epsilon}} \\
        & \underline{\mathbf{y}} = \underline{\mathbf{X}} \underline{\boldsymbol{b}} + \underline{\mathbf{X}}_{red} \underline{\boldsymbol{b}}_{red} + \underline{\boldsymbol{e}} ,
    \end{aligned}
    (\#eq:AssumptionsRedundantSetting)
\end{equation}
where $\underline{\boldsymbol{b}}_{red}$ is the vector of the estimates of parameters for the redundant variables $\underline{\mathbf{X}}_{red}$ on a sample. It will not be zero in sample, because the values of $\underline{\mathbf{X}}_{red}$ can explain some randomness in $\underline{\mathbf{y}}$, thus reducing the size of variance of the residuals $\underline{\boldsymbol{e}}$ in comparison with the estimation of the correct model. This leads to the effect known as "overfitting" in forecasting.

The second equation in \@ref(eq:AssumptionsRedundantSetting) can also be written in a compact form:
\begin{equation}
    \underline{\mathbf{y}} = \underline{\mathbf{Z}} \underline{\boldsymbol{c}}  + \underline{\boldsymbol{e}} ,
    (\#eq:AssumptionsRedundantAppliedModel)
\end{equation}
where $\underline{\mathbf{Z}} = \begin{pmatrix} \underline{\mathbf{X}} & \underline{\mathbf{X}}_{red} \end{pmatrix}$ is the matrix that contains all variables under consideration, and $\underline{\boldsymbol{c}} = \begin{pmatrix} \underline{\boldsymbol{b}} \\ \underline{\boldsymbol{b}}_{red} \end{pmatrix}$. While it is hard to show explicitly using formulae of OLS and CLS, it is well known in statistics that the estimates of parameters in such model are unbiased, because the true $\underline{\boldsymbol{b}}_{red}$ is zero. At the same time, the inclusion of redundant variables increases the standard errors of parameters, because the covariance matrices from Section \@ref(MCLRInference) would rely on different combinations of $\underline{\mathbf{Z}}$ and its transposition (direct and conjugate), which means that the final variances would be impacted by the variability of redundant variables $\underline{\mathbf{X}}_{red}$. The exact effect is difficult to capture and might depend on the direct/conjugate correlations between variables. We leave this task for future research.


<!-- \begin{equation} -->
<!--     \begin{aligned} -->
<!--     \underline{\boldsymbol{c}}^{\text{OLS}} = -->
<!--         & \left( \underline{\mathbf{Z}}^\prime \underline{\mathbf{Z}} \right)^{-1} \underline{\mathbf{Z}}^\prime \underline{\mathbf{y}} = \\ -->
<!--         & \left( \underline{\mathbf{Z}}^\prime \underline{\mathbf{Z}} \right)^{-1} \underline{\mathbf{Z}}^\prime \left(\underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\boldsymbol{\epsilon}} \right) = \\ -->
<!--         & \left( \underline{\mathbf{Z}}^\prime \underline{\mathbf{Z}} \right)^{-1} \left( \underline{\mathbf{Z}}^\prime \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \underline{\mathbf{Z}}^\prime \underline{\boldsymbol{\epsilon}} \right) . -->
<!--     \end{aligned} -->
<!--     (\#eq:AssumptionsRedundantOLS01) -->
<!-- \end{equation} -->
<!-- Substituting $\underline{\mathbf{Z}} = \begin{pmatrix} \underline{\mathbf{X}} & \underline{\mathbf{X}}_{red} \end{pmatrix}$ and $\underline{\mathbf{Z}}^\prime = \begin{pmatrix} \underline{\mathbf{X}}^\prime \\ \underline{\mathbf{X}}_{red}^\prime \end{pmatrix}$ in \@ref(eq:AssumptionsRedundantOLS01), we get: -->
<!-- \begin{equation} -->
<!--     \begin{aligned} -->
<!--     \underline{\boldsymbol{c}}^{\text{OLS}} = -->
<!--         & \left( \begin{pmatrix} \underline{\mathbf{X}}^\prime \\ \underline{\mathbf{X}}_{red}^\prime \end{pmatrix} \begin{pmatrix} \underline{\mathbf{X}} & \underline{\mathbf{X}}_{red} \end{pmatrix} \right)^{-1} \left( \begin{pmatrix} \underline{\mathbf{X}}^\prime \\ \underline{\mathbf{X}}_{red}^\prime \end{pmatrix} \underline{\mathbf{X}} \underline{\boldsymbol{\beta}} + \begin{pmatrix} \underline{\mathbf{X}}^\prime \\ \underline{\mathbf{X}}_{red}^\prime \end{pmatrix} \underline{\boldsymbol{\epsilon}} \right) = \\ -->
<!--         & \begin{pmatrix} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} & \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}_{red} \\ \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}} & \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}}_{red} \end{pmatrix}^{-1} \left( \begin{pmatrix} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} \\ \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}} \end{pmatrix} \underline{\boldsymbol{\beta}} + \begin{pmatrix} \underline{\mathbf{X}}^\prime \\ \underline{\mathbf{X}}_{red}^\prime \end{pmatrix} \underline{\boldsymbol{\epsilon}} \right) -->
<!--     \end{aligned} -->
<!--     (\#eq:AssumptionsRedundantOLS02) -->
<!-- \end{equation} -->

<!-- The inverse block matrix in \@ref(eq:AssumptionsRedundantOLS02) is cumbersome, but each element of the resulting square block matrix can be written as: -->
<!-- \begin{equation} -->
<!--     \begin{pmatrix} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}} & \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}_{red} \\ \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}} & \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}}_{red} \end{pmatrix}^{-1} = \begin{pmatrix} \mathbf{A} & \mathbf{B} \\ \mathbf{C} & \mathbf{D} \end{pmatrix} . -->
<!--     (\#eq:AssumptionsRedundantOLS03) -->
<!-- \end{equation} -->
<!-- The first element of that matrix is: -->
<!-- \begin{equation} -->
<!--     \begin{aligned} -->
<!--         A = & \begin{pmatrix} \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} + \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}_{red} \left(\underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}}_{red} - \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}} \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}_{red} \right)^{-1}  \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}} \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} \end{pmatrix} = \\ -->
<!--         & \begin{pmatrix} \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} + \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}_{red} \left(\underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}}_{red} - \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}} \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}_{red} \right)^{-1}  \underline{\mathbf{X}}_{red}^\prime \underline{\mathbf{X}} \left(\underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1} \end{pmatrix} -->
<!--     \end{aligned} -->
<!--     (\#eq:AssumptionsRedundantOLS04) -->
<!-- \end{equation} -->


### Variables transformations {#assumptionsSpecificationTransformation}
While the first two assumptions are universal for any statistical model, the third one has some special implications in case of CLR. This is because of the effect they have on the variables: transformations of separate parts of a complex variable are not equivalent to the transformations of the whole variable. For example, the logarithm of a complex variable $\underline{z}$ as shown in \@ref(eq:complexNumberLogarithm) is:
\begin{equation*}
    \ln \underline{z} = \ln r + i \phi ,
\end{equation*}
which is not equivalent to the complex variable $\ln x_r + i \ln x_i$. In fact, there is no known transformation from $x_r + x_i$ to $\ln x_r + i \ln x_i$ except for transforming separately the real and imaginary parts of the variable. Still, logarithms can be used to linearise a non-linear complex model to simplify its estimation. For example, the following multiplicative model:
\begin{equation*}
    \begin{aligned}
    y_{r} + i y_{i} = & (\beta_{0,r} + i \beta_{0,i}) \times (x_{1,r} + i x_{1,i}) ^{\beta_{1,r} + i \beta_{1,i}} \times \dots \times \\
                      & (x_{k-1,r} + i x_{k-1,i}) ^{\beta_{k-1,r} + i \beta_{k-1,i}} \times (\epsilon_{r} + i \epsilon_{i})
    \end{aligned}
\end{equation*}
can be linearised using natural logarithm to:
\begin{equation*}
    \begin{aligned}
    \ln (y_{r} + i y_{i}) = & \ln (\beta_{0,r} + i \beta_{0,i}) + (\beta_{1,r} + i \beta_{1,i}) \ln(x_{1,r} + i x_{1,i}) + \dots + \\
                            & (\beta_{k-1,r} + i \beta_{k-1,i}) \ln (x_{k-1,r} + i x_{k-1,i}) + \ln (\epsilon_{r} + i \epsilon_{i}),
    \end{aligned}
\end{equation*}
which can then be estimated using OLS, CLS or likelihood maximisation as discussed in Section \@ref(mlcrEstimation). On its own, the multiplicative models of complex variables have useful features, because they allow modelling highly non-linear processes due to the rotation as discussed in Section \@ref(theoryOfComplexNumbers). For example, here how the linear increase of real and imaginary parts of a complex variable $\underline{x}$ leads to a non-linear transform of a variable $\underline{y}=\underline{x}^{0.5+0.5i}$:

```{r fig.width=4, fig.height=4, fig.cap="Non-linear transformation of a complex variable that changes linearly."}
x <- 1:100 + 1:100*1i
y <- x^(0.5+0.5i)
cplot(x, y)
```

Even if the imaginary part of $\underline{x}$ is zero (and as a result we deal with a real number, not a complex one), the complex power leads to highly non-linear transformation of the variable. This is a useful feature if the non-linearity is suspected in the data.

The diagnostics of correct transformations in CLR is challenging, but can be done by analysing the residuals. Consider the following example in R (using functions from the `complex` package in R):

```{r}
# Set random seed for reproducibility
set.seed(41)
# Sample size
obs <- 1000
# Create the explanatory variable
x <- complex(real=rnorm(obs,100,10), imaginary=rnorm(obs,50,5))
# Generate parameters
b0 <- 1 - 1.5i
b1 <- 2.5 + 1.5i
# Generate error term from the complex normal distribution
e <- complex(real=rnorm(obs, 0, 0.1), imaginary=rnorm(obs, 0, 0.2))
# e <- rcnorm(obs, 0, sigma2=0.25, varsigma2=0.16+0.09i)
# Generate the data using non-linear model
y <- exp(b0 + b1 * log(x) + e)
# Merge it to the matrix
complexData <- cbind(y=y,x=x)
```

For demonstration purposes, we will first use a complex linear regression model on the data that was generated using a non-linear one:
```{r}
# Apply a linear model
complexModel <- clm(y~x, complexData)
```

The issues of the model can be diagnosed using various plots. For example, here how the standardised residuals vs fitted would look for the model above (see Figure \@ref(fig:nonlinearLinStdResid)):
```{r nonlinearLinStdResid, fig.cap="Standardised residuals vs Fitted for the CLR on non-linear data."}
par(mfcol=c(1,2), mar=c(2,2,3,1))
plot(complexModel, which=2, main="")
```

The plots above demonstrate that there is a slight non-linear pattern in the residuals (especially for the real part of the model) and that their variances might not be constant. These are the indicators of a possible non-linearity in the data. In order to capture it correctly, we would need to transform both response and the explanatory complex variables and estimate the model in logarithms:

```{r}
# Transform the data
complexDataLogs <- log(complexData)
# Apply a model to log data
complexModelLogs <- clm(y~x, complexDataLogs)
```

After which the same plot will look more reasonable, with residuals not exhibiting the u-shape anymore (see Figure \@ref(fig:nonlinearStdResid)):

```{r nonlinearStdResid, fig.cap="Standardised residuals vs Fitted for the log-log CLR on non-linear data."}
par(mfcol=c(1,2), mar=c(2,2,3,1))
plot(complexModelLogs, which=2, main="")
```


### No outliers {#assumptionsSpecificationOutlier}
Finally, the presence of outliers in the residuals typically implies that either there is an error in recording the data, or the model omits an important variable (e.g. a dummy variable for an external event). In that case the effect on estimates of parameters will be similar to the one discussed in Subsection \@ref(assumptionsSpecificationOmit): the estimates of parameters will be in general biased with some specific effects depending on the direct/conjugate correlation and the estimation method used.

The simplest way to detect outliers is to produce a diagnostic plot of fitted vs standardised residuals (similar to the one in Figure \@ref(fig:nonlinearStdResid)) and analyse those values that lie outside of the constructed confidence interval. If the value of an outlier cannot be explained (e.g. this is not a calendar event, this is not a promotion etc) then it can be removed or interpolated. In the latter case, creating a dummy variable (see Section \@ref(multipleCLRDummy)) that equals to one on that specific observation and to zero on all the others is one of the simplest way of dealing with the outlier.


## Residuals are i.i.d. {#assumptionsResiduals}
The next group of assumptions has five elements in it:

1. Residuals are homoscedastic;
2. No autocorrelation in residuals;
3. Expectation of residuals is zero (no matter what);
4. Residuals follow an assumed distribution;
5. Residuals follow a circular distribution.

Some of these assumptions are universal for statistical models, while the others (e.g. the last one) are specific to CLR.

### Homoscedastic residuals {#assumptionsResidualsHomoscedastic}
The first assumption implies that both direct and conjugate variances of the residuals are constant and do not change with any changes of variables. This follows directly from the formulae for the direct and conjugate covariances from Section \@ref(MCLRInference): if either of the covariances is not constant, and we use the formulae for calculating the standard errors of parameters, we will obtain averaged-out values, that would be lower than needed for some observations and higher than needed for the others.

In case of the formulation of CLR as a vector model and the likelihood estimation, the assumption implies that the covariance matrix of residuals $\Sigma_\epsilon$ stays the same no matter what. This assumption aligns well with a similar assumption of conventional multivariate models, such as VAR [@Lutkepohl] or VETS [@SvetunkovVETS].

The homoscedasticity assumption can be typically diagnosed visually by producing the plot of absolute residuals vs fitted. In case of the CLR, this can be modified to producing plots for each part of the complex residuals. Figure \@ref(fig:heteroDiagnostics) shows how the residuals of the linear model applied to the non-linear data (example from Subsection \@ref(assumptionsSpecificationTransformation)) look:

```{r heteroDiagnostics, fig.cap="Absolute residuals vs fitted"}
par(mfcol=c(1,2), mar=c(2,2,3,1))
plot(complexModel, which=4, main="")
```

The plots in Figure \@ref(fig:heteroDiagnostics) show that the variability of residuals (and the local mean) increases with the increase of fitted values. This is a signal of the heteroscedasticity in the data. Some non-linear transformations (e.g. taking logarithm) typically resolve the issue.


### No autocorrelation in residuals {#assumptionsResidualsAuto}
This assumption only applies to time series data. In case of the real-valued models, it means that the residuals in the past should not impact the ones in the future. Typically, this effect appears because of the wrong specification of the model (e.g. the appropriate transformations are not done, or some autoregressive elements are missing). In case of the CLR, the idea is very similar, but now we are talking about complex relations between the residuals, which could arise, again, because of the wrong transformations or because of missing elements (e.g. complex autoregression, discussed later in this book).

To diagnose this, we can analyse the autocorrelation functions. In the real-valued domain, Autocorrelation Function (ACF) and the Partial Autocorrelation Function (PACF) are typically used. In the complex-valued domain, these transform to the direct and conjugate complex ACF/PACF (or cACF/cPACF, based on the correlations discussed in Chapter \@ref(correlationAnalysis)), giving an analyst four instruments instead of two. Furthermore, using the idea with correlation on MDS (see Section \@ref(correlationMDSPearson)), we can calculate ACF/PACF based on Pearson's correlation between the complex variables reduced to real ones.

In general, the idea with direct and conjugate ACF/PACF for complex variables is similar to the one for the conventional ACF/PACF for the real ones. The complex ACF measures the correlations between the variable on observation $t$ and the one with lag $\tau=\{1, \dots, T\}$, where $T$ is the maximum lag we calculate the correlation for. If we calculate the conjugate correlations, we obtain a vector of values $\rho_{1}, \dots, \rho_{T}$ (specific formulae are discussed in Section \@ref(correlationConjugate)). In case of the direct one (see Section \@ref(correlationDirect)), we get a vector $\varrho_{1}, \dots, \varrho_{T}$. In both cases the index $j$ in subscription refers to the specific lag, so, for example, $\rho_{1}$ will measure the conjugate autocorrelation for lag 1, i.e. conjugate correlation between $\underline{y}_{t}$ and $\underline{y}_{t-1}$.

When it comes to PACF, the logic is to estimating recursively regression models of the $\underline{y}_{t}$ from all the previous $\underline{y}_{t-j}$ for $j=\{1, \dots, J\}$ for each $J$ from 1 to $T$ and collecting respective estimates of parameters in front of $\underline{y}_{t-J}$. This way, the coefficients are clear of the interim effects and we get a clear correlation between $\underline{y}_{t}$ and $\underline{y}_{t-J}$. In case of the direct complex PACF, we use the CLS, for the conjugate one we use OLS and for the Pearson's, we do MDS and then calculate the conventional PACF (see Chapter \@ref(correlationAnalysis) for discussion of different correlation coefficients).

In R, there are functions `cacf()` and `cpacf()` in the `complex` package that implement respective complex ACF/PACF with a parameter `method` specifying, which of the correlations to calculate. To demonstrate how they work, we generate a data from a complex autoregressive model of order one and apply it to the data (note that for diagnostic purposes, the functions below should be applied to the residuals of the model):

```{r}
# Generate complex data with an autocorrelation of order 1
# For reproducibility, fix random seed
set.seed(41)
# Number of observations
obs <- 100
# Vector of the final variable
y <- vector("complex", obs)
# Initial values for the first observation
y[1] <- 300 + 250i
for(i in 2:obs){
    y[i] <- 100 + 200i + (0.5 - 0.25i) * y[i-1] +
        complex(real=rnorm(1,0,10), imaginary=rnorm(1,0,5))
}
```

The conjugate ACF (due to its nature) is represented by a set of real values (Figure \@ref(fig:complexAR1ACFConj)):

```{r complexAR1ACFConj, fig.cap="Complex conjugate autocorrelation function"}
cacf(y, method="conjugate", plot=TRUE, main="")
```

The plot in Figure \@ref(fig:complexAR1ACFConj) shows that the autocorrelation for lag one is statistically different from zero on the 5% significance level (the latter is regulated by `level` parameter in the function). The non-rejection region on the plot is costructed based on the t-statistics and helps testing the null hypothesis that autocorrelation coefficient for each lag equals to zero (with the alternative being not equal). The value of the coefficient itself does not provide any specific information about the autoregressive relation in the data, but merely shows that the correlation between variables is mediocre.

In contrast, the conjugate PACF provides more details and shows the specific values for the real and the imaginary parts of the coefficient in the process (see Figure \@ref(fig:complexAR1PACFConj)):

```{r complexAR1PACFConj, fig.cap="Partial complex conjugate autocorrelation function"}
cpacf(y, method="conjugate", plot=TRUE, main="")
```

Figure \@ref(fig:complexAR1PACFConj) shows three plots: real vs imaginary complex ACF plot at the top and two separate plots for the real and the imaginary parts of the complex PACF at the bottom. As shown in the plots both the real and the imaginary parts of the complex coefficient are statistically different from zero on the 5% significance level for the first lag and not significant for the rest of the lags. This agrees with the data generated process that we used. Furthermore, the real part of the cPACF is close to the true value of 0.5, while the imaginary one is close to the -0.25, which were used in the data generation.

Similarly, we can analyse the complex direct ACF of a variable, which will generate a plot similar to the ones in Figure \@ref(fig:complexAR1PACFConj) (see Figure \@ref(fig:complexAR1PACFDir)):
```{r complexAR1ACFDir, fig.cap="Complex direct autocorrelation function"}
cacf(y, method="direct", plot=TRUE, main="")
```

While the dynamics of direct cACF is similar to the one of the conjugate cPACF, they show different things: the latter is clear of the interim relations between variables, while the former shows the specific correlations between the actual value and each of its lags. Similarly to how we did that before, we can also produce the direct cPACF (Figure \@ref(fig:complexAR1PACFDir)):
```{r complexAR1PACFDir, fig.cap="Partial complex direct autocorrelation function"}
cpacf(y, method="direct", plot=TRUE, main="")
```

Because we generated the data from the complex autoregressive model of order one, the cACF and cPACF give us roughly the same message: the lag one is significant on the 5% level, while the rest of values are not. Note however that the direct cACF/cPACF shows that there are other lags that are significantly different from zero. This appears by chance and should be ignored.

Finally, we can generate ACF/PACF for the variable after its MDS transformation (Figure \@ref(fig:complexAR1Pearson)):
```{r complexAR1Pearson, fig.cap="ACF and PACF of the complex variable after the MDS."}
par(mfcol=c(2,1), mar=c(2,2,2,1))
cacf(y, method="pearson", plot=TRUE)
cpacf(y, method="pearson", plot=TRUE)
```

Similarly to the conjugate cACF, the plots in Figure \@ref(fig:complexAR1Pearson) show that there is a significant spike for the lag one, implying that the variable is autocorrelated. However, they do not give any specific information about the value of the parameter (like conjugate and direct pACF do).

Furthermore, there is another presentation for complex ACF/PACF - in the polar coordinates one. In that case instead of plotting separate real and imaginary parts of the function, we produce the magnitude (absolute value) and the angle (argument) of the complex variables. In that presentation, the higher the magnitude is, the higher overall the value of a complex variable is. Figure \@ref(fig:complexAR1Ploar) demonstrates how this looks for the conjugate cPACF:

```{r complexAR1Ploar, fig.cap="Complex ACF in polar coordinates."}
cpacf(y, method="conjugate", plot=FALSE) |>
    plot(2)
```

The distribution of the absolute value of a bivariate normal complex variable follows Hoyt's distribution, which can be used for the confidence interval construction for the plot above. However, the derivations become complicated and we do not provide them here (we might provide them in the future editions of this monograph).

Finally, one can analyse the ACF/PACF of the separate real and imaginary parts of the residuals, thus ignoring the potential dynamics between them. The analysis in this case becomes trivial and has been discussed in many textbooks and monographs [for example, see @SvetunkovAdam]. In R, if a model is estimated using the `clm()` function from the `complex` package, the `plot()` method supports producing ACF/PACF for each separate variable. For example, for the same model from the Subsection \@ref(assumptionsSpecificationTransformation), we have (see Figure \@ref(fig:complexAR1Ploar)):

```{r complexARDiagnosticsACFPACF, fig.cap="ACF/PACF for the real (Series 1) and imaginary (Series 2) parts of the complex residuals."}
par(mfcol=c(2,2))
plot(complexModel,which=c(10,11))
```

The residuals of this model show that there are no important lags on the 5% significance level. The fact that there are some correlations with values lying outside of the non-rejection region can be explained by random chance. We should take such cases with a pinch of salt not to overcomplicate a model.

In addition, we can do more thorough diagnostics based on the complex ACF/PACF (as discussed above) for any complex-valued model. In that case we need to extract residuals and then apply `cacf()` or `cpacf()` functions. For example, using this code:

```{r eval=FALSE}
residuals(complexModel) |>
    cacf(method="direct")
```

After doing the diagnostics of the residuals of a CLR, we can change its form by introducing complex AR (autoregression) or MA (moving average) elements, depending on what lags seem significant on the selected level. The complex AR/MA model is discussed in Chapter \@ref(ComplexAR).


### Expectation of residuals is zero
While many other assumptions can be diagnosed in one way or another, this assumption in many cases cannot be. For example, if we use OLS, the unconditional mean of the residuals will be equal to zero by design, which does not provide any useful information about the "true" model. However, this assumption can be analysed conditionally, e.g. conditional on the fitted values:

\begin{equation}
    \mathrm{E}(\underline{e}_j | \underline{\mathbf{x}}_j)=0 ,
    (\#eq:residsExpectation)
\end{equation}
meaning that the expectation of complex residual conditional on the values of explanatory variables should be equal to zero. If the conditional mean is not constant and not zero, then we can conclude that some important elements were omitted in the model. In a way, the assumption becomes similar to the assumption about the omitted variables, wrong transformations, and/or autocorrelated residuals. To that extent, we do not discuss it in more detail here.


### Residuals follow an assumed distribution
In the classical real-valued regression, this assumption is not considered to be crucial, and if a model is estimated using OLS, it typically comes to the "residuals follow the Normal distribution". In context of complex variables, the equivalent assumption would be that the complex residuals follow the Complex Normal distribution discussed in Subsection \@ref(distributionCNorm). If the model is formulated using matrix notations \@ref(eq:CLRSystemVectorFinal), the assumption transforms to "Multivariate Normal" one, discussed in Section \@ref(MVNorm). In statistics, this assumption is typically considered as not critical in comparison with the others. This is because it can be shown that in case of OLS/CLS even when it does not hold, but all the others do, the estimates of parameters should be unbiased, efficient and consistent. However, this assumption becomes essential when prediction interval needs to be generated from the model. In that case we want to capture the uncertainty about the data correctly, and if the residuals do not follow the Normal distribution, the prediction interval will be miscalibrated.

One of the simplest ways of checking the assumption is by producing QQ-plots for each part of the complex residuals. This approach ignores the multivariate nature of the residuals (e.g. then can be correlated), but given that each part of the c.r.v. following Complex Normal distribution is Normal as well, this should suffice for diagnostic purposes.

In R, this can be done using the following command (for the model discussed in Subsection \@ref(assumptionsSpecificationTransformation), see Figure \@ref(fig:complexNormDiagnostics)):

```{r complexNormDiagnostics, fig.cap="QQ-plots for the real (Series 1) and imaginary (Series 2) parts of the residuals."}
par(mfcol=c(1,2))
plot(complexModelLogs,which=6)
```

Similarly to the conventional diagnostics related to distributional assumptions, the QQ-plots in Figure \@ref(fig:complexNormDiagnostics) can be interpreted as follows. If all the points (empirical quantiles) lie on or close to the lines (theoretical quantiles) then the empirical distribuion looks similar to the theoretical one. In the example in Figure \@ref(fig:complexNormDiagnostics), we see that the points are indeed close to the lines for both real (called "Series 1") and imaginary (called "Series 2" in the plot) parts of the residuals. There is a slight deviation in the right tail of the imaginary part, but it is not substantial. Based on this diagnostics we can conclude that the residuals look Normal or at least very close to it.


### Residuals follow a circular distribution
Finally, in signal processing literature, there is an assumption that residuals follow so called "circular distribution". This means that the covariance between the residuals equals to zero and that the variance of the real part is equal to the variance of the imaginary one. This also implies that the direct variance of residuals is equal to zero. While this might be a suitable assumption in signal processing, there is no good rationale for this to hold universally for all CLR models. In fact, the power of the CLR comes from different variances and potentially non-zero covariance between the residuals, because in that case the model can capture a complex dynamics between the parts of the complex variable. So, we suggest ignoring this assumption, especially given that it does not effect estimates of parameters of the forecasts from the model.

However, it is important to note that the used estimation methods might imply some parts of this assumption. For example, OLS minimises the sum of squared residuals of the real and imaginary parts, completely neglecting the covariance between them (see Subsection \@ref(SCLREstimationOLS)). At the same time, the CLS (Subsection \@ref(SCLREstimationCLS)) ignores the specific sizes of the squared residuals and focuses on making their squares equal, minimising at the same time the covariance between them. If we believe that the residuals might have a more complicated distribution (elliptic Normal) then MLE (Subsection \@ref(SCLREstimationLikelihood)) might be the best option for the model estimation.



## Explanatory variables are not correlated with anything but the response variable
The two assumptions in this group relate to the estimation of model rather than anything else:

1. No endogeneity
2. No multicollinearity

The first assumption comes to the idea that a linear regression model can only capture a one-directional relation, where an explanatory variable impacts the response variable. If the relation in the real life is bi-directional then the estimates of parameters of a regression model will be biased. In that case, the relation needs to be somehow made one-directional, for example by substituting the explanatory variable that causes a problem with something similar that is not impacted by the response variable. In case of the complex linear regression, the situation would be similar to the conventional one. We do not plan to discuss how to solve the problem in case of CLR in this monograph. But we believe that the methods from econometrics of real-valued models should be widely applicable here as well (e.g. using proxies or instrumental variables).

As for the multicollinearity issue, in the real-valued statistics, it arises when least squares or maximum likelihood is used for estimation. It is, in a way, a technical issue: if the explanatory variables are linearly related it becomes difficult to split the impact of each of the explanatory variables on the response variable and thus challenging to correctly capture the relations between the explanatory variables and the response one.

In case of an OLS applied to a conventional real-valued regression model, the issue comes to inverting the matrix $\left( {\mathbf{X}}^\prime {\mathbf{X}} \right)^{-1}$. If some of variables in $\mathbf{X}$ are strongly linearly related, the determinant of the matrix becomes close to zero and thus the inversion becomes challenging. Even if it is still possible to invert the matrix, the estimates of parameters become inefficient because small changes in the related explanatory variables may lead to substantial changes in the inversion.

In case of CLR, the situation is similar, but with some specific features. If the OLS is used for parameters estimation then a potential issue might arise from the inversion of $\left( \underline{\mathbf{X}}^\prime \underline{\mathbf{X}}\right)^{-1}$. In one of the special cases, this comes to the conjugate covariance between variables. If it is too high in comparison to the conjugate variances, then we would face the multicollinearity issue, which would make the estimates of parameters inefficient. More generally speaking, if a complex linear regression between an explanatory variable and other explanatory variables estimated using OLS captures the relations between variables well (i.e. having a high coefficient of determination), then we might be facing a multicollinearity problem. Similar problems might appear in case of CLS, but they come to a slightly different inversion, $\left( \underline{\mathbf{X}}^\top \underline{\mathbf{X}}\right)^{-1}$ and thus relates to direct covariances rather than the conjugate ones. Finally, when it comes to the likelihood, due to the model formulation, the conventional real-valued relations between explanatory variables should be considered when multicollinearity is suspected. All of this means that a choice of an estimator can be dictated by the strength of the conventional Pearson's, direct and/or conjugate correlations (and more widely conventional linear model and/or complex linear model for explanatory variables estimated using OLS/CLS).


## Diagnostics

sigma & Sigma

R^2, R^2 - adjusted
R^2 for different parts or for the whole model
R^2 for covariances? Percentage of explained covariance between variables?

```{r}
# 1- cvar(e, method="conj") / cvar(y,method="conj")

# 1-covar(e) / covar(y)
```


Visual analysis


<!-- ## Forecasting -->


<!-- ## Examples of application (Production functions) -->

